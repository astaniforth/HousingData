{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3441ae",
   "metadata": {},
   "source": [
    "# CEQR API Test\n",
    "\n",
    "Search CEQR (City Environmental Quality Review) projects by borough, block, and lot.\n",
    "\n",
    "**No browser needed** - fully automated with Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"‚úÖ Ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2826dd1",
   "metadata": {},
   "source": [
    "## Parse Results Function\n",
    "\n",
    "Function to parse CEQR search results and extract detail page links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a322b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parser loaded (with detail page links)\n"
     ]
    }
   ],
   "source": [
    "def parse_ceqr_results(response):\n",
    "    \"\"\"Extract CEQR results table from HTML response, including detail page links.\"\"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find results table\n",
    "    table = soup.find('table', {'id': lambda x: x and 'grdSearchResults' in x})\n",
    "    \n",
    "    if not table:\n",
    "        # Try finding by content\n",
    "        tables = soup.find_all('table')\n",
    "        for t in tables:\n",
    "            if 'CEQR Number' in t.get_text() or 'Project Name' in t.get_text():\n",
    "                table = t\n",
    "                break\n",
    "    \n",
    "    if not table:\n",
    "        print(\"‚ö†Ô∏è  No results table found\")\n",
    "        return None\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return None\n",
    "    \n",
    "    # Get headers\n",
    "    headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "    \n",
    "    # Get data with detail links\n",
    "    data = []\n",
    "    detail_links = []\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if cells:\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if any(cell.strip() for cell in row_data):\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract detail page link\n",
    "                detail_link = row.find('a', {'id': lambda x: x and 'hlnkOpenDetails' in x})\n",
    "                if detail_link and detail_link.get('href'):\n",
    "                    full_url = f\"https://a002-ceqraccess.nyc.gov/ceqr/{detail_link['href']}\"\n",
    "                    detail_links.append(full_url)\n",
    "                else:\n",
    "                    detail_links.append(\"\")\n",
    "    \n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame with detail links column\n",
    "    df = pd.DataFrame(data, columns=headers[:len(data[0])])\n",
    "    df['Detail Page'] = detail_links\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Parser loaded (with detail page links)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b31f3a",
   "metadata": {},
   "source": [
    "## Search Functions\n",
    "\n",
    "### BBL Parser & Search\n",
    "\n",
    "Search by BBL (10-digit Borough-Block-Lot number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function loaded\n"
     ]
    }
   ],
   "source": [
    "def search_ceqr_by_bbl(bbl):\n",
    "    \"\"\"\n",
    "    Search CEQR database by BBL (Borough-Block-Lot).\n",
    "    \n",
    "    Args:\n",
    "        bbl: 10-digit BBL number (string or int)\n",
    "             Format: BBBBBLLLL where B=borough (1-5), BBBBB=block, LLLL=lot\n",
    "    \n",
    "    Returns: DataFrame with results or None\n",
    "    \"\"\"\n",
    "    # Convert to string and pad if needed\n",
    "    bbl_str = str(bbl).zfill(10)\n",
    "    \n",
    "    if len(bbl_str) != 10:\n",
    "        print(f\"‚ùå Invalid BBL: {bbl} (must be 10 digits)\")\n",
    "        return None\n",
    "    \n",
    "    # Parse BBL\n",
    "    boro_code = bbl_str[0]\n",
    "    block = bbl_str[1:6].lstrip('0') or '0'  # Remove leading zeros\n",
    "    lot = bbl_str[6:10].lstrip('0') or '0'   # Remove leading zeros\n",
    "    \n",
    "    # Map borough code to name\n",
    "    boro_map = {\n",
    "        '1': 'Manhattan',\n",
    "        '2': 'Bronx', \n",
    "        '3': 'Brooklyn',\n",
    "        '4': 'Queens',\n",
    "        '5': 'Staten Island'\n",
    "    }\n",
    "    \n",
    "    borough = boro_map.get(boro_code)\n",
    "    if not borough:\n",
    "        print(f\"‚ùå Invalid borough code: {boro_code}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìç BBL {bbl} ‚Üí {borough}, Block {block}, Lot {lot}\")\n",
    "    \n",
    "    # Search and parse\n",
    "    success, result = search_ceqr(borough, block, lot)\n",
    "    \n",
    "    if success:\n",
    "        df = parse_ceqr_results(result)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"‚ùå Search failed: {result}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_ceqr(borough, block, lot=\"\"):\n",
    "    \"\"\"\n",
    "    Search CEQR database by borough, block, and lot.\n",
    "    \n",
    "    Returns: tuple (success: bool, response or error message)\n",
    "    \"\"\"\n",
    "    url = \"https://a002-ceqraccess.nyc.gov/ceqr/\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Step 1: GET initial page to get VIEWSTATE\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Searching: {borough}, Block {block}\" + (f\", Lot {lot}\" if lot else \"\"))\n",
    "        \n",
    "        # GET the page\n",
    "        init_resp = session.get(url, headers=headers, timeout=30)\n",
    "        if init_resp.status_code != 200:\n",
    "            return False, f\"Failed to load page: {init_resp.status_code}\"\n",
    "        \n",
    "        # Extract VIEWSTATE fields\n",
    "        soup = BeautifulSoup(init_resp.text, 'html.parser')\n",
    "        viewstate = soup.find('input', {'id': '__VIEWSTATE'})\n",
    "        viewstate_gen = soup.find('input', {'id': '__VIEWSTATEGENERATOR'})\n",
    "        eventval = soup.find('input', {'id': '__EVENTVALIDATION'})\n",
    "        \n",
    "        if not viewstate:\n",
    "            return False, \"Could not find VIEWSTATE\"\n",
    "        \n",
    "        print(\"‚úÖ Got session\")\n",
    "        \n",
    "        # Step 2: POST search\n",
    "        form_data = {\n",
    "            \"__LASTFOCUS\": \"\",\n",
    "            \"__EVENTTARGET\": \"\",\n",
    "            \"__EVENTARGUMENT\": \"\",\n",
    "            \"__VIEWSTATE\": viewstate['value'],\n",
    "            \"__VIEWSTATEGENERATOR\": viewstate_gen['value'] if viewstate_gen else \"F2CE38DF\",\n",
    "            \"__SCROLLPOSITIONX\": \"0\",\n",
    "            \"__SCROLLPOSITIONY\": \"0\",\n",
    "            \"__VIEWSTATEENCRYPTED\": \"\",\n",
    "            \"__EVENTVALIDATION\": eventval['value'] if eventval else \"\",\n",
    "            \"ctl00$MainContent$txtKeyword\": \"\",\n",
    "            \"ctl00$MainContent$ddlLeadAgency\": \"XYU@2!\",\n",
    "            \"ctl00$MainContent$txtCeqrNumber\": \"\",\n",
    "            \"ctl00$MainContent$txtProjectName\": \"\",\n",
    "            \"ctl00$MainContent$ddlCommunityDistrict\": \"XYU@2!\",\n",
    "            \"ctl00$MainContent$ddlBorough\": borough,\n",
    "            \"ctl00$MainContent$txtBlock\": block,\n",
    "            \"ctl00$MainContent$txtLot\": lot,\n",
    "            \"ctl00$MainContent$btnSearch\": \" Search\"\n",
    "        }\n",
    "        \n",
    "        post_headers = {\n",
    "            **headers,\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Origin\": \"https://a002-ceqraccess.nyc.gov\",\n",
    "            \"Referer\": url,\n",
    "            \"Cache-Control\": \"max-age=0\"\n",
    "        }\n",
    "        \n",
    "        response = session.post(url, headers=post_headers, data=form_data, timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return False, f\"Search failed: {response.status_code}\"\n",
    "        \n",
    "        # Check for results\n",
    "        if 'grdSearchResults' in response.text or 'Search Results' in response.text:\n",
    "            print(\"‚úÖ Got results\")\n",
    "            return True, response\n",
    "        elif 'Error' in response.text or 'Unhandled' in response.text:\n",
    "            return False, \"Server error\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No results found\")\n",
    "            return True, response\n",
    "            \n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183378f",
   "metadata": {},
   "source": [
    "## Parse Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6588141",
   "metadata": {},
   "source": [
    "## Detail Page Scraping\n",
    "\n",
    "Functions to navigate to detail pages, extract text content, and find PDF links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f51268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Detail page scraping functions loaded\n"
     ]
    }
   ],
   "source": [
    "def scrape_detail_page(detail_url, session=None):\n",
    "    \"\"\"\n",
    "    Navigate to a CEQR detail page and extract text content and PDF links.\n",
    "    \n",
    "    Args:\n",
    "        detail_url: Full URL to the detail page\n",
    "        session: Optional requests.Session object for maintaining cookies\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - 'url': The detail page URL\n",
    "            - 'text': Full text content from the page\n",
    "            - 'pdf_links': List of PDF URLs found on the page\n",
    "            - 'success': Boolean indicating if scraping succeeded\n",
    "            - 'error': Error message if failed\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "    }\n",
    "    \n",
    "    result = {\n",
    "        'url': detail_url,\n",
    "        'text': '',\n",
    "        'pdf_links': [],\n",
    "        'success': False,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üåê Fetching: {detail_url[:80]}...\")\n",
    "        \n",
    "        response = session.get(detail_url, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            result['error'] = f\"HTTP {response.status_code}\"\n",
    "            return result\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content (removing script and style tags)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get main content - try to find the main content area\n",
    "        main_content = soup.find('div', {'id': lambda x: x and ('MainContent' in x or 'content' in x.lower())})\n",
    "        if not main_content:\n",
    "            main_content = soup.find('body')\n",
    "        \n",
    "        if main_content:\n",
    "            # Get all text, preserving some structure\n",
    "            text = main_content.get_text(separator='\\n', strip=True)\n",
    "            result['text'] = text\n",
    "        else:\n",
    "            result['text'] = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Find all PDF links on the detail page\n",
    "        pdf_links = []\n",
    "        \n",
    "        # Helper function to normalize URLs\n",
    "        def normalize_url(href, base_url):\n",
    "            \"\"\"Convert relative URLs to absolute URLs.\"\"\"\n",
    "            if href.startswith('http'):\n",
    "                return href\n",
    "            elif href.startswith('/'):\n",
    "                # Absolute path on same domain\n",
    "                domain = '/'.join(base_url.split('/')[:3])\n",
    "                return domain + href\n",
    "            else:\n",
    "                # Relative path\n",
    "                base = '/'.join(base_url.split('/')[:-1])\n",
    "                return base + '/' + href\n",
    "        \n",
    "        # Look for all links that might be PDFs\n",
    "        # CEQR detail pages often have PDFs in tables or file sections\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            link_text = link.get_text(strip=True).lower()\n",
    "            \n",
    "            # Check if it's a direct PDF link\n",
    "            is_pdf = (\n",
    "                href.endswith('.pdf') or \n",
    "                '.pdf' in href.lower() or\n",
    "                # Check for handler URLs (ProjectFile.ashx) - these serve PDFs\n",
    "                'projectfile.ashx' in href.lower() or\n",
    "                'filehandler.ashx' in href.lower() or\n",
    "                'handlers/projectfile.ashx' in href.lower() or\n",
    "                # Check for file= parameter (common in handler URLs)\n",
    "                'file=' in href.lower()\n",
    "            )\n",
    "            \n",
    "            # Also check link text for PDF indicators (KB/MB file sizes are common)\n",
    "            has_pdf_indicator = (\n",
    "                'pdf' in link_text or\n",
    "                'kb' in link_text or  # File size indicator (e.g., \"156.5KB\")\n",
    "                'mb' in link_text or\n",
    "                link_text.endswith('.pdf') or\n",
    "                # Check parent elements for file-related text\n",
    "                (link.parent and ('file' in link.parent.get_text(strip=True).lower() or \n",
    "                                  'document' in link.parent.get_text(strip=True).lower()))\n",
    "            )\n",
    "            \n",
    "            # Include if it's clearly a PDF link or has PDF indicators with handler/file params\n",
    "            if is_pdf or (has_pdf_indicator and ('handler' in href.lower() or 'file=' in href.lower() or 'ashx' in href.lower())):\n",
    "                normalized_url = normalize_url(href, detail_url)\n",
    "                pdf_links.append(normalized_url)\n",
    "        \n",
    "        # Also check for direct PDF links in iframes or embedded content\n",
    "        for iframe in soup.find_all('iframe', src=True):\n",
    "            src = iframe['src']\n",
    "            if '.pdf' in src.lower() or 'projectfile.ashx' in src.lower():\n",
    "                normalized_url = normalize_url(src, detail_url)\n",
    "                pdf_links.append(normalized_url)\n",
    "        \n",
    "        # Look for PDF links in specific CEQR sections (Files section, Documents section, etc.)\n",
    "        # Check for tables or divs that might contain file listings\n",
    "        file_sections = soup.find_all(['table', 'div'], class_=lambda x: x and (\n",
    "            'file' in x.lower() or \n",
    "            'document' in x.lower() or\n",
    "            'pdf' in x.lower()\n",
    "        ))\n",
    "        \n",
    "        for section in file_sections:\n",
    "            for link in section.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if ('projectfile.ashx' in href.lower() or \n",
    "                    'file=' in href.lower() or \n",
    "                    '.pdf' in href.lower()):\n",
    "                    normalized_url = normalize_url(href, detail_url)\n",
    "                    if normalized_url not in pdf_links:\n",
    "                        pdf_links.append(normalized_url)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_pdf_links = []\n",
    "        for link in pdf_links:\n",
    "            if link not in seen:\n",
    "                seen.add(link)\n",
    "                unique_pdf_links.append(link)\n",
    "        \n",
    "        result['pdf_links'] = unique_pdf_links\n",
    "        result['success'] = True\n",
    "        \n",
    "        print(f\"‚úÖ Scraped: {len(result['text'])} chars, {len(result['pdf_links'])} PDFs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        print(f\"‚ùå Error scraping {detail_url}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def scrape_all_detail_pages(df, detail_column='Detail Page', session=None):\n",
    "    \"\"\"\n",
    "    Scrape all detail pages from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with detail page URLs\n",
    "        detail_column: Name of column containing detail page URLs\n",
    "        session: Optional requests.Session object\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added columns:\n",
    "            - 'detail_text': Full text from detail page\n",
    "            - 'pdf_links': List of PDF URLs (as string, comma-separated)\n",
    "            - 'pdf_count': Number of PDFs found\n",
    "            - 'scrape_success': Boolean indicating if scraping succeeded\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    if detail_column not in df.columns:\n",
    "        print(f\"‚ùå Column '{detail_column}' not found in DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Create new columns\n",
    "    df = df.copy()\n",
    "    df['detail_text'] = ''\n",
    "    df['pdf_links'] = ''\n",
    "    df['pdf_count'] = 0\n",
    "    df['scrape_success'] = False\n",
    "    \n",
    "    print(f\"\\nüìÑ Scraping {len(df)} detail pages...\\n\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        detail_url = row[detail_column]\n",
    "        \n",
    "        if not detail_url or pd.isna(detail_url) or detail_url == '':\n",
    "            continue\n",
    "        \n",
    "        result = scrape_detail_page(detail_url, session)\n",
    "        \n",
    "        df.at[idx, 'detail_text'] = result['text']\n",
    "        df.at[idx, 'pdf_links'] = ', '.join(result['pdf_links'])\n",
    "        df.at[idx, 'pdf_count'] = len(result['pdf_links'])\n",
    "        df.at[idx, 'scrape_success'] = result['success']\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        import time\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    successful = df['scrape_success'].sum()\n",
    "    total_pdfs = df['pdf_count'].sum()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed: {successful}/{len(df)} pages scraped, {total_pdfs} PDFs found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Detail page scraping functions loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8722c",
   "metadata": {},
   "source": [
    "### Download PDFs\n",
    "\n",
    "Function to download PDF files from the extracted links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd670074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF download functions loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def download_pdf(pdf_url, output_dir='pdfs', session=None, filename=None):\n",
    "    \"\"\"\n",
    "    Download a PDF file from a URL.\n",
    "    \n",
    "    Args:\n",
    "        pdf_url: URL of the PDF to download\n",
    "        output_dir: Directory to save PDFs (default: 'pdfs')\n",
    "        session: Optional requests.Session object\n",
    "        filename: Optional custom filename (otherwise extracted from URL)\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - 'success': Boolean\n",
    "            - 'filepath': Path to saved file (if successful)\n",
    "            - 'error': Error message (if failed)\n",
    "            - 'skipped': Boolean indicating if file was skipped (already existed)\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    result = {\n",
    "        'success': False,\n",
    "        'filepath': None,\n",
    "        'error': None,\n",
    "        'skipped': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get filename from URL if not provided\n",
    "        if not filename:\n",
    "            parsed = urlparse(pdf_url)\n",
    "            \n",
    "            # For handler URLs (ProjectFile.ashx), try to extract filename from query params\n",
    "            if 'projectfile.ashx' in pdf_url.lower() or 'filehandler.ashx' in pdf_url.lower():\n",
    "                # Try to get filename from query parameters\n",
    "                query_params = parse_qs(parsed.query)\n",
    "                \n",
    "                # Check if there's a 'file' parameter that might contain filename info\n",
    "                if 'file' in query_params:\n",
    "                    # The file parameter is often base64 encoded, so we'll use a hash\n",
    "                    url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:12]\n",
    "                    filename = f\"ceqr_file_{url_hash}.pdf\"\n",
    "                else:\n",
    "                    # Use URL hash as fallback\n",
    "                    url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:12]\n",
    "                    filename = f\"ceqr_file_{url_hash}.pdf\"\n",
    "            else:\n",
    "                # For regular URLs, extract from path\n",
    "                filename = os.path.basename(parsed.path)\n",
    "                if not filename or not filename.endswith('.pdf'):\n",
    "                    # Generate filename from URL\n",
    "                    filename = pdf_url.split('/')[-1].split('?')[0]\n",
    "                    if not filename or not filename.endswith('.pdf'):\n",
    "                        # Use URL hash as fallback\n",
    "                        url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:12]\n",
    "                        filename = f\"ceqr_file_{url_hash}.pdf\"\n",
    "        \n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Skip if file already exists\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"‚è≠Ô∏è  Skipping (exists): {filename}\")\n",
    "            result['success'] = True\n",
    "            result['filepath'] = filepath\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        \n",
    "        print(f\"‚¨áÔ∏è  Downloading: {filename}\")\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "            \"Accept\": \"application/pdf,*/*\"\n",
    "        }\n",
    "        \n",
    "        response = session.get(pdf_url, headers=headers, timeout=60, stream=True)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            result['error'] = f\"HTTP {response.status_code}\"\n",
    "            return result\n",
    "        \n",
    "        # Check if it's actually a PDF\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'pdf' not in content_type.lower() and not pdf_url.lower().endswith('.pdf'):\n",
    "            print(f\"‚ö†Ô∏è  Warning: Content-Type is {content_type}, not PDF\")\n",
    "        \n",
    "        # Save file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"‚úÖ Saved: {filename} ({file_size:,} bytes)\")\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['filepath'] = filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        print(f\"‚ùå Error downloading {pdf_url}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def download_all_pdfs(df, pdf_links_column='pdf_links', output_dir='pdfs', session=None):\n",
    "    \"\"\"\n",
    "    Download all PDFs from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with PDF links\n",
    "        pdf_links_column: Column name containing PDF links (comma-separated string)\n",
    "        output_dir: Directory to save PDFs\n",
    "        session: Optional requests.Session object\n",
    "    \n",
    "    Returns:\n",
    "        dict with download statistics\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    if pdf_links_column not in df.columns:\n",
    "        print(f\"‚ùå Column '{pdf_links_column}' not found in DataFrame\")\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'total_pdfs': 0,\n",
    "        'downloaded': 0,\n",
    "        'skipped': 0,\n",
    "        'failed': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚¨áÔ∏è  Downloading PDFs to '{output_dir}'...\\n\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        pdf_links_str = row[pdf_links_column]\n",
    "        \n",
    "        if not pdf_links_str or pd.isna(pdf_links_str) or pdf_links_str == '':\n",
    "            continue\n",
    "        \n",
    "        # Parse comma-separated links\n",
    "        pdf_urls = [url.strip() for url in str(pdf_links_str).split(',') if url.strip()]\n",
    "        \n",
    "        for pdf_url in pdf_urls:\n",
    "            stats['total_pdfs'] += 1\n",
    "            \n",
    "            result = download_pdf(pdf_url, output_dir, session)\n",
    "            \n",
    "            if result['success']:\n",
    "                if result.get('skipped', False):\n",
    "                    stats['skipped'] += 1\n",
    "                else:\n",
    "                    stats['downloaded'] += 1\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "                stats['errors'].append(f\"{pdf_url}: {result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Small delay between downloads\n",
    "            import time\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nüìä Download Summary:\")\n",
    "    print(f\"   Total PDFs: {stats['total_pdfs']}\")\n",
    "    print(f\"   Downloaded: {stats['downloaded']}\")\n",
    "    print(f\"   Skipped (already exists): {stats['skipped']}\")\n",
    "    print(f\"   Failed: {stats['failed']}\")\n",
    "    \n",
    "    if stats['errors']:\n",
    "        print(f\"\\n‚ùå Errors:\")\n",
    "        for error in stats['errors'][:10]:  # Show first 10 errors\n",
    "            print(f\"   {error}\")\n",
    "        if len(stats['errors']) > 10:\n",
    "            print(f\"   ... and {len(stats['errors']) - 10} more errors\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print(\"‚úÖ PDF download functions loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643bb8f",
   "metadata": {},
   "source": [
    "## Complete Workflow: Search ‚Üí Scrape Detail Pages ‚Üí Extract PDFs ‚Üí Download\n",
    "\n",
    "The workflow is:\n",
    "1. **Search** for CEQR projects (by BBL or borough/block/lot)\n",
    "2. **Navigate to detail pages** from search results\n",
    "3. **Scrape detail pages** to extract text content and **find PDF links on those detail pages**\n",
    "4. **Download PDFs** that were found on the detail pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501c7601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "STEP 1: Searching for CEQR projects\n",
      "====================================================================================================\n",
      "üìç BBL 3014890011 ‚Üí Brooklyn, Block 1489, Lot 11\n",
      "üîç Searching: Brooklyn, Block 1489, Lot 11\n",
      "‚úÖ Got session\n",
      "‚úÖ Got results\n",
      "\n",
      "‚úÖ Found 2 projects\n",
      "   Detail page URLs will be extracted from search results\n",
      "\n",
      "====================================================================================================\n",
      "STEP 2: Scraping detail pages to extract PDF links\n",
      "====================================================================================================\n",
      "   (This visits each detail page and finds PDF links on those pages)\n",
      "\n",
      "\n",
      "üìÑ Scraping 2 detail pages...\n",
      "\n",
      "üåê Fetching: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MDZIUEQwMDFL0&signature=bc0454...\n",
      "‚úÖ Scraped: 781 chars, 3 PDFs\n",
      "üåê Fetching: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MTlIUEQwNTdL0&signature=8a44e7...\n",
      "‚úÖ Scraped: 340 chars, 5 PDFs\n",
      "\n",
      "‚úÖ Completed: 2/2 pages scraped, 8 PDFs found\n",
      "\n",
      "====================================================================================================\n",
      "STEP 3: Summary of PDFs found on detail pages\n",
      "====================================================================================================\n",
      "\n",
      "üìä Results:\n",
      "   Projects with PDFs: 2/2\n",
      "   Total PDFs found on detail pages: 8\n",
      "\n",
      "‚úÖ PDFs were successfully extracted from detail pages!\n",
      "   Ready to download in next step\n"
     ]
    }
   ],
   "source": [
    "# Complete workflow example: Search ‚Üí Scrape Detail Pages ‚Üí Download PDFs\n",
    "# This demonstrates that PDFs are extracted FROM the detail pages\n",
    "\n",
    "# Step 1: Search for CEQR projects\n",
    "print(\"=\"*100)\n",
    "print(\"STEP 1: Searching for CEQR projects\")\n",
    "print(\"=\"*100)\n",
    "example_bbl = \"3014890011\"  # Change this to your BBL\n",
    "df = search_ceqr_by_bbl(example_bbl)\n",
    "\n",
    "if df is not None and len(df) > 0:\n",
    "    print(f\"\\n‚úÖ Found {len(df)} projects\")\n",
    "    print(f\"   Detail page URLs will be extracted from search results\\n\")\n",
    "    \n",
    "    # Step 2: Scrape detail pages to extract PDFs\n",
    "    print(\"=\"*100)\n",
    "    print(\"STEP 2: Scraping detail pages to extract PDF links\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"   (This visits each detail page and finds PDF links on those pages)\\n\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    df = scrape_all_detail_pages(df, detail_column='Detail Page', session=session)\n",
    "    \n",
    "    # Step 3: Show what was found\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"STEP 3: Summary of PDFs found on detail pages\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    total_pdfs = df['pdf_count'].sum()\n",
    "    projects_with_pdfs = (df['pdf_count'] > 0).sum()\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Projects with PDFs: {projects_with_pdfs}/{len(df)}\")\n",
    "    print(f\"   Total PDFs found on detail pages: {total_pdfs}\")\n",
    "    \n",
    "    if total_pdfs > 0:\n",
    "        print(f\"\\n‚úÖ PDFs were successfully extracted from detail pages!\")\n",
    "        print(f\"   Ready to download in next step\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No PDFs found on detail pages\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No projects found. Try a different BBL.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a1154",
   "metadata": {},
   "source": [
    "## Test Detail Page Scraping\n",
    "\n",
    "Test scraping detail pages and extracting PDF links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a7b813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "STEP 1: Scraping detail pages to extract PDF links\n",
      "====================================================================================================\n",
      "Found 2 detail pages to scrape\n",
      "\n",
      "\n",
      "üìÑ Scraping 2 detail pages...\n",
      "\n",
      "üåê Fetching: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MDZIUEQwMDFL0&signature=bc0454...\n",
      "‚úÖ Scraped: 781 chars, 3 PDFs\n",
      "üåê Fetching: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MTlIUEQwNTdL0&signature=8a44e7...\n",
      "‚úÖ Scraped: 340 chars, 5 PDFs\n",
      "\n",
      "‚úÖ Completed: 2/2 pages scraped, 8 PDFs found\n",
      "\n",
      "====================================================================================================\n",
      "SCRAPED DETAIL PAGES - PDFs EXTRACTED FROM DETAIL PAGES\n",
      "====================================================================================================\n",
      "\n",
      "üìÑ CEQR Number: 06HPD001K\n",
      "   Detail Page URL: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MDZIUEQwMDFL0&signature=bc04541d153279ec253c32419792a4bd91839cff\n",
      "   Scrape Success: True\n",
      "   Text length: 781 characters\n",
      "   PDFs found on detail page: 3\n",
      "   PDF links extracted from detail page:\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAwNlwwNkhQRDAwMUtcbGVhZF9hZ...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAwNlwwNkhQRDAwMUtcZGV0X3NpZ...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAwNlwwNkhQRDAwMUtcZWFzXDA2S...\n",
      "\n",
      "   Page text preview: Geographical Information Block Lot Community District Street Address Zip Code 1489 6 BK04 11216 1489 11 BK04 11216 1489 12 BK04 11216 1489 13 BK04 11216 1489 14 BK04 11216 1489 15 BK04 11216 3387 4 BK04 11216 3387 5 BK04 11216 3387 6 BK04 11216 3387 7 BK04 11216 3387 9 BK04 11216 3387 11 BK04 11216 ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ CEQR Number: 19HPD057K\n",
      "   Detail Page URL: https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MTlIUEQwNTdL0&signature=8a44e76b0dd63a6a5ef7c569ac7201ef7866a4c6\n",
      "   Scrape Success: True\n",
      "   Text length: 340 characters\n",
      "   PDFs found on detail page: 5\n",
      "   PDF links extracted from detail page:\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAxOVwxOUhQRDA1N0tcbGVhZF9hZ...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAxOVwxOUhQRDA1N0tcZGV0X3NpZ...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAxOVwxOUhQRDA1N0tcZGV0X3NpZ...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAxOVwxOUhQRDA1N0tcZWFzXDE5S...\n",
      "      - https://a002-ceqraccess.nyc.gov/ceqr/../Handlers/ProjectFile.ashx?file=MjAxOVwxOUhQRDA1N0tcZWFzXDE5S...\n",
      "\n",
      "   Page text preview: Geographical Information Block Lot Community District Street Address Zip Code 1489 6 BK16 1510 Broadway 10029 1489 11 BK16 1510 Broadway 10029 1489 12 BK16 1510 Broadway 10029 1489 13 BK16 1510 Broadway 10029 1489 14 BK16 1510 Broadway 10029 1489 15 BK16 1510 Broadway 10029 1489 16 BK16 1510 Broadwa...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìä Summary:\n",
      "   Detail pages scraped: 2/2\n",
      "   Total PDFs found across all detail pages: 8\n"
     ]
    }
   ],
   "source": [
    "# Test scraping detail pages from the previous search results\n",
    "# This extracts PDF links FROM the detail pages\n",
    "if 'df' in locals() and df is not None and 'Detail Page' in df.columns:\n",
    "    # Create a session to maintain cookies\n",
    "    session = requests.Session()\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"STEP 1: Scraping detail pages to extract PDF links\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Found {len(df)} detail pages to scrape\\n\")\n",
    "    \n",
    "    # Scrape all detail pages (this visits each detail page and extracts PDFs)\n",
    "    df_with_details = scrape_all_detail_pages(df, detail_column='Detail Page', session=session)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SCRAPED DETAIL PAGES - PDFs EXTRACTED FROM DETAIL PAGES\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for idx, row in df_with_details.iterrows():\n",
    "        print(f\"\\nüìÑ CEQR Number: {row.get('CEQR Number', 'N/A')}\")\n",
    "        print(f\"   Detail Page URL: {row['Detail Page']}\")\n",
    "        print(f\"   Scrape Success: {row['scrape_success']}\")\n",
    "        print(f\"   Text length: {len(row['detail_text'])} characters\")\n",
    "        print(f\"   PDFs found on detail page: {row['pdf_count']}\")\n",
    "        \n",
    "        if row['pdf_count'] > 0:\n",
    "            print(f\"   PDF links extracted from detail page:\")\n",
    "            for pdf_link in row['pdf_links'].split(', '):\n",
    "                if pdf_link:\n",
    "                    # Show shortened version for display\n",
    "                    if len(pdf_link) > 100:\n",
    "                        print(f\"      - {pdf_link[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"      - {pdf_link}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No PDFs found on this detail page\")\n",
    "        \n",
    "        # Show first 300 chars of text\n",
    "        if row['detail_text']:\n",
    "            preview = row['detail_text'][:300].replace('\\n', ' ')\n",
    "            print(f\"\\n   Page text preview: {preview}...\")\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "    \n",
    "    # Update df variable\n",
    "    df = df_with_details\n",
    "    \n",
    "    # Summary\n",
    "    total_pdfs = df['pdf_count'].sum()\n",
    "    successful_scrapes = df['scrape_success'].sum()\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Detail pages scraped: {successful_scrapes}/{len(df)}\")\n",
    "    print(f\"   Total PDFs found across all detail pages: {total_pdfs}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No DataFrame with detail pages found. Run a search first.\")\n",
    "    print(\"   Example: df = search_ceqr_by_bbl('3014890011')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933becd",
   "metadata": {},
   "source": [
    "## Download PDFs\n",
    "\n",
    "Download all PDFs found on the detail pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f5ea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "STEP 2: Downloading PDFs extracted from detail pages\n",
      "====================================================================================================\n",
      "Found 8 PDFs to download (extracted from detail pages)\n",
      "\n",
      "\n",
      "‚¨áÔ∏è  Downloading PDFs to 'pdfs'...\n",
      "\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_34d943cde35a.pdf\n",
      "‚úÖ Saved: ceqr_file_34d943cde35a.pdf (61,570 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_5d18dad37547.pdf\n",
      "‚úÖ Saved: ceqr_file_5d18dad37547.pdf (160,270 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_46b26accb27b.pdf\n",
      "‚úÖ Saved: ceqr_file_46b26accb27b.pdf (5,066,111 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_373912afa8eb.pdf\n",
      "‚úÖ Saved: ceqr_file_373912afa8eb.pdf (216,552 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_7ad839ae6368.pdf\n",
      "‚úÖ Saved: ceqr_file_7ad839ae6368.pdf (232,748 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_adc66c7e0352.pdf\n",
      "‚úÖ Saved: ceqr_file_adc66c7e0352.pdf (237,154 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_c622083017b9.pdf\n",
      "‚úÖ Saved: ceqr_file_c622083017b9.pdf (42,463,882 bytes)\n",
      "‚¨áÔ∏è  Downloading: ceqr_file_73e88c173ee7.pdf\n",
      "‚úÖ Saved: ceqr_file_73e88c173ee7.pdf (40,776,723 bytes)\n",
      "\n",
      "üìä Download Summary:\n",
      "   Total PDFs: 8\n",
      "   Downloaded: 8\n",
      "   Skipped (already exists): 0\n",
      "   Failed: 0\n",
      "\n",
      "‚úÖ Download complete!\n",
      "   PDFs saved to: pdfs/\n"
     ]
    }
   ],
   "source": [
    "# Download all PDFs that were extracted from detail pages\n",
    "if 'df' in locals() and df is not None and 'pdf_links' in df.columns:\n",
    "    # Create a session to maintain cookies\n",
    "    session = requests.Session()\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"STEP 2: Downloading PDFs extracted from detail pages\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Count total PDFs to download\n",
    "    total_pdfs_to_download = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['pdf_links'] and pd.notna(row['pdf_links']):\n",
    "            pdf_list = [url.strip() for url in str(row['pdf_links']).split(',') if url.strip()]\n",
    "            total_pdfs_to_download += len(pdf_list)\n",
    "    \n",
    "    print(f\"Found {total_pdfs_to_download} PDFs to download (extracted from detail pages)\\n\")\n",
    "    \n",
    "    # Download PDFs\n",
    "    stats = download_all_pdfs(df, pdf_links_column='pdf_links', output_dir='pdfs', session=session)\n",
    "    \n",
    "    if stats:\n",
    "        print(f\"\\n‚úÖ Download complete!\")\n",
    "        print(f\"   PDFs saved to: pdfs/\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No PDFs were downloaded\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No PDF links found. You need to:\")\n",
    "    print(\"   1. Run a search: df = search_ceqr_by_bbl('3014890011')\")\n",
    "    print(\"   2. Scrape detail pages: df = scrape_all_detail_pages(df, ...)\")\n",
    "    print(\"   3. Then download PDFs: download_all_pdfs(df, ...)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parser loaded (with detail page links)\n"
     ]
    }
   ],
   "source": [
    "def parse_ceqr_results(response):\n",
    "    \"\"\"Extract CEQR results table from HTML response, including detail page links.\"\"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find results table\n",
    "    table = soup.find('table', {'id': lambda x: x and 'grdSearchResults' in x})\n",
    "    \n",
    "    if not table:\n",
    "        # Try finding by content\n",
    "        tables = soup.find_all('table')\n",
    "        for t in tables:\n",
    "            if 'CEQR Number' in t.get_text() or 'Project Name' in t.get_text():\n",
    "                table = t\n",
    "                break\n",
    "    \n",
    "    if not table:\n",
    "        print(\"‚ö†Ô∏è  No results table found\")\n",
    "        return None\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return None\n",
    "    \n",
    "    # Get headers\n",
    "    headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "    \n",
    "    # Get data with detail links\n",
    "    data = []\n",
    "    detail_links = []\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if cells:\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if any(cell.strip() for cell in row_data):\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract detail page link\n",
    "                detail_link = row.find('a', {'id': lambda x: x and 'hlnkOpenDetails' in x})\n",
    "                if detail_link and detail_link.get('href'):\n",
    "                    full_url = f\"https://a002-ceqraccess.nyc.gov/ceqr/{detail_link['href']}\"\n",
    "                    detail_links.append(full_url)\n",
    "                else:\n",
    "                    detail_links.append(\"\")\n",
    "    \n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame with detail links column\n",
    "    df = pd.DataFrame(data, columns=headers[:len(data[0])])\n",
    "    df['Detail Page'] = detail_links\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Parser loaded (with detail page links)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1bc9a",
   "metadata": {},
   "source": [
    "## Test Searches\n",
    "\n",
    "### Test 1: Search by BBL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç BBL 3014890011 ‚Üí Brooklyn, Block 1489, Lot 11\n",
      "üîç Searching: Brooklyn, Block 1489, Lot 11\n",
      "‚úÖ Got session\n",
      "‚úÖ Got results\n",
      "\n",
      "üìä Found 2 results\n",
      "\n",
      "======================================================================================================================================================\n",
      "CEQR Number                                                                                                                                                                                                        Project Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Project Description                                                                                                        Detail Page\n",
      "  06HPD001K                                                          Anchor Sites 2 & 3Latest Milestone02/16/2006 Negative DeclarationFiles06HPD001K_Determination_Of_Significance_02162006156.5KBFiles & Details for 06HPD001K                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Site 1:  New construction 23,456 sf building on 2 floors w/13,422 sf footprint located on the North side of Broadway between Jefferson Avenue and Hancoock Street https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MDZIUEQwMDFL0&signature=bc04541d153279ec253c32419792a4bd91839cff\n",
      "  19HPD057K 1510 BroadwayLatest Milestone11/21/2019 Negative DeclarationFiles19HPD057K_Determination_Of_Significance_10072019227.3KB19HPD057K_Determination_Of_Significance_Errata_11212019231.6KBFiles & Details for 19HPD057K The proposal involves an application by HPD, on behalf of the project sponsor, MacQuesten Development, LLC, for discretionary actions subject to City Planning Commission (CPC) approval (the ‚Äúproposed actions‚Äù) including: zoning map amendment, zoning text amendment, Urban Development Action Area Project (UDAAP) designation, Project Approval and Disposition of City-owned property. The proposed actions will facilitate the development of an 8-story (85-feet tall),\\r\\n112,648 gross square feet (gsf) mixed use building in Brooklyn, Block 1489, Lots 6 and 11-18. The development site is currently vacant except for a one-story garage which would be demolished. The new building would be developed with approximately 97,096 above grade gross square feet (gsf) of residential floor area and approximately 9,987 above grade gsf of retail/commercial space on the first floor on the Broadway frontage. The project would have 108 residential units,\\r\\nincluding 107 affordable units, serving a range of income levels between 27 percent and 80 percent AMI, and one residential unit intended for a live-in superintendent. Building amenities would include bike storage and an outdoor landscaped courtyard on the second floor for residents. The applicant is seeking capital funds from HPD through the Extremely Low & Low-Income Affordability (ELLA) Program. The proposed zoning map amendment would apply to the entirety of Brooklyn Block 1489 and would modify the project area from an R6 zoning district with a C1-3 overlay district to an R7-1 zoning district with a C2-4 overlay district; the project area would also be mapped as a Mandatory Inclusionary Housing (MIH) Area through a text amendment to the New York City Zoning Resolution (ZR) Appendix F. The proposed action is projected to facilitate 27,729 gsf of new development on the lot adjacent to the development site. https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MTlIUEQwNTdL0&signature=8a44e76b0dd63a6a5ef7c569ac7201ef7866a4c6\n",
      "======================================================================================================================================================\n",
      "\n",
      "üîó Detail Pages:\n",
      "  1. https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MDZIUEQwMDFL0&signature=bc04541d153279ec253c32419792a4bd91839cff\n",
      "  2. https://a002-ceqraccess.nyc.gov/ceqr/Details?data=MTlIUEQwNTdL0&signature=8a44e76b0dd63a6a5ef7c569ac7201ef7866a4c6\n"
     ]
    }
   ],
   "source": [
    "df = search_ceqr_by_bbl(\"3014890011\")\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nüìä Found {len(df)} results\\n\")\n",
    "    print(\"=\" * 150)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 150)\n",
    "    \n",
    "    # Show detail links\n",
    "    if 'Detail Page' in df.columns:\n",
    "        print(\"\\nüîó Detail Pages:\")\n",
    "        for idx, link in enumerate(df['Detail Page'], 1):\n",
    "            print(f\"  {idx}. {link}\")\n",
    "else:\n",
    "    print(\"No results found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde04c0",
   "metadata": {},
   "source": [
    "### Test 2: Search by Borough/Block/Lot directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching: Brooklyn, Block 7061, Lot 27\n",
      "‚úÖ Got session\n",
      "‚úÖ Got results\n",
      "‚úÖ Found 2 results using direct search\n"
     ]
    }
   ],
   "source": [
    "# Alternative: search directly by borough/block/lot\n",
    "success, result = search_ceqr(\"Brooklyn\", \"7061\", \"27\")\n",
    "\n",
    "if success:\n",
    "    df = parse_ceqr_results(result)\n",
    "    if df is not None:\n",
    "        print(f\"‚úÖ Found {len(df)} results using direct search\")\n",
    "    else:\n",
    "        print(\"No results\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ee7b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CEQR Number</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Detail Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08DME007K</td>\n",
       "      <td>Coney Island RezoningLatest Milestone12/06/201...</td>\n",
       "      <td>The proposed actions would include zoning map ...</td>\n",
       "      <td>https://a002-ceqraccess.nyc.gov/ceqr/Details?d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24HPD040K</td>\n",
       "      <td>Coney Island Taconic Phase 3Latest Milestone08...</td>\n",
       "      <td>The Department of Housing Preservation and Dev...</td>\n",
       "      <td>https://a002-ceqraccess.nyc.gov/ceqr/Details?d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CEQR Number                                       Project Name  \\\n",
       "0   08DME007K  Coney Island RezoningLatest Milestone12/06/201...   \n",
       "1   24HPD040K  Coney Island Taconic Phase 3Latest Milestone08...   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The proposed actions would include zoning map ...   \n",
       "1  The Department of Housing Preservation and Dev...   \n",
       "\n",
       "                                         Detail Page  \n",
       "0  https://a002-ceqraccess.nyc.gov/ceqr/Details?d...  \n",
       "1  https://a002-ceqraccess.nyc.gov/ceqr/Details?d...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_ceqr_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e23524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
