{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Workflow Notebook\n",
    "\n",
    "End-to-end workflow runner for the housing data pipeline with dataframe views at each step.\n",
    "\n",
    "This notebook wires together the individual steps:\n",
    "1. Verify and fetch HPD data from NYC Open Data\n",
    "2. Classify projects by financing type (LL44 funding)\n",
    "3. Search DOB NB/New Building filings (with optional BBL fallback)\n",
    "4. Search Certificate of Occupancy filings\n",
    "5. Create timeline joins\n",
    "6. Generate charts\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Set your workflow options here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options (equivalent to command line arguments)\n",
    "CONFIG = {\n",
    "    'refresh_hpd': False,  # Set to True to fetch fresh HPD data\n",
    "    'skip_ll44': False,    # Set to True to skip LL44 funding lookup\n",
    "    'skip_dob': False,     # Set to True to skip DOB queries\n",
    "    'skip_co': False,      # Set to True to skip CO queries\n",
    "    'skip_join': False,    # Set to True to skip timeline joins\n",
    "    'skip_charts': False,  # Set to True to skip chart generation\n",
    "    'disable_bbl_fallback': False,  # Set to True to disable BBL fallback\n",
    "    'hpd_csv': None,       # Custom HPD CSV path (None = auto-detect)\n",
    "    'financing_output': None,  # Custom financing output path\n",
    "    'dob_output': None,    # Custom DOB output path\n",
    "    'co_output': None      # Custom CO output path\n",
    "}\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import required modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Import our workflow modules\n",
    "from fetch_affordable_housing_data import update_local_data, verify_and_fetch_hpd_data\n",
    "from query_ll44_funding import query_and_add_financing\n",
    "from query_dob_filings import query_dob_filings\n",
    "from query_co_filings import query_co_filings\n",
    "from HPD_DOB_Join_On_BIN import create_separate_timelines\n",
    "from create_timeline_chart import create_timeline_chart, create_financing_charts\n",
    "from data_quality import quality_tracker\n",
    "\n",
    "print(\"\u2705 All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions needed by the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_hpd_csv() -> Path:\n",
    "    \"\"\"Pick the best-available HPD dataset on disk.\"\"\"\n",
    "    candidates = [\n",
    "        Path(\"data/processed/Affordable_Housing_Production_by_Building_with_financing.csv\"),\n",
    "        Path(\"data/raw/Affordable_Housing_Production_by_Building_with_financing.csv\"),\n",
    "        Path(\"data/raw/Affordable_Housing_Production_by_Building.csv\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return candidates[-1]\n",
    "\n",
    "\n",
    "def _normalize_bin(bin_value) -> Optional[str]:\n",
    "    \"\"\"Normalize BIN to a clean string.\"\"\"\n",
    "    if pd.isna(bin_value):\n",
    "        return None\n",
    "    try:\n",
    "        return str(int(float(bin_value)))\n",
    "    except (TypeError, ValueError):\n",
    "        value = str(bin_value).strip()\n",
    "        return value or None\n",
    "\n",
    "\n",
    "def _write_bin_file(source_csv: Path, output_txt: Path) -> Path:\n",
    "    \"\"\"Extract BINs from a CSV and write them to a text file for CO searches.\"\"\"\n",
    "    df = pd.read_csv(source_csv)\n",
    "    candidate_cols = [col for col in df.columns if col.lower() in (\"bin\", \"bin_normalized\")]\n",
    "    if not candidate_cols:\n",
    "        raise SystemExit(f\"Could not find a BIN column in {source_csv}\")\n",
    "\n",
    "    bins = [_normalize_bin(val) for val in df[candidate_cols[0]].dropna()]\n",
    "    bins = sorted({b for b in bins if b})\n",
    "\n",
    "    output_txt.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_txt.write_text(\"\\n\".join(bins))\n",
    "    print(f\"Wrote {len(bins)} BINs to {output_txt}\")\n",
    "    return output_txt\n",
    "\n",
    "\n",
    "def _require_file(path: Path, description: str) -> None:\n",
    "    \"\"\"Exit with a helpful message if a required file is missing.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise SystemExit(f\"{description} not found at {path}\")\n",
    "\n",
    "print(\"\u2705 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify and Fetch HPD Data\n",
    "\n",
    "Load or refresh the HPD affordable housing dataset from NYC Open Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: VERIFY AND FETCH HPD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start quality tracking\n",
    "quality_tracker.start_processing()\n",
    "\n",
    "if CONFIG[\"refresh_hpd\"]:\n",
    "    print(\"Force refresh requested - fetching fresh HPD data...\")\n",
    "    hpd_df = update_local_data()\n",
    "    hpd_csv = Path(\"data/raw/Affordable_Housing_Production_by_Building.csv\")\n",
    "else:\n",
    "    print(\"Verifying local HPD data against API...\")\n",
    "    hpd_df = verify_and_fetch_hpd_data()\n",
    "    hpd_csv = Path(\"data/raw/Affordable_Housing_Production_by_Building.csv\")\n",
    "\n",
    "_require_file(hpd_csv, \"HPD dataset\")\n",
    "\n",
    "# Record initial dataset size\n",
    "quality_tracker.analyze_hpd_data(hpd_df, \"Full_HPD_Dataset\")\n",
    "quality_tracker.record_pipeline_stage(\"raw_hpd_data\", len(hpd_df), \"Raw HPD affordable housing dataset\")\n",
    "\n",
    "print(f\"\u2705 Step 1 complete: {len(hpd_df):,} records loaded\")\n",
    "print(f\"\ud83d\udcc1 Data saved to: {hpd_csv}\")\n",
    "\n",
    "# Display the dataframe\n",
    "print(\"\\n\ud83d\udd0d HPD Dataset Overview:\")\n",
    "print(f\"Shape: {hpd_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in hpd_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Sample Data:\")\n",
    "display(hpd_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Basic Statistics:\")\n",
    "display(hpd_df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Financing Classification\n",
    "\n",
    "Classify projects by financing type using LL44 funding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: ADD FINANCING CLASSIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if CONFIG[\"skip_ll44\"]:\n",
    "    print(\"Skipping LL44 financing classification as requested.\")\n",
    "    # Still record the stage for tracking\n",
    "    financing_df = hpd_df.copy()\n",
    "    quality_tracker.record_pipeline_stage(\"after_financing_skip\", len(financing_df), \"Financing classification skipped\")\n",
    "    building_csv = hpd_csv\n",
    "else:\n",
    "    financing_output = Path(CONFIG[\"financing_output\"]) if CONFIG[\"financing_output\"] else Path(\n",
    "        \"data/processed/Affordable_Housing_Production_by_Building_with_financing.csv\"\n",
    "    )\n",
    "    financing_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Classifying financing types -> {financing_output}\")\n",
    "    financing_df = query_and_add_financing(str(hpd_csv), output_path=str(financing_output))\n",
    "    building_csv = financing_output\n",
    "\n",
    "    # Record dataset after financing classification\n",
    "    quality_tracker.analyze_hpd_data(financing_df, \"Filtered_HPD\")\n",
    "    quality_tracker.record_pipeline_stage(\"after_financing\", len(financing_df), \"Added LL44 financing classification\")\n",
    "\n",
    "print(f\"\u2705 Step 2 complete: {len(financing_df):,} records with financing classification\")\n",
    "\n",
    "# Display the dataframe with financing info\n",
    "print(\"\\n\ud83d\udd0d Financing Classification Results:\")\n",
    "print(f\"Shape: {financing_df.shape}\")\n",
    "\n",
    "# Check if financing columns were added\n",
    "financing_cols = [col for col in financing_df.columns if \"financ\" in col.lower() or \"ll44\" in col.lower()]\n",
    "print(f\"\\nFinancing-related columns: {financing_cols}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Sample Data with Financing:\")\n",
    "display(financing_df.head())\n",
    "\n",
    "# Show financing type distribution\n",
    "if financing_cols:\n",
    "    for col in financing_cols:\n",
    "        if col in financing_df.columns:\n",
    "            print(f\"\\n\ud83d\udcc8 Distribution of {col}:\")\n",
    "            display(financing_df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Enrich with DOB and CO Data\n",
    "\n",
    "Query DOB APIs for New Building filings and Certificate of Occupancy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: ENRICH WITH DOB AND CO DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare inputs for DOB/CO queries\n",
    "dob_search_source = Path(CONFIG[\"hpd_csv\"]) if CONFIG[\"hpd_csv\"] else building_csv\n",
    "_require_file(dob_search_source, \"DOB search input\")\n",
    "\n",
    "# Generate BIN file for CO searches\n",
    "bin_output = Path(\"data/processed/workflow_bins.txt\")\n",
    "bin_file = _write_bin_file(building_csv, bin_output)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb BIN file created: {bin_file}\")\n",
    "print(f\"Contains {len(bin_file.read_text().split())} BINs\")\n",
    "\n",
    "# DOB filings\n",
    "dob_output = Path(CONFIG[\"dob_output\"]) if CONFIG[\"dob_output\"] else Path(\n",
    "    f\"data/processed/{dob_search_source.stem}_dob_filings.csv\"\n",
    ")\n",
    "dob_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check for existing DOB files when skipping\n",
    "if CONFIG[\"skip_dob\"]:\n",
    "    print(\"\u23ed\ufe0f  Skipping DOB queries as requested\")\n",
    "    # Look for existing files in processed folder or external folder\n",
    "    alt_dob_path = Path(f\"data/external/{dob_search_source.stem}_dob_filings.csv\")\n",
    "    if dob_output.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing DOB data at {dob_output}\")\n",
    "        dob_df = pd.read_csv(dob_output)\n",
    "    elif alt_dob_path.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing DOB data from external folder: {alt_dob_path}\")\n",
    "        dob_output = alt_dob_path\n",
    "        dob_df = pd.read_csv(dob_output)\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No existing DOB data found; timeline will omit DOB entries.\")\n",
    "        dob_df = None\n",
    "        dob_output = None\n",
    "else:\n",
    "    print(f\"\ud83d\udd0d Querying DOB APIs using {dob_search_source} -> {dob_output}\")\n",
    "    print(\"   This may take several minutes...\")\n",
    "    query_dob_filings(\n",
    "        str(dob_search_source),\n",
    "        output_path=str(dob_output),\n",
    "        use_bbl_fallback=not CONFIG[\"disable_bbl_fallback\"],\n",
    "    )\n",
    "    print(f\"\u2705 DOB query completed: {dob_output}\")\n",
    "    dob_df = pd.read_csv(dob_output)\n",
    "\n",
    "# Display DOB data if available\n",
    "if dob_df is not None:\n",
    "    print(\"\\n\ud83d\udd0d DOB Filings Data:\")\n",
    "    print(f\"Shape: {dob_df.shape}\")\n",
    "    print(\"Columns:\")\n",
    "    for col in dob_df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Sample DOB Data:\")\n",
    "    display(dob_df.head())\n",
    "    \n",
    "    # Show some statistics\n",
    "    if \"filing_date\" in dob_df.columns:\n",
    "        print(\"\\n\ud83d\udcc8 DOB Filing Date Statistics:\")\n",
    "        display(dob_df[\"filing_date\"].describe())\n",
    "\n",
    "# Certificate of Occupancy filings\n",
    "co_output = Path(CONFIG[\"co_output\"]) if CONFIG[\"co_output\"] else Path(\n",
    "    f\"data/processed/{bin_file.stem}_co_filings.csv\"\n",
    ")\n",
    "co_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if CONFIG[\"skip_co\"]:\n",
    "    # Look for existing CO files in multiple locations\n",
    "    alt_co_path = Path(f\"data/external/{bin_file.stem}_co_filings.csv\")\n",
    "    if co_output.exists():\n",
    "        print(f\"Using existing CO data at {co_output}\")\n",
    "        co_df = pd.read_csv(co_output)\n",
    "    elif alt_co_path.exists():\n",
    "        print(f\"Using existing CO data from external folder: {alt_co_path}\")\n",
    "        co_output = alt_co_path\n",
    "        co_df = pd.read_csv(co_output)\n",
    "    else:\n",
    "        print(\"No CO data supplied; timeline will omit CO entries.\")\n",
    "        co_df = None\n",
    "        co_output = None\n",
    "else:\n",
    "    print(f\"\ud83c\udfdb\ufe0f  Querying CO APIs using {bin_file} -> {co_output}\")\n",
    "    query_co_filings(str(bin_file), output_path=str(co_output))\n",
    "    co_df = pd.read_csv(co_output)\n",
    "\n",
    "# Display CO data if available\n",
    "if co_df is not None:\n",
    "    print(\"\\n\ud83d\udd0d Certificate of Occupancy Data:\")\n",
    "    print(f\"Shape: {co_df.shape}\")\n",
    "    print(\"Columns:\")\n",
    "    for col in co_df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Sample CO Data:\")\n",
    "    display(co_df.head())\n",
    "    \n",
    "    # Show some statistics\n",
    "    if \"issue_date\" in co_df.columns:\n",
    "        print(\"\\n\ud83d\udcc8 CO Issue Date Statistics:\")\n",
    "        display(co_df[\"issue_date\"].describe())\n",
    "\n",
    "# Record final enriched dataset\n",
    "enriched_df = pd.read_csv(building_csv)\n",
    "if dob_output is not None and dob_output.exists():\n",
    "    quality_tracker.record_pipeline_stage(\"after_dob_enrichment\", len(enriched_df), \"Enriched with DOB and CO data\")\n",
    "else:\n",
    "    quality_tracker.record_pipeline_stage(\"after_dob_enrichment\", len(enriched_df), \"DOB/CO enrichment skipped - no data available\")\n",
    "\n",
    "print(\"\\n\u2705 Step 3 complete: Dataset enriched with DOB/CO data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Timeline Charts\n",
    "\n",
    "Create timeline visualizations and charts from the enriched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: GENERATE TIMELINE CHARTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if CONFIG[\"skip_charts\"]:\n",
    "    print(\"Skipping chart generation as requested.\")\n",
    "else:\n",
    "    # Timeline join\n",
    "    hpd_timeline = Path(str(building_csv).replace(\".csv\", \"_hpd_financed_timeline.csv\"))\n",
    "    private_timeline = Path(str(building_csv).replace(\".csv\", \"_privately_financed_timeline.csv\"))\n",
    "\n",
    "    if CONFIG[\"skip_join\"]:\n",
    "        print(\"\u23ed\ufe0f  Skipping timeline join step.\")\n",
    "    else:\n",
    "        if dob_output is None:\n",
    "            print(\"\u26a0\ufe0f  No DOB data available; skipping timeline creation.\")\n",
    "        else:\n",
    "            _require_file(dob_output, \"DOB filings CSV\")\n",
    "            print(\"\ud83d\udd17 Building timelines...\")\n",
    "            create_separate_timelines(\n",
    "                str(building_csv),\n",
    "                str(dob_output),\n",
    "                str(co_output) if co_output else None,\n",
    "            )\n",
    "            \n",
    "            # Load and display timeline data\n",
    "            if hpd_timeline.exists():\n",
    "                hpd_timeline_df = pd.read_csv(hpd_timeline)\n",
    "                print(f\"\\n\ud83d\udcca HPD Financed Timeline Data ({hpd_timeline_df.shape[0]} records):\")\n",
    "                display(hpd_timeline_df.head())\n",
    "                \n",
    "                # Show event type distribution\n",
    "                if \"event_type\" in hpd_timeline_df.columns:\n",
    "                    print(\"\\n\ud83d\udcc8 Event Types in HPD Timeline:\")\n",
    "                    display(hpd_timeline_df[\"event_type\"].value_counts())\n",
    "            \n",
    "            if private_timeline.exists():\n",
    "                private_timeline_df = pd.read_csv(private_timeline)\n",
    "                print(f\"\\n\ud83d\udcca Privately Financed Timeline Data ({private_timeline_df.shape[0]} records):\")\n",
    "                display(private_timeline_df.head())\n",
    "                \n",
    "                # Show event type distribution\n",
    "                if \"event_type\" in private_timeline_df.columns:\n",
    "                    print(\"\\n\ud83d\udcc8 Event Types in Private Timeline:\")\n",
    "                    display(private_timeline_df[\"event_type\"].value_counts())\n",
    "\n",
    "    # Charts\n",
    "    print(\"\\n\ud83d\udcc8 Generating charts...\")\n",
    "    default_timeline_stem = \"Affordable_Housing_Production_by_Building_with_financing\"\n",
    "    if Path(building_csv).name == f\"{default_timeline_stem}.csv\":\n",
    "        create_financing_charts()\n",
    "        print(\"\u2705 Created financing-specific charts\")\n",
    "    else:\n",
    "        if hpd_timeline.exists():\n",
    "            create_timeline_chart(str(hpd_timeline))\n",
    "            print(f\"\u2705 Created HPD financed timeline chart\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f  No HPD financed timeline found at {hpd_timeline}; skipping.\")\n",
    "\n",
    "        if private_timeline.exists():\n",
    "            create_timeline_chart(str(private_timeline))\n",
    "            print(f\"\u2705 Created privately financed timeline chart\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f  No privately financed timeline found at {private_timeline}; skipping.\")\n",
    "\n",
    "print(\"\\n\u2705 Step 4 complete: Charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Quality Report\n",
    "\n",
    "Generate the final data quality report and workflow summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\ud83d\udcca GENERATING FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate final data quality report and Sankey diagram\n",
    "quality_tracker.end_processing()\n",
    "report_filename = quality_tracker.save_report_to_file(\"notebook_workflow\")\n",
    "sankey_filename = quality_tracker.generate_sankey_diagram()\n",
    "quality_tracker.print_report()\n",
    "\n",
    "print(\"\\n\ud83c\udf89 WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"\ud83d\udcca Data quality report: {report_filename}\")\n",
    "if sankey_filename:\n",
    "    print(f\"\ud83d\udcca Sankey diagram: {sankey_filename}\")\n",
    "\n",
    "# Summary of what we accomplished\n",
    "print(\"\\n\ud83d\udccb WORKFLOW SUMMARY:\")\n",
    "print(f\"\u2022 HPD Records Processed: {len(hpd_df):,}\")\n",
    "print(f\"\u2022 Records with Financing: {len(financing_df):,}\")\n",
    "if \"dob_df\" in locals() and dob_df is not None:\n",
    "    print(f\"\u2022 DOB Filings Found: {len(dob_df):,}\")\n",
    "if \"co_df\" in locals() and co_df is not None:\n",
    "    print(f\"\u2022 CO Filings Found: {len(co_df):,}\")\n",
    "\n",
    "print(\"\\n\u2705 Notebook workflow complete! All dataframes have been displayed for inspection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}