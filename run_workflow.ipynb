{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Workflow Notebook\n",
    "\n",
    "Modular workflow where you can run individual steps independently.\n",
    "Run cells in order or skip any steps you don't need.\n",
    "\n",
    "Each step shows dataframe views and statistics for inspection.\n",
    "\n",
    "## Quick Start\n",
    "- Run **Setup** cell first\n",
    "- Then run any combination of Step 1-4 cells\n",
    "- Skip cells you don't want to execute\n",
    "- Each cell is self-contained and shows results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup\n",
    "\n",
    "Run this cell first to import modules and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Import our workflow modules\n",
    "from fetch_affordable_housing_data import update_local_data, verify_and_fetch_hpd_data\n",
    "from query_ll44_funding import query_and_add_financing\n",
    "from query_dob_filings import query_dob_bisweb_bin, query_dob_bisweb_bbl, query_dobnow_bin, query_dobnow_bbl, decompose_bbl, query_condo_lots_for_bbl, query_dob_by_address, pad_block, pad_lot\n",
    "from query_co_filings import query_co_filings\n",
    "from HPD_DOB_Join_On_BIN import create_separate_timelines\n",
    "from create_timeline_chart import create_timeline_chart, create_financing_charts\n",
    "from data_quality import quality_tracker\n",
    "\n",
    "print(\"\u2705 All imports successful\")\n",
    "\n",
    "# Helper functions\n",
    "def _normalize_bin(bin_value) -> Optional[str]:\n",
    "    \"\"\"Normalize BIN to a clean string.\"\"\"\n",
    "    if pd.isna(bin_value):\n",
    "        return None\n",
    "    try:\n",
    "        return str(int(float(bin_value)))\n",
    "    except (TypeError, ValueError):\n",
    "        value = str(bin_value).strip()\n",
    "        return value or None\n",
    "\n",
    "def _write_bin_file(source_csv: Path, output_txt: Path) -> Path:\n",
    "    \"\"\"Extract BINs from a CSV and write them to a text file for CO searches.\"\"\"\n",
    "    df = pd.read_csv(source_csv)\n",
    "    candidate_cols = [col for col in df.columns if col.lower() in (\"bin\", \"bin_normalized\")]\n",
    "    if not candidate_cols:\n",
    "        raise SystemExit(f\"Could not find a BIN column in {source_csv}\")\n",
    "\n",
    "    bins = [_normalize_bin(val) for val in df[candidate_cols[0]].dropna()]\n",
    "    bins = sorted({b for b in bins if b})\n",
    "\n",
    "    output_txt.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_txt.write_text(\"\\n\".join(bins))\n",
    "    print(f\"Wrote {len(bins)} BINs to {output_txt}\")\n",
    "    return output_txt\n",
    "\n",
    "print(\"\u2705 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce5 Step 1: Fetch HPD Data\n",
    "\n",
    "Load or refresh the HPD affordable housing dataset.\n",
    "\n",
    "**Options:**\n",
    "- Set `refresh_data = True` to fetch fresh data\n",
    "- Set `refresh_data = False` to use existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Configuration\n",
    "refresh_data = False  # Set to True to fetch fresh HPD data\n",
    "hpd_output_path = \"data/raw/Affordable_Housing_Production_by_Building.csv\"  # Output path for HPD data\n",
    "refresh_hpd_projects = False  # Set to True to fetch fresh HPD projects data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: FETCH HPD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start quality tracking\n",
    "\n",
    "# Handle HPD projects cache refresh if requested\n",
    "if refresh_hpd_projects:\n",
    "    print(\"Force refreshing HPD projects cache...\")\n",
    "    from fetch_affordable_housing_data import verify_and_fetch_hpd_projects_data\n",
    "    hpd_projects_df, hpd_projects_path = verify_and_fetch_hpd_projects_data(use_existing=False)\n",
    "    print(f\"HPD projects cache refreshed: {len(hpd_projects_df)} records\\n\")\n",
    "\n",
    "quality_tracker.start_processing()\n",
    "\n",
    "if refresh_data:\n",
    "    print(\"Fetching fresh HPD data from NYC Open Data...\")\n",
    "    hpd_df, hpd_csv = update_local_data(hpd_output_path)\n",
    "else:\n",
    "    print(\"Verifying local HPD data against API...\")\n",
    "    hpd_df, hpd_csv = verify_and_fetch_hpd_data(output_path=hpd_output_path, use_projects_cache=not refresh_hpd_projects)\n",
    "\n",
    "if not hpd_csv.exists():\n",
    "    raise SystemExit(f\"HPD dataset not found at {hpd_csv}\")\n",
    "\n",
    "# Record initial dataset size\n",
    "\n",
    "# Get total units before filter\n",
    "original_count = len(hpd_df)\n",
    "original_units = hpd_df['Total Units'].sum()\n",
    "\n",
    "# Filter to New Construction only\n",
    "hpd_df = hpd_df[hpd_df[\"Reporting Construction Type\"] == \"New Construction\"].copy()\n",
    "\n",
    "filtered_count = len(hpd_df)\n",
    "filtered_units = hpd_df['Total Units'].sum()\n",
    "filtered_out = original_count - filtered_count\n",
    "filtered_units_out = original_units - filtered_units\n",
    "\n",
    "print(f\"\ud83c\udfd7\ufe0f Filtered to New Construction only:\")\n",
    "print(f\"  Original: {original_count:,} projects, {original_units:,} total units\")\n",
    "print(f\"  Filtered: {filtered_count:,} projects ({filtered_count/original_count*100:.1f}%), {filtered_units:,} total units ({filtered_units/original_units*100:.1f}%)\")\n",
    "print(f\"  Removed: {filtered_out:,} non-new construction projects ({filtered_out/original_count*100:.1f}%), {filtered_units_out:,} units filtered out ({filtered_units_out/original_units*100:.1f}%)\")\n",
    "\n",
    "quality_tracker.analyze_hpd_data(hpd_df, \"Full_HPD_Dataset\")\n",
    "quality_tracker.record_pipeline_stage(\"raw_hpd_data\", len(hpd_df), \"Raw HPD affordable housing dataset\")\n",
    "\n",
    "print(f\"\u2705 Step 1 complete: {len(hpd_df):,} records loaded\")\n",
    "print(f\"\ud83d\udcc1 Data location: {hpd_csv}\")\n",
    "\n",
    "# Display the dataframe\n",
    "print(\"\\n\ud83d\udd0d HPD Dataset Overview:\")\n",
    "print(f\"Shape: {hpd_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in hpd_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Sample Data:\")\n",
    "display(hpd_df.head())\n",
    "print(\"\\n\ud83d\udcc8 Basic Statistics:\")\n",
    "display(hpd_df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique counts are there by project id as primary key per program group,\n",
    "# and show total units in parentheticals (but NOT for the unique project counts).\n",
    "\n",
    "# Compute total units per Program Group (all rows)\n",
    "units_per_group = hpd_df.groupby('Program Group')['Total Units'].sum()\n",
    "\n",
    "print(\"Program Group counts (raw rows) (total units in parentheses):\")\n",
    "raw_row_counts = hpd_df['Program Group'].value_counts()\n",
    "for group, count in raw_row_counts.items():\n",
    "    units = units_per_group.get(group, 0)\n",
    "    print(f\"{group}: {count} rows ({units} units)\")\n",
    "print()\n",
    "\n",
    "# Group by Program Group, count unique Project IDs\n",
    "unique_proj_counts = hpd_df.groupby('Program Group')['Project ID'].nunique().sort_values(ascending=False)\n",
    "unique_proj_ids = (\n",
    "    hpd_df\n",
    "    .groupby('Program Group')\n",
    "    .apply(lambda df: df['Project ID'].unique())\n",
    ")\n",
    "\n",
    "print(\"Program Group counts (unique Project ID as primary key):\")\n",
    "for group, count in unique_proj_counts.items():\n",
    "    print(f\"{group}: {count} projects\")\n",
    "print()\n",
    "\n",
    "print(\"\\nTax Abatement by Program Group (based on unique Project ID):\")\n",
    "if 'Planned Tax Benefit' in hpd_df.columns:\n",
    "    # For this, deduplicate by Project ID first\n",
    "    unique_project_rows = hpd_df.drop_duplicates(subset=['Project ID'])\n",
    "    tax_abate_ct = (\n",
    "        unique_project_rows\n",
    "        .groupby('Program Group')['Planned Tax Benefit']\n",
    "        .value_counts(dropna=False)\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    # Also display total units per Program Group in this table, if desired\n",
    "    units_per_group_project = unique_project_rows.groupby('Program Group')['Total Units'].sum()\n",
    "    print(\"Total units (unique Project ID per Program Group):\")\n",
    "    display(units_per_group_project)\n",
    "    display(tax_abate_ct)\n",
    "else:\n",
    "    print(\"Column 'Planned Tax Benefit' not found in dataset.\")\n",
    "\n",
    "# Make a version of this with unit count by program and tax benefit\n",
    "if 'Planned Tax Benefit' in unique_project_rows.columns and 'Program Group' in unique_project_rows.columns:\n",
    "    units_pivot = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Total units by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(units_pivot)\n",
    "else:\n",
    "    print(\"Required columns not found for unit pivot table.\")\n",
    "\n",
    "# Calculate average units per year by Program Group and Planned Tax Benefit\n",
    "\n",
    "if 'Project Start Date' in unique_project_rows.columns and 'Total Units' in unique_project_rows.columns:\n",
    "    # Extract year from 'Project Start Date'\n",
    "    unique_project_rows = unique_project_rows.copy()\n",
    "    unique_project_rows['Project Year'] = pd.to_datetime(unique_project_rows['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    avg_units_per_year = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit', 'Project Year'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Now calculate the average units per year by program group and tax abatement\n",
    "    avg_units_table = (\n",
    "        avg_units_per_year\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Average units per year by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(avg_units_table)\n",
    "else:\n",
    "    print(\"Required columns not found for average units per year table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use the full raw HPD data, because we want all programs, not just Multifamily Finance Program\n",
    "if 'Project Start Date' in hpd_df.columns and 'Total Units' in hpd_df.columns:\n",
    "    hpd_bar_df = hpd_df.copy()\n",
    "    hpd_bar_df['Project Year'] = pd.to_datetime(hpd_bar_df['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    # Only focus on desired groups\n",
    "    programs_of_interest = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "    mask = hpd_bar_df['Program Group'].isin(programs_of_interest)\n",
    "    hpd_bar_df = hpd_bar_df[mask & hpd_bar_df['Project Year'].notna()]\n",
    "\n",
    "    # Fill NAs in Planned Tax Benefit with \"None\"\n",
    "    hpd_bar_df['Planned Tax Benefit'] = hpd_bar_df['Planned Tax Benefit'].fillna('None')\n",
    "\n",
    "    # Prepare for grouped bar with stack\n",
    "    # Pivot: rows = Project Year, columns = (Program Group, Planned Tax Benefit), values = sum of units\n",
    "    pivot = (\n",
    "        hpd_bar_df\n",
    "        .groupby(['Project Year', 'Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure proper order of years and programs\n",
    "    years = sorted(pivot['Project Year'].dropna().unique())\n",
    "    tax_benefits = sorted(pivot['Planned Tax Benefit'].unique())\n",
    "    # Keep consistent order for bars\n",
    "    program_order = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "\n",
    "    # Prepare data structure: for each year, for each program, get breakdown by tax benefit\n",
    "    bar_data = {}\n",
    "    for year in years:\n",
    "        bar_data[year] = {}\n",
    "        for prog in program_order:\n",
    "            mask = (pivot['Project Year'] == year) & (pivot['Program Group'] == prog)\n",
    "            year_prog_data = pivot[mask].set_index('Planned Tax Benefit')['Total Units'].reindex(tax_benefits, fill_value=0)\n",
    "            bar_data[year][prog] = year_prog_data.values\n",
    "\n",
    "    # Number of bars per group (2 programs), group by year, stacked by tax benefit\n",
    "    x = range(len(years))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Colors for planned tax benefits\n",
    "    import matplotlib.cm as cm\n",
    "    color_map = cm.get_cmap('tab20', len(tax_benefits))\n",
    "    colors = [color_map(i) for i in range(len(tax_benefits))]\n",
    "\n",
    "    bottoms_p1 = [0] * len(years)\n",
    "    bottoms_p2 = [0] * len(years)\n",
    "\n",
    "    # For each tax benefit, draw the stack pieces for both programs\n",
    "    legend_handles = []\n",
    "    for idx, tax in enumerate(tax_benefits):\n",
    "        values_p1 = [bar_data[year][program_order[0]][idx] for year in years]\n",
    "        values_p2 = [bar_data[year][program_order[1]][idx] for year in years]\n",
    "\n",
    "        bar1 = ax.bar(\n",
    "            [i - width/2 for i in x], values_p1, width,\n",
    "            bottom=bottoms_p1, color=colors[idx],\n",
    "            label=tax if (tax not in [h.get_label() for h in legend_handles]) else None,\n",
    "            edgecolor='black', hatch='////'\n",
    "        )\n",
    "        bar2 = ax.bar(\n",
    "            [i + width/2 for i in x], values_p2, width,\n",
    "            bottom=bottoms_p2, color=colors[idx],\n",
    "            label=None,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        if tax not in [h.get_label() for h in legend_handles]:\n",
    "            legend_handles.append(bar1)\n",
    "\n",
    "        bottoms_p1 = [b + v for b, v in zip(bottoms_p1, values_p1)]\n",
    "        bottoms_p2 = [b + v for b, v in zip(bottoms_p2, values_p2)]\n",
    "\n",
    "    # Add year labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(int(y)) for y in years], rotation=45)\n",
    "    ax.set_xlabel(\"Project Start Year\")\n",
    "    ax.set_ylabel(\"Total Units Financed\")\n",
    "    ax.set_title(\"Units Financed by Year: Multifamily Finance and Incentives Programs\\nColored by Planned Tax Benefit\")\n",
    "\n",
    "    # Custom legend for program groups\n",
    "    import matplotlib.patches as mpatches\n",
    "    progs = [\n",
    "        mpatches.Patch(color='gray', label='Multifamily Finance Program', ec='black', hatch='////'),\n",
    "        mpatches.Patch(color='gray', label='Multifamily Incentives Program', ec='black')\n",
    "    ]\n",
    "    # Only add one legend for planned tax benefit\n",
    "    handles_tax = [plt.Rectangle((0,0),1,1, color=colors[i], edgecolor='black', label=f\"{tax_benefits[i]}\") for i in range(len(tax_benefits))]\n",
    "    legend1 = ax.legend(handles=handles_tax, title=\"Planned Tax Benefit\", loc='upper right')\n",
    "    ax.add_artist(legend1)\n",
    "    # Add manual tick legend for program bars\n",
    "    bar_locs = [x[0] - width/2, x[0] + width/2]\n",
    "    ax.bar(bar_locs[0], 0, width, color='white', hatch='////', ec='black', label='Multifamily Finance Program')\n",
    "    ax.bar(bar_locs[1], 0, width, color='white', ec='black', label='Multifamily Incentives Program')\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', hatch='////', edgecolor='black', label='Multifamily Finance Program'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', edgecolor='black', label='Multifamily Incentives Program')\n",
    "        ], title=\"Program Group\", loc='upper left'\n",
    "    )\n",
    "\n",
    "    ax.grid(True, which='major', axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Required columns ('Project Start Date', 'Total Units') not found in HPD Data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and sample: Planned Tax Benefit '421a' and Project Start Date in 2025\n",
    "if \"Planned Tax Benefit\" in hpd_df.columns and \"Project Start Date\" in hpd_df.columns:\n",
    "    # Filter for 421a and 2025 start year\n",
    "    mask_421a_2025 = (\n",
    "        (hpd_df[\"Planned Tax Benefit\"] == \"421a\") &\n",
    "        (hpd_df[\"Project Start Date\"].astype(str).str.startswith(\"2025\"))\n",
    "    )\n",
    "    df_421a_2025 = hpd_df[mask_421a_2025]\n",
    "\n",
    "    # Count unique projects (by Project ID), and total units\n",
    "    total_projects = df_421a_2025[\"Project ID\"].nunique() if \"Project ID\" in df_421a_2025.columns else len(df_421a_2025)\n",
    "    total_units = df_421a_2025[\"Total Units\"].sum() if \"Total Units\" in df_421a_2025.columns else \"N/A\"\n",
    "\n",
    "    print(f\"Total projects with Planned Tax Benefit '421a' and 2025 Start Date: {total_projects:,}\")\n",
    "    print(f\"Total units in these projects: {total_units:,}\")\n",
    "\n",
    "    # Show up to 5 sample rows\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(\"\\nSample 421a Planned Tax Benefit projects with Project Start Date in 2025:\")\n",
    "    display(df_421a_2025.head(5))\n",
    "    pd.reset_option('display.max_columns')\n",
    "else:\n",
    "    print(\"One or both of the columns 'Planned Tax Benefit' or 'Project Start Date' not found in HPD DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Step 3A: Query DOB Filings\n",
    "\n",
    "Search for DOB New Building filings.\n",
    "\n",
    "**Depends on:** Step 2\n",
    "**Options:**\n",
    "- Set `skip_dob = True` to use existing DOB data\n",
    "- Set `use_bbl_fallback = False` to disable BBL fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A Configuration\n",
    "\n",
    "# We now explicitly call all 4 DOB query types for transparency\n",
    "\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "dob_output_path = None  # Optional explicit DOB output path\n",
    "building_csv = hpd_csv  # Use HPD data directly (skipping financing step)\n",
    "\n",
    "# Filter to Multifamily Finance Program only\n",
    "print(f\"Reading HPD data from {building_csv}\")\n",
    "hpd_df = pd.read_csv(building_csv)\n",
    "original_count = len(hpd_df)\n",
    "print(f\"Loaded {original_count:,} total buildings\")\n",
    "\n",
    "# Apply filters: New Construction + Multifamily Finance Program\n",
    "filtered_df = hpd_df[\n",
    "    (hpd_df[\"Reporting Construction Type\"] == \"New Construction\") &\n",
    "    (hpd_df[\"Program Group\"] == \"Multifamily Finance Program\")\n",
    "]\n",
    "filtered_count = len(filtered_df)\n",
    "\n",
    "print(f\"\ud83c\udfd7\ufe0f Filtered to Multifamily Finance Program:\")\n",
    "print(f\"  Original: {original_count:,} buildings\")\n",
    "print(f\"  Filtered: {filtered_count:,} buildings ({filtered_count/original_count*100:.1f}%)\")\n",
    "\n",
    "# Save filtered data for DOB processing\n",
    "temp_fd, temp_path = tempfile.mkstemp(suffix=\"_multifamily_finance.csv\")\n",
    "os.close(temp_fd)\n",
    "filtered_df.to_csv(temp_path, index=False)\n",
    "building_csv = Path(temp_path)\n",
    "print(f\"\ud83d\udcc1 Using filtered dataset: {filtered_count:,} Multifamily Finance Program buildings\")\n",
    "\n",
    "dob_output_base = Path(dob_output_path) if dob_output_path else Path(\n",
    "    f\"data/processed/{building_csv.stem}_dob\"\n",
    ")\n",
    "dob_output_base.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract BINs and BBLs from the filtered data\n",
    "bins = []\n",
    "# Extract BBLs properly using decompose_bbl function\n",
    "from query_dob_filings import decompose_bbl\n",
    "\n",
    "bbls = []\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    if pd.notna(row.get(\"BBL\")):\n",
    "        bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "        if bbl_result and len(bbl_result) >= 3:\n",
    "            borough, block, lot = bbl_result\n",
    "            bbls.append((borough, block, lot))\n",
    "\n",
    "\n",
    "# Filter out bad/placeholder BINs (e.g., 1000000, 2000000, 3000000, 4000000, 5000000)\n",
    "# These are placeholder values that don't exist in DOB\n",
    "def is_bad_bin(bin_str):\n",
    "    \"\"\"Check if BIN is a placeholder/bad value.\"\"\"\n",
    "    if not bin_str or pd.isna(bin_str) or str(bin_str).lower() == 'nan':\n",
    "        return True\n",
    "    bin_str_clean = str(bin_str).strip()\n",
    "    # Check for pattern: [1-5]000000 (borough placeholder BINs)\n",
    "    if bin_str_clean in ['1000000', '2000000', '3000000', '4000000', '5000000']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if 'BIN' in filtered_df.columns:\n",
    "    bins = [str(b).replace('.0', '') for b in filtered_df['BIN'].dropna() if str(b) != 'nan' and not is_bad_bin(str(b).replace('.0', ''))]\n",
    "elif 'BIN' in filtered_df.columns:\n",
    "    bins = [str(b).replace('.0', '') for b in filtered_df['BIN'].dropna() if str(b) != 'nan']\n",
    "\n",
    "if 'BBL' in filtered_df.columns:\n",
    "    bbl_col = filtered_df['BBL'].astype(str).str.zfill(10)\n",
    "    bbls = [\n",
    "        (\n",
    "            bbl_val[0],                     # borough code (as string)\n",
    "            bbl_val[1:6],                   # block (padded 5 chars)\n",
    "            bbl_val[6:]                     # lot   (padded 4 chars)\n",
    "        )\n",
    "        for bbl_val in bbl_col\n",
    "        if len(bbl_val) == 10\n",
    "    ]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3A: QUERY DOB FILINGS - Explicitly by API and key\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Implement proper API-specific fallback logic\n",
    "\n",
    "# 1. BISWEB BIN for all buildings\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 3A: BISWEB BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "dob_bisweb_bin_path = dob_output_base.with_name(dob_output_base.stem + \"_bisweb_bin.csv\")\n",
    "print(f\"\u25b6\ufe0f Querying BISWEB BIN for {len(bins)} buildings...\")\n",
    "dob_bisweb_bin_df = query_dob_bisweb_bin(bins)\n",
    "bisweb_bin_matches = set()\n",
    "if not dob_bisweb_bin_df.empty and \"bin__\" in dob_bisweb_bin_df.columns:\n",
    "    bisweb_bin_matches = set(dob_bisweb_bin_df[\"bin__\"].dropna().astype(str).unique())\n",
    "bisweb_bin_unmatched = [b for b in bins if b not in bisweb_bin_matches]\n",
    "print(f\"BISWEB BIN: {len(bisweb_bin_matches)} matches, {len(bisweb_bin_unmatched)} need BBL fallback\")\n",
    "dob_bisweb_bin_df.to_csv(dob_bisweb_bin_path, index=False)\n",
    "\n",
    "\n",
    "# 3. DOB NOW BIN for all buildings\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3B: DOB NOW BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "dob_now_bin_path = dob_output_base.with_name(dob_output_base.stem + \"_now_bin.csv\")\n",
    "print(f\"\u25b6\ufe0f Querying DOB NOW BIN for {len(bins)} buildings...\")\n",
    "dob_now_bin_df = query_dobnow_bin(bins)\n",
    "dobnow_bin_matches = set()\n",
    "if not dob_now_bin_df.empty and \"bin\" in dob_now_bin_df.columns:\n",
    "    dobnow_bin_matches = set(dob_now_bin_df[\"bin\"].dropna().astype(str).unique())\n",
    "print(f\"DOB NOW BIN: {len(dobnow_bin_matches)} matches\")\n",
    "dob_now_bin_df.to_csv(dob_now_bin_path, index=False)\n",
    "\n",
    "# 4. Combined BBL fallback for buildings that failed ALL BIN searches\n",
    "all_bin_matches = bisweb_bin_matches.union(dobnow_bin_matches)\n",
    "all_bin_unmatched = [b for b in bins if b not in all_bin_matches]\n",
    "print(f\"Combined BIN search: {len(all_bin_matches)} total matches, {len(all_bin_unmatched)} need BBL fallback\")\n",
    "\n",
    "# BISWEB BBL fallback\n",
    "if all_bin_unmatched:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3C: BISWEB BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_bisweb_bbl_path = dob_output_base.with_name(dob_output_base.stem + \"_bisweb_bbl.csv\")\n",
    "    bbl_tuples = []\n",
    "    # Also include projects with bad BINs in BBL queries\n",
    "    # These projects should skip BIN queries and go straight to BBL\n",
    "    bad_bin_projects = filtered_df[\n",
    "        filtered_df['BIN'].astype(str).str.replace('.0', '').apply(is_bad_bin)\n",
    "    ]\n",
    "    if not bad_bin_projects.empty:\n",
    "        print(f\"Found {len(bad_bin_projects)} projects with bad/placeholder BINs - adding to BBL queries\")\n",
    "        for _, row in bad_bin_projects.iterrows():\n",
    "            if pd.notna(row.get(\"BBL\")):\n",
    "                bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "                if bbl_result and len(bbl_result) >= 3:\n",
    "                    bbl_tuples.append(bbl_result)\n",
    "\n",
    "    print(f\"\u25b6\ufe0f Querying BISWEB BBL for {len(all_bin_unmatched)} buildings...\")\n",
    "    for bin_val in all_bin_unmatched:\n",
    "        matching_rows = filtered_df[\n",
    "            (filtered_df[\"BIN\"].astype(str).str.replace(\".0\", \"\") == bin_val)\n",
    "        ]\n",
    "        if not matching_rows.empty:\n",
    "            # Iterate through ALL matching rows, not just the first\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                if pd.notna(row.get(\"BBL\")):\n",
    "                    bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "                    if bbl_result and len(bbl_result) >= 3:\n",
    "                        bbl_tuples.append(bbl_result)\n",
    "    bbl_tuples = list(set(bbl_tuples))\n",
    "    print(f\"Deduplicated to {len(bbl_tuples)} unique BBLs\")\n",
    "    dob_bisweb_bbl_df = query_dob_bisweb_bbl(bbl_tuples)\n",
    "    \n",
    "    # Try condo billing BBLs for BBLs that didn't match\n",
    "    # Track which BBLs matched in the initial query\n",
    "        # Reconstruct BBLs from DOB results to see which ones matched\n",
    "        \n",
    "        # Get matched BBL tuples\n",
    "    \n",
    "        \n",
    "            # Combine with existing results\n",
    "    \n",
    "    \n",
    "    # This catches cases where permits are on billing BBLs (lot 7501) instead of base lots\n",
    "    \n",
    "        # Combine with existing results\n",
    "    \n",
    "    dob_bisweb_bbl_df.to_csv(dob_bisweb_bbl_path, index=False)\n",
    "    \n",
    "    # DOB NOW BBL fallback\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3D: DOB NOW BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_now_bbl_path = dob_output_base.with_name(dob_output_base.stem + \"_now_bbl.csv\")\n",
    "    dob_now_bbl_df = query_dobnow_bbl(bbl_tuples)\n",
    "    dob_now_bbl_df.to_csv(dob_now_bbl_path, index=False)\n",
    "# Display results summary\n",
    "    \n",
    "    # Try condo billing BBLs only for BBLs that didn't match in either BISWEB or DOB NOW\n",
    "    # Track which BBLs matched\n",
    "    \n",
    "    # Track which BBLs matched in BISWEB or DOB NOW\n",
    "    matched_bbl_tuples = set()\n",
    "    \n",
    "    # Get matched BBLs from BISWEB results\n",
    "    if not dob_bisweb_bbl_df.empty:\n",
    "        for _, row in dob_bisweb_bbl_df.iterrows():\n",
    "            if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "    \n",
    "    # Get matched BBLs from DOB NOW results\n",
    "    if not dob_now_bbl_df.empty:\n",
    "        for _, row in dob_now_bbl_df.iterrows():\n",
    "            if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find BBLs that didn't match in either API\n",
    "    unmatched_bbl_tuples = [bbl for bbl in bbl_tuples if bbl not in matched_bbl_tuples]\n",
    "    \n",
    "    if unmatched_bbl_tuples:\n",
    "        print(f\"\\nTrying condo billing BBLs for {len(unmatched_bbl_tuples)} BBLs that didn't match in BISWEB or DOB NOW...\")\n",
    "        condo_results = []\n",
    "        condo_matched_base_bbls = set()\n",
    "        for bbl_tuple in unmatched_bbl_tuples:\n",
    "            borough, block, lot = bbl_tuple\n",
    "            # Construct base BBL for condo billing lookup\n",
    "            borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "            borough_code = borough_map.get(borough.upper())\n",
    "            if borough_code:\n",
    "                block_clean = str(int(float(block.replace('.0', ''))))\n",
    "                lot_clean = str(int(float(lot.replace('.0', ''))))\n",
    "                base_bbl = borough_code + block_clean.zfill(5) + lot_clean.zfill(4)\n",
    "                condo_df = query_condo_lots_for_bbl(borough, block, lot, base_bbl=base_bbl)\n",
    "                if not condo_df.empty:\n",
    "                    condo_results.append(condo_df)\n",
    "                    condo_matched_base_bbls.add(bbl_tuple)\n",
    "                    condo_results.append(condo_df)\n",
    "        \n",
    "        if condo_results:\n",
    "            condo_df_combined = pd.concat(condo_results, ignore_index=True)\n",
    "            print(f\"Found {len(condo_df_combined)} records on condo billing BBLs\")\n",
    "            # Add to BISWEB results (they'll be combined later)\n",
    "            if not dob_bisweb_bbl_df.empty:\n",
    "                dob_bisweb_bbl_df = pd.concat([dob_bisweb_bbl_df, condo_df_combined], ignore_index=True)\n",
    "            else:\n",
    "                dob_bisweb_bbl_df = condo_df_combined\n",
    "            \n",
    "            # Update matched_bbl_tuples with billing BBLs found\n",
    "            for _, row in condo_df_combined.iterrows():\n",
    "                if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                    matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "            \n",
    "            # Mark original base BBLs as matched since we found permits via condo billing BBLs\n",
    "            for base_bbl_tuple in condo_matched_base_bbls:\n",
    "                matched_bbl_tuples.add(base_bbl_tuple)\n",
    "            for bbl_tuple in unmatched_bbl_tuples:\n",
    "                if bbl_tuple in [bbl for bbl in unmatched_bbl_tuples]:\n",
    "                    # Check if we found a condo billing BBL for this base BBL\n",
    "                    borough, block, lot = bbl_tuple\n",
    "                    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "                    borough_code = borough_map.get(borough.upper())\n",
    "                    if borough_code:\n",
    "                        block_clean = str(int(float(block.replace('.0', ''))))\n",
    "                        lot_clean = str(int(float(lot.replace('.0', ''))))\n",
    "                        base_bbl = borough_code + block_clean.zfill(5) + lot_clean.zfill(4)\n",
    "                        # Check if we found records for this base BBL's condo billing BBL\n",
    "                        from query_dob_filings import get_condo_billing_bbl\n",
    "                        billing_tuple = get_condo_billing_bbl(base_bbl)\n",
    "                        if billing_tuple:\n",
    "                            billing_borough, billing_block, billing_lot = billing_tuple\n",
    "                            # Check if this billing BBL is in our results\n",
    "                            billing_in_results = False\n",
    "                            for _, row in condo_df_combined.iterrows():\n",
    "                                if (str(row.get('borough', '')).upper() == billing_borough.upper() and\n",
    "                                    pad_block(row.get('block', '')) == billing_block and\n",
    "                                    pad_lot(row.get('lot', '')) == billing_lot):\n",
    "                                    billing_in_results = True\n",
    "                                    break\n",
    "                            if billing_in_results:\n",
    "                                # Mark the original base BBL as matched\n",
    "                                matched_bbl_tuples.add(bbl_tuple)\n",
    "    \n",
    "    # Address search as final fallback for still-unmatched BBLs\n",
    "    still_unmatched_after_condo = [bbl for bbl in unmatched_bbl_tuples if bbl not in matched_bbl_tuples]\n",
    "    \n",
    "    if still_unmatched_after_condo:\n",
    "        print(f\"\\nTrying address search for {len(still_unmatched_after_condo)} BBLs that still didn't match...\")\n",
    "        \n",
    "        # Build address list from HPD data for unmatched BBLs\n",
    "        address_list = []\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            bbl_tuple = decompose_bbl(row.get('BBL'))\n",
    "            if bbl_tuple and len(bbl_tuple) >= 3:\n",
    "                bbl_key = (str(bbl_tuple[0]).upper(), pad_block(bbl_tuple[1]), pad_lot(bbl_tuple[2]))\n",
    "                if bbl_key in still_unmatched_after_condo:\n",
    "                    number = str(row.get('Number', '')).strip()\n",
    "                    street = str(row.get('Street', '')).strip()\n",
    "                    borough = str(bbl_tuple[0]).upper()\n",
    "                    if number and street and borough:\n",
    "                        address_list.append((borough, number, street))\n",
    "        \n",
    "        if address_list:\n",
    "            # Deduplicate addresses\n",
    "            address_list = list(set(address_list))\n",
    "            print(f\"Searching {len(address_list)} unique addresses...\")\n",
    "            \n",
    "            address_df = query_dob_by_address(address_list)\n",
    "            \n",
    "            if not address_df.empty:\n",
    "                print(f\"Found {len(address_df)} records by address search\")\n",
    "                # Add to BISWEB results\n",
    "                if not dob_bisweb_bbl_df.empty:\n",
    "                    dob_bisweb_bbl_df = pd.concat([dob_bisweb_bbl_df, address_df], ignore_index=True)\n",
    "                else:\n",
    "                    dob_bisweb_bbl_df = address_df\n",
    "                \n",
    "                # Update matched_bbl_tuples with BBLs found by address\n",
    "                for _, row in address_df.iterrows():\n",
    "                    if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                        matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "        else:\n",
    "            print(\"No addresses available for address search\")\n",
    "    else:\n",
    "        print(\"All BBLs matched - no address fallback needed\")\n",
    "    \n",
    "    \n",
    "print(f\"DOB NOW BBL: {len(dob_now_bbl_df) if 'dob_now_bbl_df' in locals() else 0} records\")\n",
    "\n",
    "# Preview each dataset\n",
    "if len(dob_bisweb_bin_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BIN sample:\")\n",
    "    display(dob_bisweb_bin_df.head())\n",
    "\n",
    "if len(dob_bisweb_bbl_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BBL sample:\")\n",
    "\n",
    "if len(dob_now_bin_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BIN sample:\")\n",
    "if len(dob_now_bbl_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BBL sample:\")\n",
    "display(dob_bisweb_bin_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB BISWEB (BBL) sample:\")\n",
    "display(dob_bisweb_bbl_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB NOW (BIN) sample:\")\n",
    "display(dob_now_bin_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB NOW (BBL) sample:\")\n",
    "display(dob_now_bbl_df.head())\n",
    "\n",
    "# If needed, merge or explore further:\n",
    "# dob_df = pd.concat([dob_bisweb_bin_df, dob_bisweb_bbl_df, dob_now_bin_df, dob_now_bbl_df],\n",
    "#                    ignore_index=True, sort=False)\n",
    "# print(f\"\\nCombined DOB Data: {dob_df.shape[0]} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Multifamily Finance Program (MFP) new construction projects, find those with no DOB match in any table.\n",
    "\n",
    "# Filter only MFP, new construction projects\n",
    "mfp_new_construction = hpd_df[(hpd_df['Program Group'] == 'Multifamily Finance Program') & \n",
    "                               (hpd_df['Reporting Construction Type'].str.lower().str.contains('new'))]\n",
    "\n",
    "# Defensive: set of unique Project IDs for matching\n",
    "mfp_project_ids = set(mfp_new_construction['Project ID'].unique())\n",
    "\n",
    "# Combine all DOB dataframes and normalize BIN columns\n",
    "all_dob_dfs = []\n",
    "\n",
    "# Normalize BIN columns in each DOB dataframe\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df = dob_bisweb_bin_df.copy()\n",
    "        dob_bisweb_bin_df['bin_normalized'] = dob_bisweb_bin_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df['bbl'] = dob_bisweb_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bin_df)\n",
    "\n",
    "if not dob_bisweb_bbl_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    elif 'bin' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df['bbl'] = dob_bisweb_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bbl_df)\n",
    "\n",
    "if not dob_now_bin_df.empty:\n",
    "    if 'bin' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df = dob_now_bin_df.copy()\n",
    "        dob_now_bin_df['bin_normalized'] = dob_now_bin_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df['bbl'] = dob_now_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bin_df)\n",
    "\n",
    "if not dob_now_bbl_df.empty:\n",
    "    if 'bin' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df = dob_now_bbl_df.copy()\n",
    "        dob_now_bbl_df['bin_normalized'] = dob_now_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df['bbl'] = dob_now_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bbl_df)\n",
    "\n",
    "# Combine all DOB records\n",
    "if all_dob_dfs:\n",
    "    combined_dob = pd.concat(all_dob_dfs, ignore_index=True)\n",
    "    print(f'Total DOB records: {len(combined_dob)}')\n",
    "else:\n",
    "    combined_dob = pd.DataFrame()\n",
    "    print('No DOB records found')\n",
    "\n",
    "# Prepare HPD data for matching - normalize BIN and ensure BBL is string\n",
    "hpd_for_matching = mfp_new_construction.copy()\n",
    "hpd_for_matching['bin_normalized'] = hpd_for_matching['BIN'].astype(str).str.replace('.0', '')\n",
    "hpd_for_matching['bbl_normalized'] = hpd_for_matching['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "\n",
    "# Join on BIN first\n",
    "if not combined_dob.empty and 'bin_normalized' in combined_dob.columns:\n",
    "    dob_bin_matches = pd.merge(\n",
    "        hpd_for_matching,\n",
    "        combined_dob[['bin_normalized']].drop_duplicates(),\n",
    "        on='bin_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bin = set(dob_bin_matches['Project ID'].unique())\n",
    "    print(f'Projects matched on BIN: {len(matched_project_ids_bin)}')\n",
    "else:\n",
    "    matched_project_ids_bin = set()\n",
    "\n",
    "# Join on BBL for those that didn't match on BIN\n",
    "unmatched_on_bin = hpd_for_matching[~hpd_for_matching['Project ID'].isin(matched_project_ids_bin)]\n",
    "\n",
    "# Initialize BBL matching result\n",
    "matched_project_ids_bbl = set()\n",
    "\n",
    "# Reconstruct BBL in DOB data for sources that don't have it (like BISWEB)\n",
    "# Reconstruct BBL from borough, block, lot for records that don't have it\n",
    "def reconstruct_bbl(row):\n",
    "    if pd.isna(row.get('borough')) or pd.isna(row.get('block')) or pd.isna(row.get('lot')):\n",
    "        return None\n",
    "    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "    borough_code = borough_map.get(str(row['borough']).upper(), None)\n",
    "    if not borough_code:\n",
    "        return None\n",
    "    # Remove leading zeros from block/lot for BBL reconstruction\n",
    "    block_str = str(int(float(str(row['block']).replace('.0', ''))))\n",
    "    lot_str = str(int(float(str(row['lot']).replace('.0', ''))))\n",
    "    # Reconstruct: borough(1) + block(5) + lot(4) = 10 digits\n",
    "    bbl_str = borough_code + block_str.zfill(5) + lot_str.zfill(4)\n",
    "    return bbl_str.zfill(10)\n",
    "\n",
    "# Always reconstruct BBL for records that need it (BISWEB data doesn't have bbl column)\n",
    "combined_dob['bbl_reconstructed'] = combined_dob.apply(reconstruct_bbl, axis=1)\n",
    "# Normalize BBL in DOB data (use existing bbl or reconstructed)\n",
    "# Use bbl column if available, otherwise use reconstructed BBL\n",
    "if 'bbl' in combined_dob.columns:\n",
    "    # Use existing bbl column, normalized to 10 digits\n",
    "    combined_dob['bbl_normalized'] = combined_dob['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    # Fill missing values with reconstructed BBL\n",
    "    if 'bbl_reconstructed' in combined_dob.columns:\n",
    "        combined_dob['bbl_normalized'] = combined_dob['bbl_normalized'].fillna(combined_dob['bbl_reconstructed'])\n",
    "elif 'bbl_reconstructed' in combined_dob.columns:\n",
    "    # Use reconstructed BBL if no bbl column exists\n",
    "    combined_dob['bbl_normalized'] = combined_dob['bbl_reconstructed']\n",
    "else:\n",
    "    combined_dob['bbl_normalized'] = None\n",
    "\n",
    "# Now match on BBL\n",
    "if 'bbl_normalized' in combined_dob.columns and combined_dob['bbl_normalized'].notna().any():\n",
    "    dob_bbl_matches = pd.merge(\n",
    "        unmatched_on_bin,\n",
    "        combined_dob[['bbl_normalized']].drop_duplicates(),\n",
    "        on='bbl_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bbl = set(dob_bbl_matches['Project ID'].unique())\n",
    "    print(f'Projects matched on BBL (fallback): {len(matched_project_ids_bbl)}')\n",
    "else:\n",
    "    matched_project_ids_bbl = set()\n",
    "    print('No BBL data available for matching')\n",
    "\n",
    "# Combine all matched project IDs\n",
    "dob_matched_project_ids = matched_project_ids_bin | matched_project_ids_bbl\n",
    "\n",
    "# Find projects without DOB matches\n",
    "mfp_projects_without_dob = mfp_project_ids - dob_matched_project_ids\n",
    "\n",
    "print(f'\\nTotal Multifamily Finance Program new construction projects: {len(mfp_project_ids)}')\n",
    "print(f'Projects with DOB matches: {len(dob_matched_project_ids)}')\n",
    "print(f'Number of these with NO DOB row in any table: {len(mfp_projects_without_dob)}')\n",
    "\n",
    "# Debug: show a sample of matched and unmatched projects\n",
    "if len(matched_project_ids_bin) > 0:\n",
    "    print(f'\\nSample matched on BIN: {list(matched_project_ids_bin)[:3]}')\n",
    "if len(matched_project_ids_bbl) > 0:\n",
    "    print(f'Sample matched on BBL: {list(matched_project_ids_bbl)[:3]}')\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(f'Sample unmatched: {list(mfp_projects_without_dob)[:3]}')\n",
    "\n",
    "# DEBUG: Analyze a sample project to understand matching\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    sample_project_id = list(mfp_projects_without_dob)[0]\n",
    "    sample_project = mfp_new_construction[mfp_new_construction['Project ID'] == sample_project_id]\n",
    "    print(f'\\n=== DEBUG: Sample unmatched project ===')\n",
    "    print(f'Project ID: {sample_project_id}')\n",
    "    print(f'Number of buildings in project: {len(sample_project)}')\n",
    "    sample_bins = sample_project['BIN'].dropna().astype(str).str.replace('.0', '').tolist()\n",
    "    sample_bbls = sample_project['BBL'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).tolist()\n",
    "    print(f'BINs in project: {sample_bins[:5]}')\n",
    "    print(f'BBLs in project: {sample_bbls[:5]}')\n",
    "    \n",
    "    # Check if these BINs/BBLs exist in DOB data\n",
    "    if not combined_dob.empty:\n",
    "        if 'bin_normalized' in combined_dob.columns:\n",
    "            dob_bins = set(combined_dob['bin_normalized'].dropna().astype(str).unique())\n",
    "            matching_bins = [b for b in sample_bins if b in dob_bins]\n",
    "            print(f'BINs found in DOB data: {matching_bins[:5] if matching_bins else \"None\"}')\n",
    "        if 'bbl' in combined_dob.columns:\n",
    "            dob_bbls = set(combined_dob['bbl'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).unique())\n",
    "            matching_bbls = [b for b in sample_bbls if b in dob_bbls]\n",
    "            print(f'BBLs found in DOB data: {matching_bbls[:5] if matching_bbls else \"None\"}')\n",
    "# Show the head of the table of unmatched projects (project-level)\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(\"\\nHead of unmatched Multifamily Finance Program new construction projects:\")\n",
    "    unmatched_projects_df = mfp_new_construction[mfp_new_construction['Project ID'].isin(mfp_projects_without_dob)].copy()\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'BBL' in unmatched_projects_df.columns:\n",
    "        unmatched_projects_df['BBL'] = unmatched_projects_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    display_cols = [c for c in unmatched_projects_df.columns if c not in [\n",
    "        \"Lot Area\", \"Available Units\", \"Privately Financed Units\", \"Extremely Low Income Units\",\n",
    "        \"Very Low Income Units\", \"Low Income Units\", \"Moderate Income Units\", \"Middle Income Units\",\n",
    "        \"Studio Units\", \"One Bedroom Units\", \"Two Bedroom Units\", \"Three Bedroom Units\",\n",
    "        \"Four Bedroom Units\", \"Five Bedroom Units\", \"Six Bedroom Units\", \"Unknown Bedroom Units\",\n",
    "    ][:15]]  # Limit extra-wide tables in notebook\n",
    "    display(unmatched_projects_df[display_cols].head(10))\n",
    "else:\n",
    "    print(\"\\nAll Multifamily Finance Program projects matched to DOB data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbl_to_boro_block_lot_and_name(bbl):\n",
    "    \"\"\"\n",
    "    Convert a NYC BBL (Borough-Block-Lot) string (like '3015560003') to a tuple:\n",
    "    (borough_number, borough_name, block, lot).\n",
    "    Borough number is int (1-5). Name is string (\"Manhattan\", \"Bronx\", ...).\n",
    "\n",
    "    Example:\n",
    "        bbl_to_boro_block_lot_and_name('3015560003')\n",
    "        -> (3, 'Brooklyn', '01556', '0003')\n",
    "    \"\"\"\n",
    "    boro_names = {\n",
    "        1: \"Manhattan\",\n",
    "        2: \"Bronx\",\n",
    "        3: \"Brooklyn\",\n",
    "        4: \"Queens\",\n",
    "        5: \"Staten Island\",\n",
    "    }\n",
    "    bbl_str = str(bbl).zfill(10)\n",
    "    borough_num = int(bbl_str[0])\n",
    "    borough_name = boro_names.get(borough_num, \"Unknown\")\n",
    "    block = bbl_str[1:6]\n",
    "    lot = bbl_str[6:10]\n",
    "    return (borough_num, borough_name, block, lot)\n",
    "\n",
    "# Example usage\n",
    "bbl_example = '3015560003'\n",
    "boro_num, boro_name, block, lot = bbl_to_boro_block_lot_and_name(bbl_example)\n",
    "print(f\"BBL {bbl_example} -> Borough {boro_num} ({boro_name}), Block {block}, Lot {lot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Step 3B: Query Certificate of Occupancy\n",
    "\n",
    "Search for Certificate of Occupancy filings.\n",
    "\n",
    "**Depends on:** Step 2\n",
    "**Options:**\n",
    "- Set `skip_co = True` to use existing CO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B Configuration\n",
    "skip_co = False  # Set to True to use existing CO data\n",
    "co_output_path = None  # Custom CO output path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3B: QUERY CERTIFICATE OF OCCUPANCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate BIN file for CO searches\n",
    "bin_output = Path(\"data/processed/workflow_bins.txt\")\n",
    "bin_file = _write_bin_file(building_csv, bin_output)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb BIN file created: {bin_file}\")\n",
    "print(f\"Contains {len(bin_file.read_text().split())} BINs\")\n",
    "\n",
    "co_output = Path(co_output_path) if co_output_path else Path(\n",
    "    f\"data/processed/{bin_file.stem}_co_filings.csv\"\n",
    ")\n",
    "co_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if skip_co:\n",
    "    print(\"\u23ed\ufe0f Using existing CO data\")\n",
    "    # Look for existing CO files\n",
    "    alt_co_path = Path(f\"data/external/{bin_file.stem}_co_filings.csv\")\n",
    "    if co_output.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing CO data at {co_output}\")\n",
    "        co_df = pd.read_csv(co_output)\n",
    "    elif alt_co_path.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing CO data from external folder: {alt_co_path}\")\n",
    "        co_output = alt_co_path\n",
    "        co_df = pd.read_csv(co_output)\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No existing CO data found\")\n",
    "        co_df = None\n",
    "        co_output = None\n",
    "else:\n",
    "    print(f\"\ud83c\udfdb\ufe0f Querying CO APIs using {bin_file} -> {co_output}\")\n",
    "    query_co_filings(str(bin_file), output_path=str(co_output))\n",
    "    co_df = pd.read_csv(co_output)\n",
    "\n",
    "# Display CO data if available\n",
    "if co_df is not None:\n",
    "    print(f\"\ud83d\udcca Certificate of Occupancy Data: {co_df.shape[0]} records\")\n",
    "    print(\"Columns:\")\n",
    "    for col in co_df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Sample CO Data:\")\n",
    "    display(co_df.head())\n",
    "    \n",
    "    # Show some statistics\n",
    "    if \"issue_date\" in co_df.columns:\n",
    "        print(\"\\n\ud83d\udcc8 CO Issue Date Statistics:\")\n",
    "        display(co_df[\"issue_date\"].describe())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No CO data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 4: Generate Timelines and Charts\n",
    "\n",
    "Create timeline visualizations from enriched data.\n",
    "\n",
    "**Depends on:** Steps 2, 3A\n",
    "**Options:**\n",
    "- Set `skip_join = True` to skip timeline creation\n",
    "- Set `skip_charts = True` to skip chart generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 Configuration\n",
    "skip_join = False   # Set to True to skip timeline creation\n",
    "skip_charts = False # Set to True to skip chart generation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: GENERATE TIMELINES AND CHARTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if skip_join:\n",
    "    print(\"\u23ed\ufe0f Skipping timeline join step.\")\n",
    "else:\n",
    "    if dob_output is None or not dob_output.exists():\n",
    "        print(\"\u26a0\ufe0f No DOB data available; skipping timeline creation.\")\n",
    "    else:\n",
    "        print(\"\ud83d\udd17 Building timelines...\")\n",
    "        create_separate_timelines(\n",
    "            str(building_csv),\n",
    "            str(dob_output),\n",
    "            str(co_output) if co_output else None,\n",
    "        )\n",
    "        \n",
    "        # Load and display timeline data\n",
    "        hpd_timeline = Path(str(building_csv).replace(\".csv\", \"_hpd_financed_timeline.csv\"))\n",
    "        private_timeline = Path(str(building_csv).replace(\".csv\", \"_privately_financed_timeline.csv\"))\n",
    "        \n",
    "        if hpd_timeline.exists():\n",
    "            hpd_timeline_df = pd.read_csv(hpd_timeline)\n",
    "            print(f\"\\n\ud83d\udcca HPD Financed Timeline Data ({hpd_timeline_df.shape[0]} records):\")\n",
    "            display(hpd_timeline_df.head())\n",
    "            \n",
    "            # Show event type distribution\n",
    "            if \"event_type\" in hpd_timeline_df.columns:\n",
    "                print(\"\\n\ud83d\udcc8 Event Types in HPD Timeline:\")\n",
    "                display(hpd_timeline_df[\"event_type\"].value_counts())\n",
    "        \n",
    "        if private_timeline.exists():\n",
    "            private_timeline_df = pd.read_csv(private_timeline)\n",
    "            print(f\"\\n\ud83d\udcca Privately Financed Timeline Data ({private_timeline_df.shape[0]} records):\")\n",
    "            display(private_timeline_df.head())\n",
    "            \n",
    "            # Show event type distribution\n",
    "            if \"event_type\" in private_timeline_df.columns:\n",
    "                print(\"\\n\ud83d\udcc8 Event Types in Private Timeline:\")\n",
    "                display(private_timeline_df[\"event_type\"].value_counts())\n",
    "\n",
    "if skip_charts:\n",
    "    print(\"\u23ed\ufe0f Skipping chart generation.\")\n",
    "else:\n",
    "    # Charts\n",
    "    print(\"\\n\ud83d\udcc8 Generating charts...\")\n",
    "    default_timeline_stem = \"Affordable_Housing_Production_by_Building_with_financing\"\n",
    "    if Path(building_csv).name == f\"{default_timeline_stem}.csv\":\n",
    "        create_financing_charts()\n",
    "        print(\"\u2705 Created financing-specific charts\")\n",
    "    else:\n",
    "        hpd_timeline = Path(str(building_csv).replace(\".csv\", \"_hpd_financed_timeline.csv\"))\n",
    "        private_timeline = Path(str(building_csv).replace(\".csv\", \"_privately_financed_timeline.csv\"))\n",
    "        \n",
    "        if hpd_timeline.exists():\n",
    "            create_timeline_chart(str(hpd_timeline))\n",
    "            print(f\"\u2705 Created HPD financed timeline chart\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f No HPD financed timeline found; skipping.\")\n",
    "\n",
    "        if private_timeline.exists():\n",
    "            create_timeline_chart(str(private_timeline))\n",
    "            print(f\"\u2705 Created privately financed timeline chart\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f No privately financed timeline found; skipping.\")\n",
    "\n",
    "print(\"\\n\u2705 Step 4 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Final Summary\n",
    "\n",
    "Generate data quality report and workflow summary.\n",
    "\n",
    "**Optional:** Run this at the end to see final statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\ud83d\udcca FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate final data quality report and Sankey diagram\n",
    "quality_tracker.end_processing()\n",
    "report_filename = quality_tracker.save_report_to_file(\"notebook_workflow\")\n",
    "sankey_filename = quality_tracker.generate_sankey_diagram()\n",
    "quality_tracker.print_report()\n",
    "\n",
    "print(\"\\n\ud83c\udf89 WORKFLOW COMPLETED!\")\n",
    "print(f\"\ud83d\udcca Data quality report: {report_filename}\")\n",
    "if sankey_filename:\n",
    "    print(f\"\ud83d\udcca Sankey diagram: {sankey_filename}\")\n",
    "\n",
    "# Summary of what we accomplished\n",
    "print(\"\\n\ud83d\udccb WORKFLOW SUMMARY:\")\n",
    "try:\n",
    "    print(f\"\u2022 HPD Records Processed: {len(hpd_df):,}\")\n",
    "except NameError:\n",
    "    print(\"\u2022 HPD Records: Step 1 not run\")\n",
    "try:\n",
    "    print(f\"\u2022 Records with Financing: {len(financing_df):,}\")\n",
    "except NameError:\n",
    "    print(\"\u2022 Records with Financing: Step 2 not run\")\n",
    "try:\n",
    "    if dob_df is not None:\n",
    "        print(f\"\u2022 DOB Filings Found: {len(dob_df):,}\")\n",
    "    else:\n",
    "        print(\"\u2022 DOB Filings: No data\")\n",
    "except NameError:\n",
    "    print(\"\u2022 DOB Filings: Step 3A not run\")\n",
    "try:\n",
    "    if co_df is not None:\n",
    "        print(f\"\u2022 CO Filings Found: {len(co_df):,}\")\n",
    "    else:\n",
    "        print(\"\u2022 CO Filings: No data\")\n",
    "except NameError:\n",
    "    print(\"\u2022 CO Filings: Step 3B not run\")\n",
    "\n",
    "print(\"\\n\u2705 Notebook workflow complete!\")\n",
    "print(\"Each step showed dataframe views for inspection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}