{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Workflow Notebook\n",
    "\n",
    "Modular workflow where you can run individual steps independently.\n",
    "Run cells in order or skip any steps you don't need.\n",
    "\n",
    "Each step shows dataframe views and statistics for inspection.\n",
    "\n",
    "## Quick Start\n",
    "- Run **Setup** cell first\n",
    "- Then run any combination of Step 1-4 cells\n",
    "- Skip cells you don't want to execute\n",
    "- Each cell is self-contained and shows results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup\n",
    "\n",
    "Run this cell first to import modules and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Import our workflow modules\n",
    "from fetch_affordable_housing_data import update_local_data, verify_and_fetch_hpd_data\n",
    "from query_ll44_funding import query_and_add_financing\n",
    "from query_dob_filings import query_dob_bisweb_bin, query_dob_bisweb_bbl, query_dobnow_bin, query_dobnow_bbl, decompose_bbl, query_condo_lots_for_bbl, query_dob_by_address, pad_block, pad_lot\n",
    "from query_co_filings import query_co_filings\n",
    "from HPD_DOB_Join_On_BIN import create_separate_timelines\n",
    "from create_timeline_chart import create_timeline_chart, create_financing_charts\n",
    "\n",
    "print(\"\u2705 All imports successful\")\n",
    "\n",
    "# Helper functions\n",
    "def _normalize_bin(bin_value) -> Optional[str]:\n",
    "    \"\"\"Normalize BIN to a clean string.\"\"\"\n",
    "    if pd.isna(bin_value):\n",
    "        return None\n",
    "    try:\n",
    "        return str(int(float(bin_value)))\n",
    "    except (TypeError, ValueError):\n",
    "        value = str(bin_value).strip()\n",
    "        return value or None\n",
    "\n",
    "def _extract_bins_from_df(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Extract BINs from a DataFrame and return as a list for CO searches.\"\"\"\n",
    "    candidate_cols = []\n",
    "    for col in df.columns:\n",
    "        if col.lower() in (\"bin\", \"bin_normalized\"):\n",
    "            candidate_cols.append(col)\n",
    "    if not candidate_cols:\n",
    "        raise SystemExit(f\"Could not find a BIN column in DataFrame\")\n",
    "\n",
    "    bins = []\n",
    "    for val in df[candidate_cols[0]].dropna():\n",
    "        normalized = _normalize_bin(val)\n",
    "        if normalized:\n",
    "            bins.append(normalized)\n",
    "    \n",
    "    # Remove duplicates using set, then sort\n",
    "    unique_bins = set()\n",
    "    for b in bins:\n",
    "        if b:\n",
    "            unique_bins.add(b)\n",
    "    return sorted(unique_bins)\n",
    "\n",
    "def _query_co_filings_from_bins(bin_list: list[str], output_path: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"Query CO filings using a list of BINs (no file needed).\"\"\"\n",
    "    from query_co_filings import query_co_api, DOB_NOW_CO_URL, DOB_CO_URL\n",
    "    \n",
    "    # Convert BINs to integers for API query\n",
    "    bin_ints = []\n",
    "    for bin_str in bin_list:\n",
    "        if str(bin_str).isdigit():\n",
    "            bin_ints.append(int(bin_str))\n",
    "    bins = sorted(list(set(bin_ints)))\n",
    "    \n",
    "    # Query DOB NOW Certificate of Occupancy API\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QUERYING DOB NOW CERTIFICATE OF OCCUPANCY\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_now_co = query_co_api(DOB_NOW_CO_URL, bins, bin_column=\"bin\")\n",
    "    \n",
    "    # Query DOB Certificate Of Occupancy API\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUERYING DOB CERTIFICATE OF OCCUPANCY\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_co = query_co_api(DOB_CO_URL, bins, bin_column=\"bin_number\")\n",
    "    \n",
    "    # Combine results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not dob_now_co.empty:\n",
    "        print(f\"\\nDOB NOW CO Filings: {len(dob_now_co)} records\")\n",
    "        dob_now_co['source'] = 'DOB_NOW_CO'\n",
    "        if 'bin' in dob_now_co.columns:\n",
    "            dob_now_co['bin_normalized'] = dob_now_co['bin'].astype(str)\n",
    "    \n",
    "    if not dob_co.empty:\n",
    "        print(f\"\\nDOB CO Filings: {len(dob_co)} records\")\n",
    "        dob_co['source'] = 'DOB_CO'\n",
    "        if 'bin_number' in dob_co.columns:\n",
    "            dob_co['bin_normalized'] = dob_co['bin_number'].astype(str)\n",
    "    \n",
    "    # Combine both dataframes\n",
    "    if not dob_now_co.empty and not dob_co.empty:\n",
    "        all_cols = list(set(dob_now_co.columns.tolist() + dob_co.columns.tolist()))\n",
    "        if 'bin_normalized' not in all_cols:\n",
    "            all_cols.append('bin_normalized')\n",
    "        if 'source' not in all_cols:\n",
    "            all_cols.append('source')\n",
    "        dob_now_co_aligned = dob_now_co.reindex(columns=all_cols)\n",
    "        dob_co_aligned = dob_co.reindex(columns=all_cols)\n",
    "        combined = pd.concat([dob_now_co_aligned, dob_co_aligned], ignore_index=True)\n",
    "    elif not dob_now_co.empty:\n",
    "        combined = dob_now_co.copy()\n",
    "    elif not dob_co.empty:\n",
    "        combined = dob_co.copy()\n",
    "    else:\n",
    "        print(\"\\nNo CO filings found in either API\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nTotal combined records: {len(combined)}\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"\u2705 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce5 Step 1: Fetch HPD Data\n",
    "\n",
    "Load or refresh the HPD affordable housing dataset.\n",
    "\n",
    "**Options:**\n",
    "- Set `refresh_data = True` to fetch fresh data\n",
    "- Set `refresh_data = False` to use existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Configuration\n",
    "refresh_data = False  # Set to True to fetch fresh HPD data\n",
    "hpd_output_path = \"data/raw/Affordable_Housing_Production_by_Building.csv\"  # Output path for HPD data\n",
    "refresh_hpd_projects = False  # Set to True to fetch fresh HPD projects data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: FETCH HPD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start quality tracking\n",
    "\n",
    "# Handle HPD projects cache refresh if requested\n",
    "if refresh_hpd_projects:\n",
    "    print(\"Force refreshing HPD projects cache...\")\n",
    "    from fetch_affordable_housing_data import verify_and_fetch_hpd_projects_data\n",
    "    hpd_projects_df, hpd_projects_path = verify_and_fetch_hpd_projects_data(use_existing=False)\n",
    "    print(f\"HPD projects cache refreshed: {len(hpd_projects_df)} records\\n\")\n",
    "\n",
    "if refresh_data:\n",
    "    print(\"Fetching fresh HPD data from NYC Open Data...\")\n",
    "    hpd_df, hpd_csv = update_local_data(hpd_output_path)\n",
    "else:\n",
    "    print(\"Verifying local HPD data against API...\")\n",
    "    hpd_df, hpd_csv = verify_and_fetch_hpd_data(output_path=hpd_output_path, use_projects_cache=not refresh_hpd_projects)\n",
    "\n",
    "if not hpd_csv.exists():\n",
    "    raise SystemExit(f\"HPD dataset not found at {hpd_csv}\")\n",
    "\n",
    "# Get total units before filter\n",
    "original_count = len(hpd_df)\n",
    "original_units = hpd_df['Total Units'].sum()\n",
    "\n",
    "# Filter to New Construction only\n",
    "hpd_new_construction_df = hpd_df[hpd_df[\"Reporting Construction Type\"] == \"New Construction\"].copy()\n",
    "\n",
    "filtered_count = len(hpd_new_construction_df)\n",
    "filtered_units = hpd_new_construction_df['Total Units'].sum()\n",
    "filtered_out = original_count - filtered_count\n",
    "filtered_units_out = original_units - filtered_units\n",
    "\n",
    "print(f\"\ud83c\udfd7\ufe0f Filtered to New Construction only:\")\n",
    "print(f\"  Original: {original_count:,} projects, {original_units:,} total units\")\n",
    "print(f\"  Filtered: {filtered_count:,} projects ({filtered_count/original_count*100:.1f}%), {filtered_units:,} total units ({filtered_units/original_units*100:.1f}%)\")\n",
    "print(f\"  Removed: {filtered_out:,} non-new construction projects ({filtered_out/original_count*100:.1f}%), {filtered_units_out:,} units filtered out ({filtered_units_out/original_units*100:.1f}%)\")\n",
    "\n",
    "print(f\"\u2705 Step 1 complete: {len(hpd_new_construction_df):,} records loaded\")\n",
    "print(f\"\ud83d\udcc1 Data location: {hpd_csv}\")\n",
    "\n",
    "# Display the dataframe\n",
    "print(\"\\n\ud83d\udd0d HPD Dataset Overview (New Construction only):\")\n",
    "print(f\"Shape: {hpd_new_construction_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in hpd_new_construction_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Sample Data:\")\n",
    "display(hpd_new_construction_df.head())\n",
    "print(\"\\n\ud83d\udcc8 Basic Statistics:\")\n",
    "display(hpd_new_construction_df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique counts are there by project id as primary key per program group,\n",
    "# and show total units in parentheticals (but NOT for the unique project counts).\n",
    "\n",
    "# Compute total units per Program Group (all rows)\n",
    "units_per_group = hpd_new_construction_df.groupby('Program Group')['Total Units'].sum()\n",
    "\n",
    "print(\"Program Group counts (raw rows) (total units in parentheses):\")\n",
    "raw_row_counts = hpd_new_construction_df['Program Group'].value_counts()\n",
    "for group, count in raw_row_counts.items():\n",
    "    units = units_per_group.get(group, 0)\n",
    "    print(f\"{group}: {count} rows ({units} units)\")\n",
    "print()\n",
    "\n",
    "# Group by Program Group, count unique Project IDs\n",
    "unique_proj_counts = hpd_new_construction_df.groupby('Program Group')['Project ID'].nunique().sort_values(ascending=False)\n",
    "unique_proj_ids = (\n",
    "    hpd_new_construction_df\n",
    "    .groupby('Program Group')\n",
    "    .apply(lambda df: df['Project ID'].unique())\n",
    ")\n",
    "\n",
    "print(\"Program Group counts (unique Project ID as primary key):\")\n",
    "for group, count in unique_proj_counts.items():\n",
    "    print(f\"{group}: {count} projects\")\n",
    "print()\n",
    "\n",
    "print(\"\\nTax Abatement by Program Group (based on unique Project ID):\")\n",
    "if 'Planned Tax Benefit' in hpd_new_construction_df.columns:\n",
    "    # For this, deduplicate by Project ID first\n",
    "    unique_project_rows = hpd_new_construction_df.drop_duplicates(subset=['Project ID'])\n",
    "    tax_abate_ct = (\n",
    "        unique_project_rows\n",
    "        .groupby('Program Group')['Planned Tax Benefit']\n",
    "        .value_counts(dropna=False)\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    # Also display total units per Program Group in this table, if desired\n",
    "    units_per_group_project = unique_project_rows.groupby('Program Group')['Total Units'].sum()\n",
    "    print(\"Total units (unique Project ID per Program Group):\")\n",
    "    display(units_per_group_project)\n",
    "    display(tax_abate_ct)\n",
    "else:\n",
    "    print(\"Column 'Planned Tax Benefit' not found in dataset.\")\n",
    "\n",
    "# Make a version of this with unit count by program and tax benefit\n",
    "if 'Planned Tax Benefit' in unique_project_rows.columns and 'Program Group' in unique_project_rows.columns:\n",
    "    units_pivot = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Total units by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(units_pivot)\n",
    "else:\n",
    "    print(\"Required columns not found for unit pivot table.\")\n",
    "\n",
    "# Calculate average units per year by Program Group and Planned Tax Benefit\n",
    "\n",
    "if 'Project Start Date' in unique_project_rows.columns and 'Total Units' in unique_project_rows.columns:\n",
    "    # Extract year from 'Project Start Date'\n",
    "    unique_project_rows = unique_project_rows.copy()\n",
    "    unique_project_rows['Project Year'] = pd.to_datetime(unique_project_rows['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    avg_units_per_year = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit', 'Project Year'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Now calculate the average units per year by program group and tax abatement\n",
    "    avg_units_table = (\n",
    "        avg_units_per_year\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Average units per year by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(avg_units_table)\n",
    "else:\n",
    "    print(\"Required columns not found for average units per year table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use the full raw HPD data, because we want all programs, not just Multifamily Finance Program\n",
    "if 'Project Start Date' in hpd_new_construction_df.columns and 'Total Units' in hpd_new_construction_df.columns:\n",
    "    hpd_bar_df = hpd_new_construction_df.copy()\n",
    "    hpd_bar_df['Project Year'] = pd.to_datetime(hpd_bar_df['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    # Only focus on desired groups\n",
    "    programs_of_interest = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "    mask = hpd_bar_df['Program Group'].isin(programs_of_interest)\n",
    "    hpd_bar_df = hpd_bar_df[mask & hpd_bar_df['Project Year'].notna()]\n",
    "\n",
    "    # Fill NAs in Planned Tax Benefit with \"None\"\n",
    "    hpd_bar_df['Planned Tax Benefit'] = hpd_bar_df['Planned Tax Benefit'].fillna('None')\n",
    "\n",
    "    # Prepare for grouped bar with stack\n",
    "    # Pivot: rows = Project Year, columns = (Program Group, Planned Tax Benefit), values = sum of units\n",
    "    pivot = (\n",
    "        hpd_bar_df\n",
    "        .groupby(['Project Year', 'Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure proper order of years and programs\n",
    "    years = sorted(pivot['Project Year'].dropna().unique())\n",
    "    tax_benefits = sorted(pivot['Planned Tax Benefit'].unique())\n",
    "    # Keep consistent order for bars\n",
    "    program_order = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "\n",
    "    # Prepare data structure: for each year, for each program, get breakdown by tax benefit\n",
    "    bar_data = {}\n",
    "    for year in years:\n",
    "        bar_data[year] = {}\n",
    "        for prog in program_order:\n",
    "            mask = (pivot['Project Year'] == year) & (pivot['Program Group'] == prog)\n",
    "            year_prog_data = pivot[mask].set_index('Planned Tax Benefit')['Total Units'].reindex(tax_benefits, fill_value=0)\n",
    "            bar_data[year][prog] = year_prog_data.values\n",
    "\n",
    "    # Number of bars per group (2 programs), group by year, stacked by tax benefit\n",
    "    x = range(len(years))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Colors for planned tax benefits\n",
    "    import matplotlib.cm as cm\n",
    "    color_map = cm.get_cmap('tab20', len(tax_benefits))\n",
    "    colors = []\n",
    "    for i in range(len(tax_benefits)):\n",
    "        colors.append(color_map(i))\n",
    "\n",
    "    bottoms_p1 = [0] * len(years)\n",
    "    bottoms_p2 = [0] * len(years)\n",
    "\n",
    "    # For each tax benefit, draw the stack pieces for both programs\n",
    "    legend_handles = []\n",
    "    for idx, tax in enumerate(tax_benefits):\n",
    "        values_p1 = []\n",
    "        for year in years:\n",
    "            values_p1.append(bar_data[year][program_order[0]][idx])\n",
    "        values_p2 = []\n",
    "        for year in years:\n",
    "            values_p2.append(bar_data[year][program_order[1]][idx])\n",
    "\n",
    "        x_positions_p1 = []\n",
    "        for i in x:\n",
    "            x_positions_p1.append(i - width/2)\n",
    "        x_positions_p2 = []\n",
    "        for i in x:\n",
    "            x_positions_p2.append(i + width/2)\n",
    "        \n",
    "        legend_labels = []\n",
    "        for h in legend_handles:\n",
    "            legend_labels.append(h.get_label())\n",
    "        \n",
    "        bar1 = ax.bar(\n",
    "            x_positions_p1, values_p1, width,\n",
    "            bottom=bottoms_p1, color=colors[idx],\n",
    "            label=tax if (tax not in legend_labels) else None,\n",
    "            edgecolor='black', hatch='////'\n",
    "        )\n",
    "        bar2 = ax.bar(\n",
    "            x_positions_p2, values_p2, width,\n",
    "            bottom=bottoms_p2, color=colors[idx],\n",
    "            label=None,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        legend_labels = []\n",
    "        for h in legend_handles:\n",
    "            legend_labels.append(h.get_label())\n",
    "        if tax not in legend_labels:\n",
    "            legend_handles.append(bar1)\n",
    "\n",
    "        new_bottoms_p1 = []\n",
    "        for b, v in zip(bottoms_p1, values_p1):\n",
    "            new_bottoms_p1.append(b + v)\n",
    "        bottoms_p1 = new_bottoms_p1\n",
    "        \n",
    "        new_bottoms_p2 = []\n",
    "        for b, v in zip(bottoms_p2, values_p2):\n",
    "            new_bottoms_p2.append(b + v)\n",
    "        bottoms_p2 = new_bottoms_p2\n",
    "\n",
    "    # Add year labels\n",
    "    ax.set_xticks(x)\n",
    "    year_labels = []\n",
    "    for y in years:\n",
    "        year_labels.append(str(int(y)))\n",
    "    ax.set_xticklabels(year_labels, rotation=45)\n",
    "    ax.set_xlabel(\"Project Start Year\")\n",
    "    ax.set_ylabel(\"Total Units Financed\")\n",
    "    ax.set_title(\"Units Financed by Year: Multifamily Finance and Incentives Programs\\nColored by Planned Tax Benefit\")\n",
    "\n",
    "    # Custom legend for program groups\n",
    "    import matplotlib.patches as mpatches\n",
    "    progs = [\n",
    "        mpatches.Patch(color='gray', label='Multifamily Finance Program', ec='black', hatch='////'),\n",
    "        mpatches.Patch(color='gray', label='Multifamily Incentives Program', ec='black')\n",
    "    ]\n",
    "    # Only add one legend for planned tax benefit\n",
    "    handles_tax = []\n",
    "    for i in range(len(tax_benefits)):\n",
    "        handles_tax.append(plt.Rectangle((0,0),1,1, color=colors[i], edgecolor='black', label=f\"{tax_benefits[i]}\"))\n",
    "    legend1 = ax.legend(handles=handles_tax, title=\"Planned Tax Benefit\", loc='upper right')\n",
    "    ax.add_artist(legend1)\n",
    "    # Add manual tick legend for program bars\n",
    "    bar_locs = [x[0] - width/2, x[0] + width/2]\n",
    "    ax.bar(bar_locs[0], 0, width, color='white', hatch='////', ec='black', label='Multifamily Finance Program')\n",
    "    ax.bar(bar_locs[1], 0, width, color='white', ec='black', label='Multifamily Incentives Program')\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', hatch='////', edgecolor='black', label='Multifamily Finance Program'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', edgecolor='black', label='Multifamily Incentives Program')\n",
    "        ], title=\"Program Group\", loc='upper left'\n",
    "    )\n",
    "\n",
    "    ax.grid(True, which='major', axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Required columns ('Project Start Date', 'Total Units') not found in HPD Data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and sample: Planned Tax Benefit '421a' and Project Start Date in 2025\n",
    "if \"Planned Tax Benefit\" in hpd_new_construction_df.columns and \"Project Start Date\" in hpd_new_construction_df.columns:\n",
    "    # Filter for 421a and 2025 start year\n",
    "    mask_421a_2025 = (\n",
    "        (hpd_new_construction_df[\"Planned Tax Benefit\"] == \"421a\") &\n",
    "        (hpd_new_construction_df[\"Project Start Date\"].astype(str).str.startswith(\"2025\"))\n",
    "    )\n",
    "    hpd_421a_2025_df = hpd_new_construction_df[mask_421a_2025]\n",
    "\n",
    "    # Count unique projects (by Project ID), and total units\n",
    "    total_projects = hpd_421a_2025_df[\"Project ID\"].nunique() if \"Project ID\" in hpd_421a_2025_df.columns else len(hpd_421a_2025_df)\n",
    "    total_units = hpd_421a_2025_df[\"Total Units\"].sum() if \"Total Units\" in hpd_421a_2025_df.columns else \"N/A\"\n",
    "\n",
    "    print(f\"Total projects with Planned Tax Benefit '421a' and 2025 Start Date: {total_projects:,}\")\n",
    "    print(f\"Total units in these projects: {total_units:,}\")\n",
    "\n",
    "    # Show up to 5 sample rows\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(\"\\nSample 421a Planned Tax Benefit projects with Project Start Date in 2025:\")\n",
    "    display(hpd_421a_2025_df)\n",
    "    pd.reset_option('display.max_columns')\n",
    "else:\n",
    "    print(\"One or both of the columns 'Planned Tax Benefit' or 'Project Start Date' not found in HPD DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Step 2.5: Filter to Multifamily Finance Program\n",
    "\n",
    "Filter the HPD New Construction dataset to Multifamily Finance Program only.\n",
    "\n",
    "**Depends on:** Step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Multifamily Finance Program (already filtered to New Construction in Step 1)\n",
    "original_count = len(hpd_new_construction_df)\n",
    "print(f\"Using HPD data from Step 1: {original_count:,} total buildings (New Construction)\")\n",
    "\n",
    "# Apply filters: Multifamily Finance Program (already filtered to New Construction in Step 1)\n",
    "hpd_multifamily_finance_new_construction_df = hpd_new_construction_df[\n",
    "    hpd_new_construction_df[\"Program Group\"] == \"Multifamily Finance Program\"\n",
    "].copy()\n",
    "filtered_count = len(hpd_multifamily_finance_new_construction_df)\n",
    "\n",
    "print(f\"\ud83c\udfd7\ufe0f Filtered to Multifamily Finance Program:\")\n",
    "print(f\"  Original: {original_count:,} buildings (New Construction)\")\n",
    "print(f\"  Filtered: {filtered_count:,} buildings ({filtered_count/original_count*100:.1f}%)\")\n",
    "print(f\"\ud83d\udcc1 Created critical DataFrame in memory: {filtered_count:,} Multifamily Finance Program (New Construction) buildings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Step 3A: Query DOB Filings\n",
    "\n",
    "Search for DOB New Building filings.\n",
    "\n",
    "**Depends on:** Step 2\n",
    "**Options:**\n",
    "- Set `skip_dob = True` to use existing DOB data\n",
    "- Set `use_bbl_fallback = False` to disable BBL fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A: BIN/BBL Prep and Filtering\n",
    "\n",
    "# Use hpd_multifamily_finance_new_construction_df from Step 2.5 (in memory)\n",
    "print(f\"Using filtered dataset from Step 2.5: {len(hpd_multifamily_finance_new_construction_df):,} Multifamily Finance Program (New Construction) buildings\")\n",
    "\n",
    "# Extract BINs and BBLs from the filtered data\n",
    "bins = []\n",
    "# Extract BBLs properly using decompose_bbl function\n",
    "from query_dob_filings import decompose_bbl\n",
    "\n",
    "bbls = []\n",
    "for idx, row in hpd_multifamily_finance_new_construction_df.iterrows():\n",
    "    if pd.notna(row.get(\"BBL\")):\n",
    "        bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "        if bbl_result and len(bbl_result) >= 3:\n",
    "            borough, block, lot = bbl_result\n",
    "            bbls.append((borough, block, lot))\n",
    "\n",
    "\n",
    "# Filter out bad/placeholder BINs (e.g., 1000000, 2000000, 3000000, 4000000, 5000000)\n",
    "# These are placeholder values that don't exist in DOB\n",
    "def is_bad_bin(bin_str):\n",
    "    \"\"\"Check if BIN is a placeholder/bad value.\"\"\"\n",
    "    if not bin_str or pd.isna(bin_str) or str(bin_str).lower() == 'nan':\n",
    "        return True\n",
    "    bin_str_clean = str(bin_str).strip()\n",
    "    # Check for pattern: [1-5]000000 (borough placeholder BINs)\n",
    "    if bin_str_clean in ['1000000', '2000000', '3000000', '4000000', '5000000']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if 'BIN' in hpd_multifamily_finance_new_construction_df.columns:\n",
    "    bins = []\n",
    "    for b in hpd_multifamily_finance_new_construction_df['BIN'].dropna():\n",
    "        b_str = str(b)\n",
    "        if b_str != 'nan':\n",
    "            b_clean = b_str.replace('.0', '')\n",
    "            if not is_bad_bin(b_clean):\n",
    "                bins.append(b_clean)\n",
    "\n",
    "if 'BBL' in hpd_multifamily_finance_new_construction_df.columns:\n",
    "    bbl_col = hpd_multifamily_finance_new_construction_df['BBL'].astype(str).str.zfill(10)\n",
    "    bbls = []\n",
    "    for bbl_val in bbl_col:\n",
    "        if len(bbl_val) == 10:\n",
    "            bbl_tuple = (\n",
    "                bbl_val[0],                     # borough code (as string)\n",
    "                bbl_val[1:6],                   # block (padded 5 chars)\n",
    "                bbl_val[6:]                     # lot   (padded 4 chars)\n",
    "            )\n",
    "            bbls.append(bbl_tuple)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Prepared {len(bins)} BINs and {len(bbls)} BBLs for DOB queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BISWEB BIN for all buildings",
    "",
    "print(\"BISWEB BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\u25b6\ufe0f Querying BISWEB BIN for {len(bins)} buildings...\")\n",
    "dob_bisweb_bin_df = query_dob_bisweb_bin(bins)",
    "bisweb_bin_matches = set()",
    "if not dob_bisweb_bin_df.empty and \"bin__\" in dob_bisweb_bin_df.columns:",
    "    bisweb_bin_matches = set(dob_bisweb_bin_df[\"bin__\"].dropna().astype(str).unique())\n",
    "bisweb_bin_unmatched = []\n",
    "for b in bins:\n",
    "    if b not in bisweb_bin_matches:\n",
    "        bisweb_bin_unmatched.append(b)\n",
    "print(f\"BISWEB BIN: {len(bisweb_bin_matches)} matches, {len(bisweb_bin_unmatched)} need BBL fallback\")\n",
    "\n",
    "\n",
    "",
    "",
    "# Preview BISWEB BIN results",
    "if not dob_bisweb_bin_df.empty:",
    "    print(\"\\n\ud83d\udcca BISWEB BIN sample:\")",
    "    display(dob_bisweb_bin_df.head())",
    "else:",
    "    print(\"\\n\u26a0\ufe0f No BISWEB BIN results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DOB NOW BIN for all buildings\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOB NOW BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\u25b6\ufe0f Querying DOB NOW BIN for {len(bins)} buildings...\")\n",
    "dob_now_bin_df = query_dobnow_bin(bins)\n",
    "dobnow_bin_matches = set()\n",
    "if not dob_now_bin_df.empty and \"bin\" in dob_now_bin_df.columns:\n",
    "\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BIN sample:\")\n",
    "    display(dob_bisweb_bin_df.head())\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No BISWEB BIN results\")\n",
    "    dobnow_bin_matches = set(dob_now_bin_df[\"bin\"].dropna().astype(str).unique())\n",
    "print(f\"DOB NOW BIN: {len(dobnow_bin_matches)} matches\")\n",
    "\n",
    "# 4. Combined BBL fallback for buildings that failed ALL BIN searches\n",
    "all_bin_matches = bisweb_bin_matches.union(dobnow_bin_matches)\n",
    "all_bin_unmatched = []\n",
    "for b in bins:\n",
    "    if b not in all_bin_matches:\n",
    "        all_bin_unmatched.append(b)\n",
    "print(f\"Combined BIN search: {len(all_bin_matches)} total matches, {len(all_bin_unmatched)} need BBL fallback\")\n",
    "\n",
    "\n",
    "# Preview DOB NOW BIN results\n",
    "if not dob_now_bin_df.empty:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BIN sample:\")\n",
    "    display(dob_now_bin_df.head())\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No DOB NOW BIN results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BISWEB BBL fallback\n",
    "if all_bin_unmatched:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BISWEB BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    bbl_tuples = []\n",
    "    # Also include projects with bad BINs in BBL queries\n",
    "    # These projects should skip BIN queries and go straight to BBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    bad_bin_projects = hpd_multifamily_finance_new_construction_df[\n",
    "        hpd_multifamily_finance_new_construction_df['BIN'].astype(str).str.replace('.0', '').apply(is_bad_bin)\n",
    "    ]\n",
    "    if not bad_bin_projects.empty:\n",
    "        print(f\"Found {len(bad_bin_projects)} projects with bad/placeholder BINs - adding to BBL queries\")\n",
    "        for _, row in bad_bin_projects.iterrows():\n",
    "            if pd.notna(row.get(\"BBL\")):\n",
    "                bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "                if bbl_result and len(bbl_result) >= 3:\n",
    "                    bbl_tuples.append(bbl_result)\n",
    "\n",
    "    print(f\"\u25b6\ufe0f Querying BISWEB BBL for {len(all_bin_unmatched)} buildings...\")\n",
    "    for bin_val in all_bin_unmatched:\n",
    "        matching_rows = hpd_multifamily_finance_new_construction_df[\n",
    "            (hpd_multifamily_finance_new_construction_df[\"BIN\"].astype(str).str.replace(\".0\", \"\") == bin_val)\n",
    "        ]\n",
    "        if not matching_rows.empty:\n",
    "            # Iterate through ALL matching rows, not just the first\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                if pd.notna(row.get(\"BBL\")):\n",
    "                    bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "                    if bbl_result and len(bbl_result) >= 3:\n",
    "                        bbl_tuples.append(bbl_result)\n",
    "    bbl_tuples = list(set(bbl_tuples))\n",
    "    print(f\"Deduplicated to {len(bbl_tuples)} unique BBLs\")\n",
    "    dob_bisweb_bbl_df = query_dob_bisweb_bbl(bbl_tuples)\n",
    "    \n",
    "    # DOB NOW BBL fallback\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DOB NOW BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_now_bbl_df = query_dobnow_bbl(bbl_tuples)\n",
    "else:\n",
    "    # Initialize empty dataframes if no BBL fallback needed\n",
    "    dob_bisweb_bbl_df = pd.DataFrame()\n",
    "    dob_now_bbl_df = pd.DataFrame()\n",
    "# Display results summary\n",
    "    \n",
    "    # Try condo billing BBLs only for BBLs that didn't match in either BISWEB or DOB NOW\n",
    "    # Track which BBLs matched\n",
    "    \n",
    "    # Track which BBLs matched in BISWEB or DOB NOW\n",
    "    matched_bbl_tuples = set()\n",
    "    \n",
    "    # Get matched BBLs from BISWEB results\n",
    "    if not dob_bisweb_bbl_df.empty:\n",
    "        for _, row in dob_bisweb_bbl_df.iterrows():\n",
    "            if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "    \n",
    "    # Get matched BBLs from DOB NOW results\n",
    "    if not dob_now_bbl_df.empty:\n",
    "        for _, row in dob_now_bbl_df.iterrows():\n",
    "            if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find BBLs that didn't match in either API\n",
    "    unmatched_bbl_tuples = []\n",
    "    for bbl in bbl_tuples:\n",
    "        if bbl not in matched_bbl_tuples:\n",
    "            unmatched_bbl_tuples.append(bbl)\n",
    "    \n",
    "    if unmatched_bbl_tuples:\n",
    "        print(f\"\\nTrying condo billing BBLs for {len(unmatched_bbl_tuples)} BBLs that didn't match in BISWEB or DOB NOW...\")\n",
    "        condo_results = []\n",
    "        condo_matched_base_bbls = set()\n",
    "        for bbl_tuple in unmatched_bbl_tuples:\n",
    "            borough, block, lot = bbl_tuple\n",
    "            # Construct base BBL for condo billing lookup\n",
    "            borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "            borough_code = borough_map.get(borough.upper())\n",
    "            if borough_code:\n",
    "                block_clean = str(int(float(block.replace('.0', ''))))\n",
    "                lot_clean = str(int(float(lot.replace('.0', ''))))\n",
    "                base_bbl = borough_code + block_clean.zfill(5) + lot_clean.zfill(4)\n",
    "                condo_df = query_condo_lots_for_bbl(borough, block, lot, base_bbl=base_bbl)\n",
    "                if not condo_df.empty:\n",
    "                    condo_results.append(condo_df)\n",
    "                    condo_matched_base_bbls.add(bbl_tuple)\n",
    "                    condo_results.append(condo_df)\n",
    "        \n",
    "        if condo_results:\n",
    "            condo_df_combined = pd.concat(condo_results, ignore_index=True)\n",
    "            print(f\"Found {len(condo_df_combined)} records on condo billing BBLs\")\n",
    "            # Add to BISWEB results (they'll be combined later)\n",
    "            if not dob_bisweb_bbl_df.empty:\n",
    "                dob_bisweb_bbl_df = pd.concat([dob_bisweb_bbl_df, condo_df_combined], ignore_index=True)\n",
    "            else:\n",
    "                dob_bisweb_bbl_df = condo_df_combined\n",
    "            \n",
    "            # Update matched_bbl_tuples with billing BBLs found\n",
    "            for _, row in condo_df_combined.iterrows():\n",
    "                if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                    matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "            \n",
    "            # Mark original base BBLs as matched since we found permits via condo billing BBLs\n",
    "            for base_bbl_tuple in condo_matched_base_bbls:\n",
    "                matched_bbl_tuples.add(base_bbl_tuple)\n",
    "            for bbl_tuple in unmatched_bbl_tuples:\n",
    "                # Check if bbl_tuple is in unmatched_bbl_tuples (redundant check, but keeping logic)\n",
    "                bbl_in_unmatched = False\n",
    "                for bbl in unmatched_bbl_tuples:\n",
    "                    if bbl_tuple == bbl:\n",
    "                        bbl_in_unmatched = True\n",
    "                        break\n",
    "                if bbl_in_unmatched:\n",
    "                    # Check if we found a condo billing BBL for this base BBL\n",
    "                    borough, block, lot = bbl_tuple\n",
    "                    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "                    borough_code = borough_map.get(borough.upper())\n",
    "                    if borough_code:\n",
    "                        block_clean = str(int(float(block.replace('.0', ''))))\n",
    "                        lot_clean = str(int(float(lot.replace('.0', ''))))\n",
    "                        base_bbl = borough_code + block_clean.zfill(5) + lot_clean.zfill(4)\n",
    "                        # Check if we found records for this base BBL's condo billing BBL\n",
    "                        from query_dob_filings import get_condo_billing_bbl\n",
    "                        billing_tuple = get_condo_billing_bbl(base_bbl)\n",
    "                        if billing_tuple:\n",
    "                            billing_borough, billing_block, billing_lot = billing_tuple\n",
    "                            # Check if this billing BBL is in our results\n",
    "                            billing_in_results = False\n",
    "                            for _, row in condo_df_combined.iterrows():\n",
    "                                if (str(row.get('borough', '')).upper() == billing_borough.upper() and\n",
    "                                    pad_block(row.get('block', '')) == billing_block and\n",
    "                                    pad_lot(row.get('lot', '')) == billing_lot):\n",
    "                                    billing_in_results = True\n",
    "                                    break\n",
    "                            if billing_in_results:\n",
    "                                # Mark the original base BBL as matched\n",
    "                                matched_bbl_tuples.add(bbl_tuple)\n",
    "    \n",
    "    # Address search as final fallback for still-unmatched BBLs\n",
    "    still_unmatched_after_condo = []\n",
    "    for bbl in unmatched_bbl_tuples:\n",
    "        if bbl not in matched_bbl_tuples:\n",
    "            still_unmatched_after_condo.append(bbl)\n",
    "    \n",
    "    if still_unmatched_after_condo:\n",
    "        print(f\"\\nTrying address search for {len(still_unmatched_after_condo)} BBLs that still didn't match...\")\n",
    "        \n",
    "        # Build address list from HPD data for unmatched BBLs\n",
    "        address_list = []\n",
    "        for _, row in hpd_multifamily_finance_new_construction_df.iterrows():\n",
    "            bbl_tuple = decompose_bbl(row.get('BBL'))\n",
    "            if bbl_tuple and len(bbl_tuple) >= 3:\n",
    "                bbl_key = (str(bbl_tuple[0]).upper(), pad_block(bbl_tuple[1]), pad_lot(bbl_tuple[2]))\n",
    "                if bbl_key in still_unmatched_after_condo:\n",
    "                    number = str(row.get('Number', '')).strip()\n",
    "                    street = str(row.get('Street', '')).strip()\n",
    "                    borough = str(bbl_tuple[0]).upper()\n",
    "                    if number and street and borough:\n",
    "                        address_list.append((borough, number, street))\n",
    "        \n",
    "        if address_list:\n",
    "            # Deduplicate addresses\n",
    "            address_list = list(set(address_list))\n",
    "            print(f\"Searching {len(address_list)} unique addresses...\")\n",
    "            \n",
    "            address_df = query_dob_by_address(address_list)\n",
    "            \n",
    "            if not address_df.empty:\n",
    "                print(f\"Found {len(address_df)} records by address search\")\n",
    "                # Add to BISWEB results\n",
    "                if not dob_bisweb_bbl_df.empty:\n",
    "                    dob_bisweb_bbl_df = pd.concat([dob_bisweb_bbl_df, address_df], ignore_index=True)\n",
    "                else:\n",
    "                    dob_bisweb_bbl_df = address_df\n",
    "                \n",
    "                # Update matched_bbl_tuples with BBLs found by address\n",
    "                for _, row in address_df.iterrows():\n",
    "                    if pd.notna(row.get('borough')) and pd.notna(row.get('block')) and pd.notna(row.get('lot')):\n",
    "                        matched_bbl_tuples.add((str(row['borough']).upper(), pad_block(row['block']), pad_lot(row['lot'])))\n",
    "        else:\n",
    "            print(\"No addresses available for address search\")\n",
    "    else:\n",
    "        print(\"All BBLs matched - no address fallback needed\")\n",
    "    \n",
    "    \n",
    "print(f\"DOB NOW BBL: {len(dob_now_bbl_df) if 'dob_now_bbl_df' in locals() else 0} records\")\n",
    "\n",
    "# Preview each dataset\n",
    "if len(dob_bisweb_bin_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BIN sample:\")\n",
    "    display(dob_bisweb_bin_df.head())\n",
    "\n",
    "if len(dob_bisweb_bbl_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BBL sample:\")\n",
    "\n",
    "if len(dob_now_bin_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BIN sample:\")\n",
    "if len(dob_now_bbl_df) > 0:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BBL sample:\")\n",
    "display(dob_bisweb_bin_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB BISWEB (BBL) sample:\")\n",
    "display(dob_bisweb_bbl_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB NOW (BIN) sample:\")\n",
    "display(dob_now_bin_df.head())\n",
    "\n",
    "print(\"\\n\ud83d\udcca DOB NOW (BBL) sample:\")\n",
    "display(dob_now_bbl_df.head())\n",
    "\n",
    "\n",
    "# Preview BBL fallback results\n",
    "if not dob_bisweb_bbl_df.empty:\n",
    "    print(\"\\n\ud83d\udcca BISWEB BBL sample:\")\n",
    "    display(dob_bisweb_bbl_df.head())\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No BISWEB BBL results\")\n",
    "\n",
    "if not dob_now_bbl_df.empty:\n",
    "    print(\"\\n\ud83d\udcca DOB NOW BBL sample:\")\n",
    "    display(dob_now_bbl_df.head())\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No DOB NOW BBL results\")\n",
    "\n",
    "# Combine all DOB dataframes in memory\n",
    "dob_dfs_to_combine = []\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    dob_dfs_to_combine.append(dob_bisweb_bin_df)\n",
    "if not dob_bisweb_bbl_df.empty:\n",
    "    dob_dfs_to_combine.append(dob_bisweb_bbl_df)\n",
    "if not dob_now_bin_df.empty:\n",
    "    dob_dfs_to_combine.append(dob_now_bin_df)\n",
    "if not dob_now_bbl_df.empty:\n",
    "    dob_dfs_to_combine.append(dob_now_bbl_df)\n",
    "\n",
    "if dob_dfs_to_combine:\n",
    "    combined_dob_df = pd.concat(dob_dfs_to_combine, ignore_index=True, sort=False)\n",
    "    print(f\"\\n\ud83d\udcca Combined DOB Data in memory: {combined_dob_df.shape[0]} records\")\n",
    "else:\n",
    "    combined_dob_df = pd.DataFrame()\n",
    "    print(f\"\\n\u26a0\ufe0f No DOB data found\")\n",
    "\n",
    "\n",
    "# Preview combined DOB results\n",
    "if not combined_dob_df.empty:\n",
    "    print(\"\\n\ud83d\udcca Combined DOB Data sample:\")\n",
    "    display(combined_dob_df.head())\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No combined DOB data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Multifamily Finance Program (MFP) new construction projects, find those with no DOB match in any table.\n",
    "\n",
    "# Use hpd_multifamily_finance_new_construction_df from Step 3A (already filtered to MFP new construction)\n",
    "hpd_multifamily_finance_new_construction_for_matching_df = hpd_multifamily_finance_new_construction_df.copy()\n",
    "\n",
    "# Defensive: set of unique Project IDs for matching\n",
    "mfp_project_ids = set(hpd_multifamily_finance_new_construction_for_matching_df['Project ID'].unique())\n",
    "\n",
    "# Combine all DOB dataframes and normalize BIN columns\n",
    "all_dob_dfs = []\n",
    "\n",
    "# Normalize BIN columns in each DOB dataframe\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df = dob_bisweb_bin_df.copy()\n",
    "        dob_bisweb_bin_df['bin_normalized'] = dob_bisweb_bin_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df['bbl'] = dob_bisweb_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bin_df)\n",
    "\n",
    "if not dob_bisweb_bbl_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    elif 'bin' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df['bbl'] = dob_bisweb_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bbl_df)\n",
    "\n",
    "if not dob_now_bin_df.empty:\n",
    "    if 'bin' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df = dob_now_bin_df.copy()\n",
    "        dob_now_bin_df['bin_normalized'] = dob_now_bin_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df['bbl'] = dob_now_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bin_df)\n",
    "\n",
    "if not dob_now_bbl_df.empty:\n",
    "    if 'bin' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df = dob_now_bbl_df.copy()\n",
    "        dob_now_bbl_df['bin_normalized'] = dob_now_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df['bbl'] = dob_now_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bbl_df)\n",
    "\n",
    "# Use combined_dob_df from Step 3A if available, otherwise combine here\n",
    "if 'combined_dob_df' in globals() and not combined_dob_df.empty:\n",
    "    combined_dob_with_normalized_bbl_df = combined_dob_df.copy()\n",
    "    print(f'Total DOB records (from Step 3A): {len(combined_dob_with_normalized_bbl_df)}')\n",
    "elif all_dob_dfs:\n",
    "    combined_dob_with_normalized_bbl_df = pd.concat(all_dob_dfs, ignore_index=True)\n",
    "    print(f'Total DOB records: {len(combined_dob_with_normalized_bbl_df)}')\n",
    "else:\n",
    "    combined_dob_with_normalized_bbl_df = pd.DataFrame()\n",
    "    print('No DOB records found')\n",
    "\n",
    "# Prepare HPD data for matching - normalize BIN and ensure BBL is string\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df = hpd_multifamily_finance_new_construction_for_matching_df.copy()\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df['bin_normalized'] = hpd_multifamily_finance_new_construction_with_normalized_ids_df['BIN'].astype(str).str.replace('.0', '')\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df['bbl_normalized'] = hpd_multifamily_finance_new_construction_with_normalized_ids_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "\n",
    "# Join on BIN first\n",
    "if not combined_dob_with_normalized_bbl_df.empty and 'bin_normalized' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    hpd_matched_on_bin_df = pd.merge(\n",
    "        hpd_multifamily_finance_new_construction_with_normalized_ids_df,\n",
    "        combined_dob_with_normalized_bbl_df[['bin_normalized']].drop_duplicates(),\n",
    "        on='bin_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bin = set(hpd_matched_on_bin_df['Project ID'].unique())\n",
    "    print(f'Projects matched on BIN: {len(matched_project_ids_bin)}')\n",
    "else:\n",
    "    matched_project_ids_bin = set()\n",
    "\n",
    "# Join on BBL for those that didn't match on BIN\n",
    "hpd_unmatched_on_bin_df = hpd_multifamily_finance_new_construction_with_normalized_ids_df[~hpd_multifamily_finance_new_construction_with_normalized_ids_df['Project ID'].isin(matched_project_ids_bin)]\n",
    "\n",
    "# Initialize BBL matching result\n",
    "matched_project_ids_bbl = set()\n",
    "\n",
    "# Reconstruct BBL in DOB data for sources that don't have it (like BISWEB)\n",
    "# Reconstruct BBL from borough, block, lot for records that don't have it\n",
    "def reconstruct_bbl(row):\n",
    "    if pd.isna(row.get('borough')) or pd.isna(row.get('block')) or pd.isna(row.get('lot')):\n",
    "        return None\n",
    "    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "    borough_code = borough_map.get(str(row['borough']).upper(), None)\n",
    "    if not borough_code:\n",
    "        return None\n",
    "    # Remove leading zeros from block/lot for BBL reconstruction\n",
    "    block_str = str(int(float(str(row['block']).replace('.0', ''))))\n",
    "    lot_str = str(int(float(str(row['lot']).replace('.0', ''))))\n",
    "    # Reconstruct: borough(1) + block(5) + lot(4) = 10 digits\n",
    "    bbl_str = borough_code + block_str.zfill(5) + lot_str.zfill(4)\n",
    "    return bbl_str.zfill(10)\n",
    "\n",
    "# Always reconstruct BBL for records that need it (BISWEB data doesn't have bbl column)\n",
    "combined_dob_with_normalized_bbl_df['bbl_reconstructed'] = combined_dob_with_normalized_bbl_df.apply(reconstruct_bbl, axis=1)\n",
    "# Normalize BBL in DOB data (use existing bbl or reconstructed)\n",
    "# Use bbl column if available, otherwise use reconstructed BBL\n",
    "if 'bbl' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    # Use existing bbl column, normalized to 10 digits\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    # Fill missing values with reconstructed BBL\n",
    "    if 'bbl_reconstructed' in combined_dob_with_normalized_bbl_df.columns:\n",
    "        combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl_normalized'].fillna(combined_dob_with_normalized_bbl_df['bbl_reconstructed'])\n",
    "elif 'bbl_reconstructed' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    # Use reconstructed BBL if no bbl column exists\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl_reconstructed']\n",
    "else:\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = None\n",
    "\n",
    "# Now match on BBL\n",
    "if 'bbl_normalized' in combined_dob_with_normalized_bbl_df.columns and combined_dob_with_normalized_bbl_df['bbl_normalized'].notna().any():\n",
    "    hpd_matched_on_bbl_df = pd.merge(\n",
    "        hpd_unmatched_on_bin_df,\n",
    "        combined_dob_with_normalized_bbl_df[['bbl_normalized']].drop_duplicates(),\n",
    "        on='bbl_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bbl = set(hpd_matched_on_bbl_df['Project ID'].unique())\n",
    "    print(f'Projects matched on BBL (fallback): {len(matched_project_ids_bbl)}')\n",
    "else:\n",
    "    matched_project_ids_bbl = set()\n",
    "    print('No BBL data available for matching')\n",
    "\n",
    "# Combine all matched project IDs\n",
    "dob_matched_project_ids = matched_project_ids_bin | matched_project_ids_bbl\n",
    "\n",
    "# Find projects without DOB matches\n",
    "mfp_projects_without_dob = mfp_project_ids - dob_matched_project_ids\n",
    "\n",
    "print(f'\\nTotal Multifamily Finance Program new construction projects: {len(mfp_project_ids)}')\n",
    "print(f'Projects with DOB matches: {len(dob_matched_project_ids)}')\n",
    "print(f'Number of these with NO DOB row in any table: {len(mfp_projects_without_dob)}')\n",
    "\n",
    "# Debug: show a sample of matched and unmatched projects\n",
    "if len(matched_project_ids_bin) > 0:\n",
    "    print(f'\\nSample matched on BIN: {list(matched_project_ids_bin)[:3]}')\n",
    "if len(matched_project_ids_bbl) > 0:\n",
    "    print(f'Sample matched on BBL: {list(matched_project_ids_bbl)[:3]}')\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(f'Sample unmatched: {list(mfp_projects_without_dob)[:3]}')\n",
    "\n",
    "# DEBUG: Analyze a sample project to understand matching\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    sample_project_id = list(mfp_projects_without_dob)[0]\n",
    "    sample_project = hpd_multifamily_finance_new_construction_for_matching_df[hpd_multifamily_finance_new_construction_for_matching_df['Project ID'] == sample_project_id]\n",
    "    print(f'\\n=== DEBUG: Sample unmatched project ===')\n",
    "    print(f'Project ID: {sample_project_id}')\n",
    "    print(f'Number of buildings in project: {len(sample_project)}')\n",
    "    sample_bins = sample_project['BIN'].dropna().astype(str).str.replace('.0', '').tolist()\n",
    "    sample_bbls = sample_project['BBL'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).tolist()\n",
    "    print(f'BINs in project: {sample_bins[:5]}')\n",
    "    print(f'BBLs in project: {sample_bbls[:5]}')\n",
    "    \n",
    "    # Check if these BINs/BBLs exist in DOB data\n",
    "    if not combined_dob_with_normalized_bbl_df.empty:\n",
    "        if 'bin_normalized' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            dob_bins = set(combined_dob_with_normalized_bbl_df['bin_normalized'].dropna().astype(str).unique())\n",
    "            matching_bins = []\n",
    "            for b in sample_bins:\n",
    "                if b in dob_bins:\n",
    "                    matching_bins.append(b)\n",
    "            print(f'BINs found in DOB data: {matching_bins[:5] if matching_bins else \"None\"}')\n",
    "        if 'bbl' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            dob_bbls = set(combined_dob_with_normalized_bbl_df['bbl'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).unique())\n",
    "            matching_bbls = []\n",
    "            for b in sample_bbls:\n",
    "                if b in dob_bbls:\n",
    "                    matching_bbls.append(b)\n",
    "            print(f'BBLs found in DOB data: {matching_bbls[:5] if matching_bbls else \"None\"}')\n",
    "# Show the head of the table of unmatched projects (project-level)\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(\"\\nHead of unmatched Multifamily Finance Program new construction projects:\")\n",
    "    hpd_multifamily_finance_new_construction_unmatched_projects_df = hpd_multifamily_finance_new_construction_for_matching_df[hpd_multifamily_finance_new_construction_for_matching_df['Project ID'].isin(mfp_projects_without_dob)].copy()\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'BBL' in hpd_multifamily_finance_new_construction_unmatched_projects_df.columns:\n",
    "        hpd_multifamily_finance_new_construction_unmatched_projects_df['BBL'] = hpd_multifamily_finance_new_construction_unmatched_projects_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    excluded_cols = [\n",
    "        \"Lot Area\", \"Available Units\", \"Privately Financed Units\", \"Extremely Low Income Units\",\n",
    "        \"Very Low Income Units\", \"Low Income Units\", \"Moderate Income Units\", \"Middle Income Units\",\n",
    "        \"Studio Units\", \"One Bedroom Units\", \"Two Bedroom Units\", \"Three Bedroom Units\",\n",
    "        \"Four Bedroom Units\", \"Five Bedroom Units\", \"Six Bedroom Units\", \"Unknown Bedroom Units\",\n",
    "    ][:15]  # Limit extra-wide tables in notebook\n",
    "    display_cols = []\n",
    "    for c in hpd_multifamily_finance_new_construction_unmatched_projects_df.columns:\n",
    "        if c not in excluded_cols:\n",
    "            display_cols.append(c)\n",
    "    display(hpd_multifamily_finance_new_construction_unmatched_projects_df[display_cols].head(10))\n",
    "else:\n",
    "    print(\"\\nAll Multifamily Finance Program projects matched to DOB data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbl_to_boro_block_lot_and_name(bbl):\n",
    "    \"\"\"\n",
    "    Convert a NYC BBL (Borough-Block-Lot) string (like '3015560003') to a tuple:\n",
    "    (borough_number, borough_name, block, lot).\n",
    "    Borough number is int (1-5). Name is string (\"Manhattan\", \"Bronx\", ...).\n",
    "\n",
    "    Example:\n",
    "        bbl_to_boro_block_lot_and_name('3015560003')\n",
    "        -> (3, 'Brooklyn', '01556', '0003')\n",
    "    \"\"\"\n",
    "    boro_names = {\n",
    "        1: \"Manhattan\",\n",
    "        2: \"Bronx\",\n",
    "        3: \"Brooklyn\",\n",
    "        4: \"Queens\",\n",
    "        5: \"Staten Island\",\n",
    "    }\n",
    "    bbl_str = str(bbl).zfill(10)\n",
    "    borough_num = int(bbl_str[0])\n",
    "    borough_name = boro_names.get(borough_num, \"Unknown\")\n",
    "    block = bbl_str[1:6]\n",
    "    lot = bbl_str[6:10]\n",
    "    return (borough_num, borough_name, block, lot)\n",
    "\n",
    "# Example usage\n",
    "bbl_example = '3015560003'\n",
    "boro_num, boro_name, block, lot = bbl_to_boro_block_lot_and_name(bbl_example)\n",
    "print(f\"BBL {bbl_example} -> Borough {boro_num} ({boro_name}), Block {block}, Lot {lot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Step 3B: Query Certificate of Occupancy\n",
    "\n",
    "Search for Certificate of Occupancy filings.\n",
    "\n",
    "**Depends on:** Step 2\n",
    "**Options:**\n",
    "- Set `skip_co = True` to use existing CO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B Configuration\n",
    "skip_co = False  # Set to True to use existing CO data\n",
    "co_output_path = None  # Custom CO output path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3B: QUERY CERTIFICATE OF OCCUPANCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract BINs from hpd_multifamily_finance_new_construction_df (in memory from Step 3A)\n",
    "bins_list = _extract_bins_from_df(hpd_multifamily_finance_new_construction_df)\n",
    "print(f\"\\n\ud83d\udccb Extracted {len(bins_list)} BINs from filtered dataset\")\n",
    "\n",
    "if skip_co:\n",
    "    print(\"\u23ed\ufe0f Using existing CO data\")\n",
    "    # Look for existing CO files\n",
    "    co_output = Path(co_output_path) if co_output_path else Path(\n",
    "        f\"data/processed/workflow_bins_co_filings.csv\"\n",
    "    )\n",
    "    alt_co_path = Path(f\"data/external/workflow_bins_co_filings.csv\")\n",
    "    if co_output.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing CO data at {co_output}\")\n",
    "        co_filings_df = pd.read_csv(co_output)\n",
    "    elif alt_co_path.exists():\n",
    "        print(f\"\ud83d\udcc1 Using existing CO data from external folder: {alt_co_path}\")\n",
    "        co_filings_df = pd.read_csv(alt_co_path)\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No existing CO data found\")\n",
    "        co_filings_df = None\n",
    "else:\n",
    "    print(f\"\ud83c\udfdb\ufe0f Querying CO APIs using {len(bins_list)} BINs (in memory)\")\n",
    "    co_filings_df = _query_co_filings_from_bins(bins_list, output_path=None)\n",
    "\n",
    "# Display CO data if available\n",
    "if co_filings_df is not None:\n",
    "    print(f\"\ud83d\udcca Certificate of Occupancy Data: {co_filings_df.shape[0]} records\")\n",
    "    print(\"Columns:\")\n",
    "    for col in co_filings_df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Sample CO Data:\")\n",
    "    display(co_filings_df.head())\n",
    "    \n",
    "    # Show some statistics\n",
    "    if \"issue_date\" in co_filings_df.columns:\n",
    "        print(\"\\n\ud83d\udcc8 CO Issue Date Statistics:\")\n",
    "        display(co_filings_df[\"issue_date\"].describe())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No CO data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 4: Generate Timelines and Charts\n",
    "\n",
    "Create timeline visualizations from enriched data.\n",
    "\n",
    "**Depends on:** Steps 2, 3A\n",
    "**Options:**\n",
    "- Set `skip_join = True` to skip timeline creation\n",
    "- Set `skip_charts = True` to skip chart generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 Configuration\n",
    "skip_join = False   # Set to True to skip timeline creation\n",
    "skip_charts = False # Set to True to skip chart generation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: GENERATE TIMELINES AND CHARTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if skip_join:\n",
    "    print(\"\u23ed\ufe0f Skipping timeline join step.\")\n",
    "else:\n",
    "    # Check if we have the required dataframes in memory\n",
    "    if 'combined_dob_df' not in globals() or combined_dob_df.empty:\n",
    "        print(\"\u26a0\ufe0f No DOB data available; skipping timeline creation.\")\n",
    "    else:\n",
    "        print(\"\ud83d\udd17 Building timelines from in-memory dataframes...\")\n",
    "        \n",
    "        # Create temporary files for timeline function (it expects file paths)\n",
    "        import tempfile\n",
    "        temp_building = Path(tempfile.mktemp(suffix=\"_buildings.csv\"))\n",
    "        temp_dob = Path(tempfile.mktemp(suffix=\"_dob.csv\"))\n",
    "        temp_co = Path(tempfile.mktemp(suffix=\"_co.csv\")) if 'co_filings_df' in globals() and co_filings_df is not None and not co_filings_df.empty else None\n",
    "        \n",
    "        # Write dataframes to temporary files\n",
    "        hpd_multifamily_finance_new_construction_df.to_csv(temp_building, index=False)\n",
    "        combined_dob_df.to_csv(temp_dob, index=False)\n",
    "        if temp_co:\n",
    "            co_filings_df.to_csv(temp_co, index=False)\n",
    "        \n",
    "        try:\n",
    "            create_separate_timelines(\n",
    "                str(temp_building),\n",
    "                str(temp_dob),\n",
    "                str(temp_co) if temp_co else None,\n",
    "            )\n",
    "            \n",
    "            # Load timeline results into dataframes\n",
    "            # Function replaces .csv with _hpd_financed_timeline.csv\n",
    "            hpd_timeline_path = Path(str(temp_building).replace('.csv', '_hpd_financed_timeline.csv'))\n",
    "            private_timeline_path = Path(str(temp_building).replace('.csv', '_privately_financed_timeline.csv'))\n",
    "            \n",
    "            if hpd_timeline_path.exists():\n",
    "                hpd_timeline_df = pd.read_csv(hpd_timeline_path)\n",
    "                print(f\"\\n\ud83d\udcca HPD Financed Timeline Data ({hpd_timeline_df.shape[0]} records):\")\n",
    "                display(hpd_timeline_df.head())\n",
    "                \n",
    "                if \"Event\" in hpd_timeline_df.columns:\n",
    "                    print(\"\\n\ud83d\udcc8 Event Types in HPD Timeline:\")\n",
    "                    display(hpd_timeline_df[\"Event\"].value_counts())\n",
    "            \n",
    "            if private_timeline_path.exists():\n",
    "                private_timeline_df = pd.read_csv(private_timeline_path)\n",
    "                print(f\"\\n\ud83d\udcca Privately Financed Timeline Data ({private_timeline_df.shape[0]} records):\")\n",
    "                display(private_timeline_df.head())\n",
    "                \n",
    "                if \"Event\" in private_timeline_df.columns:\n",
    "                    print(\"\\n\ud83d\udcc8 Event Types in Private Timeline:\")\n",
    "                    display(private_timeline_df[\"Event\"].value_counts())\n",
    "        finally:\n",
    "            # Clean up temporary files\n",
    "            if temp_building.exists():\n",
    "                temp_building.unlink()\n",
    "            if temp_dob.exists():\n",
    "                temp_dob.unlink()\n",
    "            if temp_co and temp_co.exists():\n",
    "                temp_co.unlink()\n",
    "\n",
    "if skip_charts:\n",
    "    print(\"\u23ed\ufe0f Skipping chart generation.\")\n",
    "else:\n",
    "    # Charts\n",
    "    print(\"\\n\ud83d\udcc8 Generating charts...\")\n",
    "    # Charts would need timeline files, which are created above\n",
    "    # For now, skip chart generation if timelines weren't created\n",
    "    if 'hpd_timeline_df' in locals() or 'private_timeline_df' in locals():\n",
    "        print(\"\u26a0\ufe0f Chart generation from in-memory dataframes not yet implemented.\")\n",
    "        print(\"   Timeline dataframes are available in memory (hpd_timeline_df, private_timeline_df)\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No timeline data available for chart generation.\")\n",
    "\n",
    "print(\"\\n\u2705 Step 4 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Final Summary\n",
    "\n",
    "Generate data quality report and workflow summary.\n",
    "\n",
    "**Optional:** Run this at the end to see final statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\ud83d\udcca FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\ud83c\udf89 WORKFLOW COMPLETED!\")\n",
    "\n",
    "# Summary of what we accomplished\n",
    "print(\"\\n\ud83d\udccb WORKFLOW SUMMARY:\")\n",
    "try:\n",
    "    print(f\"\u2022 HPD Records Processed (New Construction): {len(hpd_new_construction_df):,}\")\n",
    "except NameError:\n",
    "    print(\"\u2022 HPD Records: Step 1 not run\")\n",
    "try:\n",
    "    print(f\"\u2022 HPD Multifamily Finance Program (New Construction): {len(hpd_multifamily_finance_new_construction_df):,}\")\n",
    "except NameError:\n",
    "    print(\"\u2022 HPD Multifamily Finance Program: Step 3A not run\")\n",
    "try:\n",
    "    if 'combined_dob_df' in globals() and combined_dob_df is not None and not combined_dob_df.empty:\n",
    "        print(f\"\u2022 DOB Filings Found: {len(combined_dob_df):,}\")\n",
    "    else:\n",
    "        print(\"\u2022 DOB Filings: No data\")\n",
    "except NameError:\n",
    "    print(\"\u2022 DOB Filings: Step 3A not run\")\n",
    "try:\n",
    "    if 'co_filings_df' in globals() and co_filings_df is not None and not co_filings_df.empty:\n",
    "        print(f\"\u2022 CO Filings Found: {len(co_filings_df):,}\")\n",
    "    else:\n",
    "        print(\"\u2022 CO Filings: No data\")\n",
    "except NameError:\n",
    "    print(\"\u2022 CO Filings: Step 3B not run\")\n",
    "\n",
    "print(\"\\n\u2705 Notebook workflow complete!\")\n",
    "print(\"Each step showed dataframe views for inspection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}