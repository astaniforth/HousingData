{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Workflow Notebook\n",
    "\n",
    "Modular workflow where you can run individual steps independently.\n",
    "Run cells in order or skip any steps you don't need.\n",
    "\n",
    "Each step shows dataframe views and statistics for inspection.\n",
    "\n",
    "## Quick Start\n",
    "- Run **Setup** cell first\n",
    "- Then run any combination of Step 1-4 cells\n",
    "- Skip cells you don't want to execute\n",
    "- Each cell is self-contained and shows results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup\n",
    "\n",
    "Run this cell first to import modules and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n",
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Import our workflow modules\n",
    "from fetch_affordable_housing_data import update_local_data, verify_and_fetch_hpd_data, verify_and_fetch_hpd_projects_data\n",
    "from query_dob_filings import query_dob_bisweb_bin, query_dob_bisweb_bbl, query_dobnow_bin, query_dobnow_bbl, decompose_bbl, query_condo_lots_for_bbl, query_dob_by_address, pad_block, pad_lot\n",
    "from query_co_filings import query_co_api, DOB_NOW_CO_URL, DOB_CO_URL\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "\n",
    "# Helper functions\n",
    "def _normalize_bin(bin_value) -> Optional[str]:\n",
    "    \"\"\"Normalize BIN to a clean string.\"\"\"\n",
    "    if pd.isna(bin_value):\n",
    "        return None\n",
    "    try:\n",
    "        return str(int(float(bin_value)))\n",
    "    except (TypeError, ValueError):\n",
    "        value = str(bin_value).strip()\n",
    "        return value or None\n",
    "\n",
    "def _extract_bins_from_df(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Extract BINs from a DataFrame and return as a list for CO searches.\"\"\"\n",
    "    candidate_cols = []\n",
    "    for col in df.columns:\n",
    "        if col.lower() in (\"bin\", \"bin_normalized\"):\n",
    "            candidate_cols.append(col)\n",
    "    if not candidate_cols:\n",
    "        raise SystemExit(f\"Could not find a BIN column in DataFrame\")\n",
    "\n",
    "    bins = []\n",
    "    for val in df[candidate_cols[0]].dropna():\n",
    "        normalized = _normalize_bin(val)\n",
    "        if normalized:\n",
    "            bins.append(normalized)\n",
    "    \n",
    "    # Remove duplicates using set, then sort\n",
    "    unique_bins = set()\n",
    "    for b in bins:\n",
    "        if b:\n",
    "            unique_bins.add(b)\n",
    "    return sorted(unique_bins)\n",
    "\n",
    "def _query_co_filings_from_bins_and_bbls(bin_list: list[str], bbl_list: list[str] = None, output_path: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"Query CO filings using a list of BINs and BBLs (no file needed).\"\"\"\n",
    "    # Convert BINs to integers for API query\n",
    "    bin_ints = []\n",
    "    for bin_str in bin_list:\n",
    "        if str(bin_str).isdigit():\n",
    "            bin_ints.append(int(bin_str))\n",
    "    bins = sorted(list(set(bin_ints)))\n",
    "    \n",
    "    # Query DOB NOW Certificate of Occupancy API\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QUERYING DOB NOW CERTIFICATE OF OCCUPANCY\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_now_co = query_co_api(DOB_NOW_CO_URL, bins, bin_column=\"bin\")\n",
    "    \n",
    "    # Query DOB Certificate Of Occupancy API\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUERYING DOB CERTIFICATE OF OCCUPANCY\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_co = query_co_api(DOB_CO_URL, bins, bin_column=\"bin_number\")\n",
    "    \n",
    "    # Query by BBL for records not found by BIN\n",
    "    dob_now_co_bbl = pd.DataFrame()\n",
    "    dob_co_bbl = pd.DataFrame()\n",
    "    \n",
    "    if bbl_list and len(bbl_list) > 0:\n",
    "        # Convert BBLs to integers for API query\n",
    "        bbl_ints = []\n",
    "        for bbl_str in bbl_list:\n",
    "            if str(bbl_str).isdigit():\n",
    "                bbl_ints.append(int(bbl_str))\n",
    "        bbls = sorted(list(set(bbl_ints)))\n",
    "        \n",
    "        if len(bbls) > 0:\n",
    "            # Query DOB NOW CO by BBL\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"QUERYING DOB NOW CERTIFICATE OF OCCUPANCY BY BBL\")\n",
    "            print(\"=\" * 70)\n",
    "            dob_now_co_bbl = query_co_api(DOB_NOW_CO_URL, bbls, bin_column=\"bbl\")\n",
    "            \n",
    "            # Query DOB CO by BBL\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"QUERYING DOB CERTIFICATE OF OCCUPANCY BY BBL\")\n",
    "            print(\"=\" * 70)\n",
    "            dob_co_bbl = query_co_api(DOB_CO_URL, bbls, bin_column=\"bbl\")\n",
    "            \n",
    "            # Add source and normalize columns\n",
    "            if not dob_now_co_bbl.empty:\n",
    "                print(f\"\\nDOB NOW CO Filings (by BBL): {len(dob_now_co_bbl)} records\")\n",
    "                dob_now_co_bbl['source'] = 'DOB_NOW_CO_BBL'\n",
    "                if 'bbl' in dob_now_co_bbl.columns:\n",
    "                    dob_now_co_bbl['bbl_normalized'] = dob_now_co_bbl['bbl'].astype(str).str.zfill(10)\n",
    "            \n",
    "            if not dob_co_bbl.empty:\n",
    "                print(f\"\\nDOB CO Filings (by BBL): {len(dob_co_bbl)} records\")\n",
    "                dob_co_bbl['source'] = 'DOB_CO_BBL'\n",
    "                if 'bbl' in dob_co_bbl.columns:\n",
    "                    dob_co_bbl['bbl_normalized'] = dob_co_bbl['bbl'].astype(str).str.zfill(10)\n",
    "    \n",
    "    # Combine results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not dob_now_co.empty:\n",
    "        print(f\"\\nDOB NOW CO Filings: {len(dob_now_co)} records\")\n",
    "        dob_now_co['source'] = 'DOB_NOW_CO'\n",
    "        if 'bin' in dob_now_co.columns:\n",
    "            dob_now_co['bin_normalized'] = dob_now_co['bin'].astype(str)\n",
    "    \n",
    "    if not dob_co.empty:\n",
    "        print(f\"\\nDOB CO Filings: {len(dob_co)} records\")\n",
    "        dob_co['source'] = 'DOB_CO'\n",
    "        if 'bin_number' in dob_co.columns:\n",
    "            dob_co['bin_normalized'] = dob_co['bin_number'].astype(str)\n",
    "    \n",
    "    # Combine all dataframes (BIN and BBL results)\n",
    "    dfs_to_combine = []\n",
    "    if not dob_now_co.empty:\n",
    "        dfs_to_combine.append(dob_now_co)\n",
    "    if not dob_co.empty:\n",
    "        dfs_to_combine.append(dob_co)\n",
    "    if not dob_now_co_bbl.empty:\n",
    "        dfs_to_combine.append(dob_now_co_bbl)\n",
    "    if not dob_co_bbl.empty:\n",
    "        dfs_to_combine.append(dob_co_bbl)\n",
    "    \n",
    "    if len(dfs_to_combine) >= 2:\n",
    "        all_cols = list(set([col for df in dfs_to_combine for col in df.columns]))\n",
    "        if 'bin_normalized' not in all_cols:\n",
    "            all_cols.append('bin_normalized')\n",
    "        if 'source' not in all_cols:\n",
    "            all_cols.append('source')\n",
    "        dob_now_co_aligned = dob_now_co.reindex(columns=all_cols)\n",
    "        dob_co_aligned = dob_co.reindex(columns=all_cols)\n",
    "        combined = pd.concat([dob_now_co_aligned, dob_co_aligned], ignore_index=True)\n",
    "    elif not dob_now_co.empty:\n",
    "        combined = dob_now_co.copy()\n",
    "    elif not dob_co.empty:\n",
    "        combined = dob_co.copy()\n",
    "    else:\n",
    "        print(\"\\nNo CO filings found in either API\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nTotal combined records: {len(combined)}\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 1: Fetch HPD Data\n",
    "\n",
    "Load or refresh the HPD affordable housing dataset.\n",
    "\n",
    "**Options:**\n",
    "- Set `refresh_data = True` to fetch fresh data\n",
    "- Set `refresh_data = False` to use existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: FETCH HPD DATA\n",
      "======================================================================\n",
      "Verifying local HPD data against API...\n",
      "======================================================================\n",
      "STEP 1: VERIFY AND FETCH HPD DATA\n",
      "======================================================================\n",
      "Local HPD data file not found at data/raw/Affordable_Housing_Production_by_Building.csv\n",
      "Fetching fresh data from API...\n",
      "Fetching affordable housing data from NYC Open Data API...\n",
      "Endpoint: https://data.cityofnewyork.us/resource/hg8x-zxpr.json\n",
      "Fetching records 1-1000...\n",
      "  Retrieved 1000 records (total: 1,000)\n",
      "Fetching records 1001-2000...\n",
      "  Retrieved 1000 records (total: 2,000)\n",
      "Fetching records 2001-3000...\n",
      "  Retrieved 1000 records (total: 3,000)\n",
      "Fetching records 3001-4000...\n",
      "  Retrieved 1000 records (total: 4,000)\n",
      "Fetching records 4001-5000...\n",
      "  Retrieved 1000 records (total: 5,000)\n",
      "Fetching records 5001-6000...\n",
      "  Retrieved 1000 records (total: 6,000)\n",
      "Fetching records 6001-7000...\n",
      "  Retrieved 1000 records (total: 7,000)\n",
      "Fetching records 7001-8000...\n",
      "  Retrieved 1000 records (total: 8,000)\n",
      "Fetching records 8001-9000...\n",
      "  Retrieved 604 records (total: 8,604)\n",
      "Fetching records 8605-9604...\n",
      "\n",
      "Completed! Retrieved 8,604 total records\n",
      "\n",
      "Enriching building data with project-level information...\n",
      "======================================================================\n",
      "VERIFYING HPD PROJECTS CACHE\n",
      "======================================================================\n",
      "Local HPD projects cache file not found at data/raw/Affordable_Housing_Production_by_Project.csv\n",
      "Fetching fresh data from API...\n",
      "Fetching HPD Projects data from NYC Open Data API...\n",
      "Endpoint: https://data.cityofnewyork.us/resource/hq68-rnsi.json\n",
      "Fetching records 1-1000...\n",
      "Error fetching data: 500 Server Error: Server Error for url: https://data.cityofnewyork.us/resource/hq68-rnsi.json?%24limit=1000&%24offset=0&%24order=project_id\n",
      "\n",
      "Completed! Retrieved 0 total records\n",
      "Saved fresh data to: data/raw/Affordable_Housing_Production_by_Project.csv\n",
      "Loaded 0 project records\n",
      "Warning: Could not enrich with project data: 'project_id'\n",
      "Continuing with building-level data only\n",
      "Saved fresh data to: data/raw/Affordable_Housing_Production_by_Building.csv\n",
      "üèóÔ∏è Filtered to New Construction only:\n",
      "  Original: 8,604 projects, 401,769 total units\n",
      "  Filtered: 4,216 projects (49.0%), 219,300 total units (54.6%)\n",
      "  Removed: 4,388 non-new construction projects (51.0%), 182,469 units filtered out (45.4%)\n",
      "‚úÖ Step 1 complete: 4,216 records loaded\n",
      "üìÅ Data location: data/raw/Affordable_Housing_Production_by_Building.csv\n",
      "\n",
      "üîç HPD Dataset Overview (New Construction only):\n",
      "Shape: (4216, 45)\n",
      "\n",
      "Columns:\n",
      "  - Project ID\n",
      "  - Project Name\n",
      "  - Project Start Date\n",
      "  - Project Completion Date\n",
      "  - Building ID\n",
      "  - Number\n",
      "  - Street\n",
      "  - Borough\n",
      "  - Postcode\n",
      "  - BBL\n",
      "  - BIN\n",
      "  - Community Board\n",
      "  - Council District\n",
      "  - Census Tract\n",
      "  - NTA - Neighborhood Tabulation Area\n",
      "  - Latitude\n",
      "  - Longitude\n",
      "  - Latitude (Internal)\n",
      "  - Longitude (Internal)\n",
      "  - Building Completion Date\n",
      "  - Reporting Construction Type\n",
      "  - Extended Affordability Only\n",
      "  - Prevailing Wage Status\n",
      "  - Extremely Low Income Units\n",
      "  - Very Low Income Units\n",
      "  - Low Income Units\n",
      "  - Moderate Income Units\n",
      "  - Middle Income Units\n",
      "  - Other Income Units\n",
      "  - Studio Units\n",
      "  - 1-BR Units\n",
      "  - 2-BR Units\n",
      "  - 3-BR Units\n",
      "  - 4-BR Units\n",
      "  - 5-BR Units\n",
      "  - 6-BR+ Units\n",
      "  - Unknown-BR Units\n",
      "  - Counted Rental Units\n",
      "  - Counted Homeownership Units\n",
      "  - All Counted Units\n",
      "  - Total Units\n",
      "  - project_Program Group\n",
      "  - project_Extended Affordability Status\n",
      "  - project_Prevailing Wage Status\n",
      "  - project_Planned Tax Benefit\n",
      "\n",
      "üìä Sample Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project ID</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Project Start Date</th>\n",
       "      <th>Project Completion Date</th>\n",
       "      <th>Building ID</th>\n",
       "      <th>Number</th>\n",
       "      <th>Street</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>BBL</th>\n",
       "      <th>...</th>\n",
       "      <th>6-BR+ Units</th>\n",
       "      <th>Unknown-BR Units</th>\n",
       "      <th>Counted Rental Units</th>\n",
       "      <th>Counted Homeownership Units</th>\n",
       "      <th>All Counted Units</th>\n",
       "      <th>Total Units</th>\n",
       "      <th>project_Program Group</th>\n",
       "      <th>project_Extended Affordability Status</th>\n",
       "      <th>project_Prevailing Wage Status</th>\n",
       "      <th>project_Planned Tax Benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44218</td>\n",
       "      <td>MEC E. 125TH ST. PARCEL B WEST</td>\n",
       "      <td>2018-12-31T00:00:00.000</td>\n",
       "      <td>None</td>\n",
       "      <td>987329</td>\n",
       "      <td>2319</td>\n",
       "      <td>3 AVENUE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10035</td>\n",
       "      <td>1017907501</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>297.0</td>\n",
       "      <td>404</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44223</td>\n",
       "      <td>ROCHESTER SUYDAM PHASE 1</td>\n",
       "      <td>2021-06-30T00:00:00.000</td>\n",
       "      <td>None</td>\n",
       "      <td>927737</td>\n",
       "      <td>335</td>\n",
       "      <td>RALPH AVENUE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11233</td>\n",
       "      <td>3015560003</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44223</td>\n",
       "      <td>ROCHESTER SUYDAM PHASE 1</td>\n",
       "      <td>2021-06-30T00:00:00.000</td>\n",
       "      <td>None</td>\n",
       "      <td>969695</td>\n",
       "      <td>35</td>\n",
       "      <td>ROCHESTER AVENUE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11233</td>\n",
       "      <td>3017090009</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44223</td>\n",
       "      <td>ROCHESTER SUYDAM PHASE 1</td>\n",
       "      <td>2021-06-30T00:00:00.000</td>\n",
       "      <td>None</td>\n",
       "      <td>975702</td>\n",
       "      <td>18-22</td>\n",
       "      <td>SUYDAM PLACE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11233</td>\n",
       "      <td>3017090028</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44223</td>\n",
       "      <td>ROCHESTER SUYDAM PHASE 1</td>\n",
       "      <td>2021-06-30T00:00:00.000</td>\n",
       "      <td>None</td>\n",
       "      <td>977564</td>\n",
       "      <td>329</td>\n",
       "      <td>RALPH AVENUE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11233</td>\n",
       "      <td>3015560007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Project ID                    Project Name       Project Start Date  \\\n",
       "0      44218  MEC E. 125TH ST. PARCEL B WEST  2018-12-31T00:00:00.000   \n",
       "1      44223        ROCHESTER SUYDAM PHASE 1  2021-06-30T00:00:00.000   \n",
       "2      44223        ROCHESTER SUYDAM PHASE 1  2021-06-30T00:00:00.000   \n",
       "3      44223        ROCHESTER SUYDAM PHASE 1  2021-06-30T00:00:00.000   \n",
       "4      44223        ROCHESTER SUYDAM PHASE 1  2021-06-30T00:00:00.000   \n",
       "\n",
       "  Project Completion Date Building ID Number            Street    Borough  \\\n",
       "0                    None      987329   2319          3 AVENUE  Manhattan   \n",
       "1                    None      927737    335      RALPH AVENUE   Brooklyn   \n",
       "2                    None      969695     35  ROCHESTER AVENUE   Brooklyn   \n",
       "3                    None      975702  18-22      SUYDAM PLACE   Brooklyn   \n",
       "4                    None      977564    329      RALPH AVENUE   Brooklyn   \n",
       "\n",
       "  Postcode         BBL  ... 6-BR+ Units Unknown-BR Units Counted Rental Units  \\\n",
       "0    10035  1017907501  ...         NaN              NaN                297.0   \n",
       "1    11233  3015560003  ...         NaN              NaN                  NaN   \n",
       "2    11233  3017090009  ...         NaN              NaN                  NaN   \n",
       "3    11233  3017090028  ...         NaN              NaN                  NaN   \n",
       "4    11233  3015560007  ...         NaN              NaN                  NaN   \n",
       "\n",
       "  Counted Homeownership Units All Counted Units Total Units  \\\n",
       "0                         NaN             297.0         404   \n",
       "1                        13.0              13.0          13   \n",
       "2                         8.0               8.0           8   \n",
       "3                        15.0              15.0          15   \n",
       "4                        10.0              10.0          10   \n",
       "\n",
       "  project_Program Group project_Extended Affordability Status  \\\n",
       "0                  None                                  None   \n",
       "1                  None                                  None   \n",
       "2                  None                                  None   \n",
       "3                  None                                  None   \n",
       "4                  None                                  None   \n",
       "\n",
       "  project_Prevailing Wage Status project_Planned Tax Benefit  \n",
       "0                           None                        None  \n",
       "1                           None                        None  \n",
       "2                           None                        None  \n",
       "3                           None                        None  \n",
       "4                           None                        None  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Basic Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project ID</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Project Start Date</th>\n",
       "      <th>Project Completion Date</th>\n",
       "      <th>Building ID</th>\n",
       "      <th>Number</th>\n",
       "      <th>Street</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>BBL</th>\n",
       "      <th>...</th>\n",
       "      <th>6-BR+ Units</th>\n",
       "      <th>Unknown-BR Units</th>\n",
       "      <th>Counted Rental Units</th>\n",
       "      <th>Counted Homeownership Units</th>\n",
       "      <th>All Counted Units</th>\n",
       "      <th>Total Units</th>\n",
       "      <th>project_Program Group</th>\n",
       "      <th>project_Extended Affordability Status</th>\n",
       "      <th>project_Prevailing Wage Status</th>\n",
       "      <th>project_Planned Tax Benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4216</td>\n",
       "      <td>4216</td>\n",
       "      <td>4216</td>\n",
       "      <td>0</td>\n",
       "      <td>3184</td>\n",
       "      <td>4216</td>\n",
       "      <td>4216</td>\n",
       "      <td>4216</td>\n",
       "      <td>3182</td>\n",
       "      <td>3115</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>3029.000000</td>\n",
       "      <td>1276.000000</td>\n",
       "      <td>4206.000000</td>\n",
       "      <td>4216.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3749</td>\n",
       "      <td>2718</td>\n",
       "      <td>1850</td>\n",
       "      <td>0</td>\n",
       "      <td>3162</td>\n",
       "      <td>1965</td>\n",
       "      <td>1048</td>\n",
       "      <td>5</td>\n",
       "      <td>146</td>\n",
       "      <td>3007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>53017</td>\n",
       "      <td>CONFIDENTIAL</td>\n",
       "      <td>2016-06-27T00:00:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>975696</td>\n",
       "      <td>----</td>\n",
       "      <td>----</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11239</td>\n",
       "      <td>2035150020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>83</td>\n",
       "      <td>1032</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1032</td>\n",
       "      <td>1032</td>\n",
       "      <td>1873</td>\n",
       "      <td>165</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.688679</td>\n",
       "      <td>37.684714</td>\n",
       "      <td>1.639498</td>\n",
       "      <td>27.636472</td>\n",
       "      <td>52.016129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.993008</td>\n",
       "      <td>67.426568</td>\n",
       "      <td>4.116363</td>\n",
       "      <td>59.488316</td>\n",
       "      <td>107.460375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>1320.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Project ID  Project Name       Project Start Date  \\\n",
       "count        4216          4216                     4216   \n",
       "unique       3749          2718                     1850   \n",
       "top         53017  CONFIDENTIAL  2016-06-27T00:00:00.000   \n",
       "freq           83          1032                       84   \n",
       "mean          NaN           NaN                      NaN   \n",
       "std           NaN           NaN                      NaN   \n",
       "min           NaN           NaN                      NaN   \n",
       "25%           NaN           NaN                      NaN   \n",
       "50%           NaN           NaN                      NaN   \n",
       "75%           NaN           NaN                      NaN   \n",
       "max           NaN           NaN                      NaN   \n",
       "\n",
       "       Project Completion Date Building ID Number Street   Borough Postcode  \\\n",
       "count                        0        3184   4216   4216      4216     3182   \n",
       "unique                       0        3162   1965   1048         5      146   \n",
       "top                        NaN      975696   ----   ----  Brooklyn    11239   \n",
       "freq                       NaN           2   1032   1032      1873      165   \n",
       "mean                       NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "std                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "min                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "25%                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "50%                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "75%                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "max                        NaN         NaN    NaN    NaN       NaN      NaN   \n",
       "\n",
       "               BBL  ... 6-BR+ Units Unknown-BR Units Counted Rental Units  \\\n",
       "count         3115  ...        12.0       106.000000          3029.000000   \n",
       "unique        3007  ...         NaN              NaN                  NaN   \n",
       "top     2035150020  ...         NaN              NaN                  NaN   \n",
       "freq             9  ...         NaN              NaN                  NaN   \n",
       "mean           NaN  ...         1.0         1.688679            37.684714   \n",
       "std            NaN  ...         0.0         6.993008            67.426568   \n",
       "min            NaN  ...         1.0         1.000000             1.000000   \n",
       "25%            NaN  ...         1.0         1.000000             3.000000   \n",
       "50%            NaN  ...         1.0         1.000000             9.000000   \n",
       "75%            NaN  ...         1.0         1.000000            38.000000   \n",
       "max            NaN  ...         1.0        73.000000           611.000000   \n",
       "\n",
       "       Counted Homeownership Units All Counted Units  Total Units  \\\n",
       "count                  1276.000000       4206.000000  4216.000000   \n",
       "unique                         NaN               NaN          NaN   \n",
       "top                            NaN               NaN          NaN   \n",
       "freq                           NaN               NaN          NaN   \n",
       "mean                      1.639498         27.636472    52.016129   \n",
       "std                       4.116363         59.488316   107.460375   \n",
       "min                       1.000000          1.000000     1.000000   \n",
       "25%                       1.000000          1.000000     1.000000   \n",
       "50%                       1.000000          5.000000    10.000000   \n",
       "75%                       1.000000         20.000000    50.000000   \n",
       "max                      75.000000        611.000000  1320.000000   \n",
       "\n",
       "       project_Program Group project_Extended Affordability Status  \\\n",
       "count                      0                                     0   \n",
       "unique                     0                                     0   \n",
       "top                      NaN                                   NaN   \n",
       "freq                     NaN                                   NaN   \n",
       "mean                     NaN                                   NaN   \n",
       "std                      NaN                                   NaN   \n",
       "min                      NaN                                   NaN   \n",
       "25%                      NaN                                   NaN   \n",
       "50%                      NaN                                   NaN   \n",
       "75%                      NaN                                   NaN   \n",
       "max                      NaN                                   NaN   \n",
       "\n",
       "       project_Prevailing Wage Status project_Planned Tax Benefit  \n",
       "count                               0                           0  \n",
       "unique                              0                           0  \n",
       "top                               NaN                         NaN  \n",
       "freq                              NaN                         NaN  \n",
       "mean                              NaN                         NaN  \n",
       "std                               NaN                         NaN  \n",
       "min                               NaN                         NaN  \n",
       "25%                               NaN                         NaN  \n",
       "50%                               NaN                         NaN  \n",
       "75%                               NaN                         NaN  \n",
       "max                               NaN                         NaN  \n",
       "\n",
       "[11 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1 Configuration\n",
    "refresh_data = False  # Set to True to fetch fresh HPD data\n",
    "hpd_output_path = \"data/raw/Affordable_Housing_Production_by_Building.csv\"  # Output path for HPD data\n",
    "refresh_hpd_projects = False  # Set to True to fetch fresh HPD projects data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: FETCH HPD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start quality tracking\n",
    "\n",
    "# Handle HPD projects cache refresh if requested\n",
    "if refresh_hpd_projects:\n",
    "    print(\"Force refreshing HPD projects cache...\")\n",
    "    hpd_projects_df, hpd_projects_path = verify_and_fetch_hpd_projects_data(use_existing=False)\n",
    "    print(f\"HPD projects cache refreshed: {len(hpd_projects_df)} records\\n\")\n",
    "\n",
    "if refresh_data:\n",
    "    print(\"Fetching fresh HPD data from NYC Open Data...\")\n",
    "    hpd_df, hpd_csv = update_local_data(hpd_output_path)\n",
    "else:\n",
    "    print(\"Verifying local HPD data against API...\")\n",
    "    hpd_df, hpd_csv = verify_and_fetch_hpd_data(output_path=hpd_output_path, use_projects_cache=not refresh_hpd_projects)\n",
    "\n",
    "if not hpd_csv.exists():\n",
    "    raise SystemExit(f\"HPD dataset not found at {hpd_csv}\")\n",
    "\n",
    "# Get total units before filter\n",
    "original_count = len(hpd_df)\n",
    "original_units = hpd_df['Total Units'].sum()\n",
    "\n",
    "# Filter to New Construction only\n",
    "hpd_new_construction_df = hpd_df[hpd_df[\"Reporting Construction Type\"] == \"New Construction\"].copy()\n",
    "\n",
    "filtered_count = len(hpd_new_construction_df)\n",
    "filtered_units = hpd_new_construction_df['Total Units'].sum()\n",
    "filtered_out = original_count - filtered_count\n",
    "filtered_units_out = original_units - filtered_units\n",
    "\n",
    "print(f\"üèóÔ∏è Filtered to New Construction only:\")\n",
    "print(f\"  Original: {original_count:,} projects, {original_units:,} total units\")\n",
    "print(f\"  Filtered: {filtered_count:,} projects ({filtered_count/original_count*100:.1f}%), {filtered_units:,} total units ({filtered_units/original_units*100:.1f}%)\")\n",
    "print(f\"  Removed: {filtered_out:,} non-new construction projects ({filtered_out/original_count*100:.1f}%), {filtered_units_out:,} units filtered out ({filtered_units_out/original_units*100:.1f}%)\")\n",
    "\n",
    "print(f\"‚úÖ Step 1 complete: {len(hpd_new_construction_df):,} records loaded\")\n",
    "print(f\"üìÅ Data location: {hpd_csv}\")\n",
    "\n",
    "# Display the dataframe\n",
    "print(\"\\nüîç HPD Dataset Overview (New Construction only):\")\n",
    "print(f\"Shape: {hpd_new_construction_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in hpd_new_construction_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nüìä Sample Data:\")\n",
    "display(hpd_new_construction_df.head())\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "display(hpd_new_construction_df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Program Group'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# How many unique counts are there by project id as primary key per program group,\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# and show total units in parentheticals (but NOT for the unique project counts).\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Compute total units per Program Group (all rows)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m units_per_group = \u001b[43mhpd_new_construction_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mProgram Group\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mTotal Units\u001b[39m\u001b[33m'\u001b[39m].sum()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProgram Group counts (raw rows) (total units in parentheses):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m raw_row_counts = hpd_new_construction_df[\u001b[33m'\u001b[39m\u001b[33mProgram Group\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/core/frame.py:9183\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9186\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9189\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Program Group'"
     ]
    }
   ],
   "source": [
    "# How many unique counts are there by project id as primary key per program group,\n",
    "# and show total units in parentheticals (but NOT for the unique project counts).\n",
    "\n",
    "# Compute total units per Program Group (all rows)\n",
    "units_per_group = hpd_new_construction_df.groupby('Program Group')['Total Units'].sum()\n",
    "\n",
    "print(\"Program Group counts (raw rows) (total units in parentheses):\")\n",
    "raw_row_counts = hpd_new_construction_df['Program Group'].value_counts()\n",
    "for group, count in raw_row_counts.items():\n",
    "    units = units_per_group.get(group, 0)\n",
    "    print(f\"{group}: {count} rows ({units} units)\")\n",
    "print()\n",
    "\n",
    "# Group by Program Group, count unique Project IDs\n",
    "unique_proj_counts = hpd_new_construction_df.groupby('Program Group')['Project ID'].nunique().sort_values(ascending=False)\n",
    "unique_proj_ids = (\n",
    "    hpd_new_construction_df\n",
    "    .groupby('Program Group')\n",
    "    .apply(lambda df: df['Project ID'].unique())\n",
    ")\n",
    "\n",
    "print(\"Program Group counts (unique Project ID as primary key):\")\n",
    "for group, count in unique_proj_counts.items():\n",
    "    print(f\"{group}: {count} projects\")\n",
    "print()\n",
    "\n",
    "print(\"\\nTax Abatement by Program Group (based on unique Project ID):\")\n",
    "if 'Planned Tax Benefit' in hpd_new_construction_df.columns:\n",
    "    # For this, deduplicate by Project ID first\n",
    "    unique_project_rows = hpd_new_construction_df.drop_duplicates(subset=['Project ID'])\n",
    "    tax_abate_ct = (\n",
    "        unique_project_rows\n",
    "        .groupby('Program Group')['Planned Tax Benefit']\n",
    "        .value_counts(dropna=False)\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    # Also display total units per Program Group in this table, if desired\n",
    "    units_per_group_project = unique_project_rows.groupby('Program Group')['Total Units'].sum()\n",
    "    print(\"Total units (unique Project ID per Program Group):\")\n",
    "    display(units_per_group_project)\n",
    "    display(tax_abate_ct)\n",
    "else:\n",
    "    print(\"Column 'Planned Tax Benefit' not found in dataset.\")\n",
    "\n",
    "# Make a version of this with unit count by program and tax benefit\n",
    "if 'Planned Tax Benefit' in unique_project_rows.columns and 'Program Group' in unique_project_rows.columns:\n",
    "    units_pivot = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Total units by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(units_pivot)\n",
    "else:\n",
    "    print(\"Required columns not found for unit pivot table.\")\n",
    "\n",
    "# Calculate average units per year by Program Group and Planned Tax Benefit\n",
    "\n",
    "if 'Project Start Date' in unique_project_rows.columns and 'Total Units' in unique_project_rows.columns:\n",
    "    # Extract year from 'Project Start Date'\n",
    "    unique_project_rows = unique_project_rows.copy()\n",
    "    unique_project_rows['Project Year'] = pd.to_datetime(unique_project_rows['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    avg_units_per_year = (\n",
    "        unique_project_rows\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit', 'Project Year'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Now calculate the average units per year by program group and tax abatement\n",
    "    avg_units_table = (\n",
    "        avg_units_per_year\n",
    "        .groupby(['Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    print(\"Average units per year by Program Group and Planned Tax Benefit (unique Project ID only):\")\n",
    "    display(avg_units_table)\n",
    "else:\n",
    "    print(\"Required columns not found for average units per year table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the full raw HPD data, because we want all programs, not just Multifamily Finance Program\n",
    "if 'Project Start Date' in hpd_new_construction_df.columns and 'Total Units' in hpd_new_construction_df.columns:\n",
    "    hpd_bar_df = hpd_new_construction_df.copy()\n",
    "    hpd_bar_df['Project Year'] = pd.to_datetime(hpd_bar_df['Project Start Date'], errors='coerce').dt.year\n",
    "\n",
    "    # Only focus on desired groups\n",
    "    programs_of_interest = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "    mask = hpd_bar_df['Program Group'].isin(programs_of_interest)\n",
    "    hpd_bar_df = hpd_bar_df[mask & hpd_bar_df['Project Year'].notna()]\n",
    "\n",
    "    # Fill NAs in Planned Tax Benefit with \"None\"\n",
    "    hpd_bar_df['Planned Tax Benefit'] = hpd_bar_df['Planned Tax Benefit'].fillna('None')\n",
    "\n",
    "    # Prepare for grouped bar with stack\n",
    "    # Pivot: rows = Project Year, columns = (Program Group, Planned Tax Benefit), values = sum of units\n",
    "    pivot = (\n",
    "        hpd_bar_df\n",
    "        .groupby(['Project Year', 'Program Group', 'Planned Tax Benefit'])['Total Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure proper order of years and programs\n",
    "    years = sorted(pivot['Project Year'].dropna().unique())\n",
    "    tax_benefits = sorted(pivot['Planned Tax Benefit'].unique())\n",
    "    # Keep consistent order for bars\n",
    "    program_order = ['Multifamily Finance Program', 'Multifamily Incentives Program']\n",
    "\n",
    "    # Prepare data structure: for each year, for each program, get breakdown by tax benefit\n",
    "    bar_data = {}\n",
    "    for year in years:\n",
    "        bar_data[year] = {}\n",
    "        for prog in program_order:\n",
    "            mask = (pivot['Project Year'] == year) & (pivot['Program Group'] == prog)\n",
    "            year_prog_data = pivot[mask].set_index('Planned Tax Benefit')['Total Units'].reindex(tax_benefits, fill_value=0)\n",
    "            bar_data[year][prog] = year_prog_data.values\n",
    "\n",
    "    # Number of bars per group (2 programs), group by year, stacked by tax benefit\n",
    "    x = range(len(years))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Colors for planned tax benefits\n",
    "    color_map = cm.get_cmap('tab20', len(tax_benefits))\n",
    "    colors = []\n",
    "    for i in range(len(tax_benefits)):\n",
    "        colors.append(color_map(i))\n",
    "\n",
    "    bottoms_p1 = [0] * len(years)\n",
    "    bottoms_p2 = [0] * len(years)\n",
    "\n",
    "    # For each tax benefit, draw the stack pieces for both programs\n",
    "    legend_handles = []\n",
    "    for idx, tax in enumerate(tax_benefits):\n",
    "        values_p1 = []\n",
    "        for year in years:\n",
    "            values_p1.append(bar_data[year][program_order[0]][idx])\n",
    "        values_p2 = []\n",
    "        for year in years:\n",
    "            values_p2.append(bar_data[year][program_order[1]][idx])\n",
    "\n",
    "        x_positions_p1 = []\n",
    "        for i in x:\n",
    "            x_positions_p1.append(i - width/2)\n",
    "        x_positions_p2 = []\n",
    "        for i in x:\n",
    "            x_positions_p2.append(i + width/2)\n",
    "        \n",
    "        legend_labels = []\n",
    "        for h in legend_handles:\n",
    "            legend_labels.append(h.get_label())\n",
    "        \n",
    "        bar1 = ax.bar(\n",
    "            x_positions_p1, values_p1, width,\n",
    "            bottom=bottoms_p1, color=colors[idx],\n",
    "            label=tax if (tax not in legend_labels) else None,\n",
    "            edgecolor='black', hatch='////'\n",
    "        )\n",
    "        bar2 = ax.bar(\n",
    "            x_positions_p2, values_p2, width,\n",
    "            bottom=bottoms_p2, color=colors[idx],\n",
    "            label=None,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        legend_labels = []\n",
    "        for h in legend_handles:\n",
    "            legend_labels.append(h.get_label())\n",
    "        if tax not in legend_labels:\n",
    "            legend_handles.append(bar1)\n",
    "\n",
    "        new_bottoms_p1 = []\n",
    "        for b, v in zip(bottoms_p1, values_p1):\n",
    "            new_bottoms_p1.append(b + v)\n",
    "        bottoms_p1 = new_bottoms_p1\n",
    "        \n",
    "        new_bottoms_p2 = []\n",
    "        for b, v in zip(bottoms_p2, values_p2):\n",
    "            new_bottoms_p2.append(b + v)\n",
    "        bottoms_p2 = new_bottoms_p2\n",
    "\n",
    "    # Add year labels\n",
    "    ax.set_xticks(x)\n",
    "    year_labels = []\n",
    "    for y in years:\n",
    "        year_labels.append(str(int(y)))\n",
    "    ax.set_xticklabels(year_labels, rotation=45)\n",
    "    ax.set_xlabel(\"Project Start Year\")\n",
    "    ax.set_ylabel(\"Total Units Financed\")\n",
    "    ax.set_title(\"Units Financed by Year: Multifamily Finance and Incentives Programs\\nColored by Planned Tax Benefit\")\n",
    "\n",
    "    # Custom legend for program groups\n",
    "    progs = [\n",
    "        mpatches.Patch(color='gray', label='Multifamily Finance Program', ec='black', hatch='////'),\n",
    "        mpatches.Patch(color='gray', label='Multifamily Incentives Program', ec='black')\n",
    "    ]\n",
    "    # Only add one legend for planned tax benefit\n",
    "    handles_tax = []\n",
    "    for i in range(len(tax_benefits)):\n",
    "        handles_tax.append(plt.Rectangle((0,0),1,1, color=colors[i], edgecolor='black', label=f\"{tax_benefits[i]}\"))\n",
    "    legend1 = ax.legend(handles=handles_tax, title=\"Planned Tax Benefit\", loc='upper right')\n",
    "    ax.add_artist(legend1)\n",
    "    # Add manual tick legend for program bars\n",
    "    bar_locs = [x[0] - width/2, x[0] + width/2]\n",
    "    ax.bar(bar_locs[0], 0, width, color='white', hatch='////', ec='black', label='Multifamily Finance Program')\n",
    "    ax.bar(bar_locs[1], 0, width, color='white', ec='black', label='Multifamily Incentives Program')\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', hatch='////', edgecolor='black', label='Multifamily Finance Program'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='white', edgecolor='black', label='Multifamily Incentives Program')\n",
    "        ], title=\"Program Group\", loc='upper left'\n",
    "    )\n",
    "\n",
    "    ax.grid(True, which='major', axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Required columns ('Project Start Date', 'Total Units') not found in HPD Data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and sample: Planned Tax Benefit '421a' and Project Start Date in 2025\n",
    "if \"Planned Tax Benefit\" in hpd_new_construction_df.columns and \"Project Start Date\" in hpd_new_construction_df.columns:\n",
    "    # Filter for 421a and 2025 start year\n",
    "    mask_421a_2025 = (\n",
    "        (hpd_new_construction_df[\"Planned Tax Benefit\"] == \"421a\") &\n",
    "        (hpd_new_construction_df[\"Project Start Date\"].astype(str).str.startswith(\"2025\"))\n",
    "    )\n",
    "    hpd_421a_2025_df = hpd_new_construction_df[mask_421a_2025]\n",
    "\n",
    "    # Count unique projects (by Project ID), and total units\n",
    "    total_projects = hpd_421a_2025_df[\"Project ID\"].nunique() if \"Project ID\" in hpd_421a_2025_df.columns else len(hpd_421a_2025_df)\n",
    "    total_units = hpd_421a_2025_df[\"Total Units\"].sum() if \"Total Units\" in hpd_421a_2025_df.columns else \"N/A\"\n",
    "\n",
    "    print(f\"Total projects with Planned Tax Benefit '421a' and 2025 Start Date: {total_projects:,}\")\n",
    "    print(f\"Total units in these projects: {total_units:,}\")\n",
    "\n",
    "    # Show up to 5 sample rows\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(\"\\nSample 421a Planned Tax Benefit projects with Project Start Date in 2025:\")\n",
    "    display(hpd_421a_2025_df)\n",
    "    pd.reset_option('display.max_columns')\n",
    "else:\n",
    "    print(\"One or both of the columns 'Planned Tax Benefit' or 'Project Start Date' not found in HPD DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Multifamily Finance Program (already filtered to New Construction in Step 1)\n",
    "original_count = len(hpd_new_construction_df)\n",
    "print(f\"Using HPD data from Step 1: {original_count:,} total buildings (New Construction)\")\n",
    "\n",
    "# Apply filters: Multifamily Finance Program (already filtered to New Construction in Step 1)\n",
    "hpd_multifamily_finance_new_construction_df = hpd_new_construction_df[\n",
    "    hpd_new_construction_df[\"Program Group\"] == \"Multifamily Finance Program\"\n",
    "].copy()\n",
    "filtered_count = len(hpd_multifamily_finance_new_construction_df)\n",
    "\n",
    "print(f\"üèóÔ∏è Filtered to Multifamily Finance Program:\")\n",
    "print(f\"  Original: {original_count:,} buildings (New Construction)\")\n",
    "print(f\"  Filtered: {filtered_count:,} buildings ({filtered_count/original_count*100:.1f}%)\")\n",
    "print(f\"üìÅ Created critical DataFrame in memory: {filtered_count:,} Multifamily Finance Program (New Construction) buildings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 3: Query DOB Filings\n",
    "\n",
    "Search for DOB New Building filings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A: BIN/BBL Prep and Filtering\n",
    "\n",
    "# Use hpd_multifamily_finance_new_construction_df from Step 2.5 (in memory)\n",
    "print(f\"Using filtered dataset from Step 2.5: {len(hpd_multifamily_finance_new_construction_df):,} Multifamily Finance Program (New Construction) buildings\")\n",
    "\n",
    "# Extract BINs and BBLs from the filtered data\n",
    "bins = []\n",
    "bin_counts = hpd_multifamily_finance_new_construction_df['BIN'].value_counts()\n",
    "unique_bins = bin_counts[bin_counts == 1].index.tolist()\n",
    "# Extract BBLs properly using decompose_bbl function\n",
    "\n",
    "bbls = []\n",
    "for idx, row in hpd_multifamily_finance_new_construction_df.iterrows():\n",
    "    if pd.notna(row.get(\"BBL\")):\n",
    "        bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "        if bbl_result and len(bbl_result) >= 3:\n",
    "            borough, block, lot = bbl_result\n",
    "            bbls.append((borough, block, lot))\n",
    "\n",
    "\n",
    "# Filter out bad/placeholder BINs (e.g., 1000000, 2000000, 3000000, 4000000, 5000000)\n",
    "# These are placeholder values that don't exist in DOB\n",
    "def is_bad_bin(bin_str):\n",
    "    \"\"\"Check if BIN is a placeholder/bad value.\"\"\"\n",
    "    if not bin_str or pd.isna(bin_str) or str(bin_str).lower() == 'nan':\n",
    "        return True\n",
    "    bin_str_clean = str(bin_str).strip()\n",
    "    # Check for pattern: [1-5]000000 (borough placeholder BINs)\n",
    "    if bin_str_clean in ['1000000', '2000000', '3000000', '4000000', '5000000']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if 'BIN' in hpd_multifamily_finance_new_construction_df.columns:\n",
    "    bins = []\n",
    "for b in hpd_multifamily_finance_new_construction_df['BIN'].dropna():\n",
    "        b_str = str(b)\n",
    "        if b_str != 'nan':\n",
    "            b_clean = b_str.replace('.0', '')\n",
    "            if not is_bad_bin(b_clean) and b_clean in unique_bins:\n",
    "                bins.append(b_clean)\n",
    "\n",
    "if 'BBL' in hpd_multifamily_finance_new_construction_df.columns:\n",
    "    bbl_col = hpd_multifamily_finance_new_construction_df['BBL'].astype(str).str.zfill(10)\n",
    "    bbls = []\n",
    "    for bbl_val in bbl_col:\n",
    "        if len(bbl_val) == 10:\n",
    "            bbl_tuple = (\n",
    "                bbl_val[0],                     # borough code (as string)\n",
    "                bbl_val[1:6],                   # block (padded 5 chars)\n",
    "                bbl_val[6:]                     # lot   (padded 4 chars)\n",
    "            )\n",
    "            bbls.append(bbl_tuple)\n",
    "\n",
    "print(f\"\\nüìã Prepared {len(bins)} BINs and {len(bbls)} BBLs for DOB queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BISWEB BIN for all buildings\n",
    "\n",
    "print(\"BISWEB BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚ñ∂Ô∏è Querying BISWEB BIN for {len(bins)} buildings...\")\n",
    "dob_bisweb_bin_df = query_dob_bisweb_bin(bins)\n",
    "bisweb_bin_matches = set()\n",
    "if not dob_bisweb_bin_df.empty and \"bin__\" in dob_bisweb_bin_df.columns:\n",
    "    bisweb_bin_matches = set(dob_bisweb_bin_df[\"bin__\"].dropna().astype(str).unique())\n",
    "bisweb_bin_unmatched = []\n",
    "for b in bins:\n",
    "    if b not in bisweb_bin_matches:\n",
    "        bisweb_bin_unmatched.append(b)\n",
    "print(f\"BISWEB BIN: {len(bisweb_bin_matches)} matches, {len(bisweb_bin_unmatched)} need BBL fallback\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preview BISWEB BIN results\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    print(\"\\nüìä BISWEB BIN sample:\")\n",
    "    display(dob_bisweb_bin_df.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No BISWEB BIN results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DOB NOW BIN for all buildings\n",
    "print(\"=\" * 70)\n",
    "print(\"DOB NOW BIN QUERY (ALL BUILDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚ñ∂Ô∏è Querying DOB NOW BIN for {len(bins)} buildings...\")\n",
    "dob_now_bin_df = query_dobnow_bin(bins)\n",
    "dobnow_bin_matches = set()\n",
    "if not dob_now_bin_df.empty:\n",
    "    dobnow_bin_matches = set(dob_now_bin_df[\"bin\"].dropna().astype(str).unique())\n",
    "    print(\"üìä DOB NOW BIN sample:\")\n",
    "    display(dob_now_bin_df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No DOB NOW BIN results\")\n",
    "\n",
    "print(f\"DOB NOW BIN: {len(dobnow_bin_matches)} unique BINs and {len(dob_now_bin_df)} total job records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DETERMINE UNMATCHED PROJECTS\n",
    "\n",
    "# Combine matched BINs from both API queries\n",
    "all_matched_bins = bisweb_bin_matches.union(dobnow_bin_matches)\n",
    "\n",
    "# Find projects with BINs that failed both BISWEB and DOB NOW queries\n",
    "# Create a DataFrame of projects whose BINs were not matched by either BISWEB or DOB NOW queries.\n",
    "# This uses .isin(all_matched_bins) to check which projects' BINs are missing from the set of matched BINs,\n",
    "# and selects only those projects to proceed to BBL fallback lookup.\n",
    "unmatched_projects_df = hpd_multifamily_finance_new_construction_df[\n",
    "    ~hpd_multifamily_finance_new_construction_df[\"BIN\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\".0\", \"\")\n",
    "        .isin(all_matched_bins)\n",
    "].copy()\n",
    "\n",
    "print(f\"BIN QUERY RESULTS SUMMARY\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Total projects in dataset: {len(hpd_multifamily_finance_new_construction_df)}\")\n",
    "matched_projects = hpd_multifamily_finance_new_construction_df[hpd_multifamily_finance_new_construction_df[\"BIN\"].astype(str).str.replace(\".0\", \"\").isin(all_matched_bins)]\n",
    "print(f\"Unique BINs found in BISWEB/DOB NOW APIs: {len(all_matched_bins)}\")\n",
    "print(f\"Projects covered by those BINs: {len(matched_projects)} (some BINs cover multiple projects)\")\n",
    "\n",
    "# Analyze BIN sharing\n",
    "bin_counts = hpd_multifamily_finance_new_construction_df[\"BIN\"].value_counts()\n",
    "bins_used_multiple_times = len(bin_counts[bin_counts > 1])\n",
    "extra_projects_from_sharing = bin_counts[bin_counts > 1].sum() - bins_used_multiple_times\n",
    "print(f\"Projects needing fallback queries: {len(unmatched_projects_df)}\")\n",
    "print(f\"BIN matching success rate: {len(matched_projects)/len(hpd_multifamily_finance_new_construction_df)*100:.1f}% of projects covered\")\n",
    "\n",
    "# Sanity check\n",
    "total_accounted_for = len(matched_projects) + len(unmatched_projects_df)\n",
    "print(f\"Sanity check: {len(matched_projects)} + {len(unmatched_projects_df)} = {total_accounted_for} (should equal {len(hpd_multifamily_finance_new_construction_df)})\")\n",
    "\n",
    "if not unmatched_projects_df.empty:\n",
    "    print(f\"Unmatched projects will proceed to fallback queries (BBL ‚Üí Condo ‚Üí Address)\")\n",
    "else:\n",
    "    print(f\"All projects successfully matched via BIN queries! No fallbacks needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpd_multifamily_finance_new_construction_df[\n",
    "    (hpd_multifamily_finance_new_construction_df[\"BIN\"].duplicated(keep=False)) & \n",
    "    (~hpd_multifamily_finance_new_construction_df[\"BIN\"].astype(str).apply(is_bad_bin))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BISWEB BBL FALLBACK\n",
    "if not unmatched_projects_df.empty:\n",
    "    print(\"\" + \"=\" * 70)\n",
    "    print(\"BISWEB BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Extract BBLs from unmatched projects\n",
    "    bbl_tuples = []\n",
    "    for _, row in unmatched_projects_df.iterrows():\n",
    "        if pd.notna(row.get(\"BBL\")):\n",
    "            bbl_result = decompose_bbl(str(row[\"BBL\"]))\n",
    "            if bbl_result and len(bbl_result) >= 3:\n",
    "                bbl_tuples.append(bbl_result)\n",
    "    \n",
    "    # Deduplicate BBLs\n",
    "    bbl_tuples = list(set(bbl_tuples))\n",
    "    print(f\"Extracted {len(bbl_tuples)} unique BBLs from unmatched projects\")\n",
    "    \n",
    "    # Query BISWEB BBL API\n",
    "    print(f\"‚ñ∂Ô∏è Querying BISWEB BBL for {len(bbl_tuples)} BBLs...\")\n",
    "    dob_bisweb_bbl_df = query_dob_bisweb_bbl(bbl_tuples)\n",
    "    \n",
    "    # DOB NOW BBL fallback\n",
    "    print(\"\" + \"=\" * 70)\n",
    "    print(\"DOB NOW BBL FALLBACK\")\n",
    "    print(\"=\" * 70)\n",
    "    dob_now_bbl_df = query_dobnow_bbl(bbl_tuples)\n",
    "else:\n",
    "    # No unmatched projects, initialize empty dataframes\n",
    "    dob_bisweb_bbl_df = pd.DataFrame()\n",
    "    dob_now_bbl_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Multifamily Finance Program (MFP) new construction projects, find those with no DOB match in any table.\n",
    "\n",
    "# Use hpd_multifamily_finance_new_construction_df from Step 3A (already filtered to MFP new construction)\n",
    "hpd_multifamily_finance_new_construction_for_matching_df = hpd_multifamily_finance_new_construction_df.copy()\n",
    "\n",
    "# Defensive: set of unique Project IDs for matching\n",
    "mfp_project_ids = set(hpd_multifamily_finance_new_construction_for_matching_df['Project ID'].unique())\n",
    "\n",
    "# Combine all DOB dataframes and normalize BIN columns\n",
    "all_dob_dfs = []\n",
    "\n",
    "# Normalize BIN columns in each DOB dataframe\n",
    "if not dob_bisweb_bin_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df = dob_bisweb_bin_df.copy()\n",
    "        dob_bisweb_bin_df['bin_normalized'] = dob_bisweb_bin_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bin_df.columns:\n",
    "        dob_bisweb_bin_df['bbl'] = dob_bisweb_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bin_df)\n",
    "\n",
    "if not dob_bisweb_bbl_df.empty:\n",
    "    if 'bin__' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin__'].astype(str).str.replace('.0', '')\n",
    "    elif 'bin' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df = dob_bisweb_bbl_df.copy()\n",
    "        dob_bisweb_bbl_df['bin_normalized'] = dob_bisweb_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_bisweb_bbl_df.columns:\n",
    "        dob_bisweb_bbl_df['bbl'] = dob_bisweb_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_bisweb_bbl_df)\n",
    "\n",
    "if not dob_now_bin_df.empty:\n",
    "    if 'bin' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df = dob_now_bin_df.copy()\n",
    "        dob_now_bin_df['bin_normalized'] = dob_now_bin_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bin_df.columns:\n",
    "        dob_now_bin_df['bbl'] = dob_now_bin_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bin_df)\n",
    "\n",
    "if not dob_now_bbl_df.empty:\n",
    "    if 'bin' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df = dob_now_bbl_df.copy()\n",
    "        dob_now_bbl_df['bin_normalized'] = dob_now_bbl_df['bin'].astype(str).str.replace('.0', '')\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'bbl' in dob_now_bbl_df.columns:\n",
    "        dob_now_bbl_df['bbl'] = dob_now_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    all_dob_dfs.append(dob_now_bbl_df)\n",
    "\n",
    "# Use combined_dob_df from Step 3A if available, otherwise combine here\n",
    "if 'combined_dob_df' in globals() and not combined_dob_df.empty:\n",
    "    combined_dob_with_normalized_bbl_df = combined_dob_df.copy()\n",
    "    # Ensure bin_normalized is created from bin__ or bin column\n",
    "    if 'bin_normalized' not in combined_dob_with_normalized_bbl_df.columns:\n",
    "        if 'bin__' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            combined_dob_with_normalized_bbl_df['bin_normalized'] = combined_dob_with_normalized_bbl_df['bin__'].astype(str).str.replace('.0', '', regex=False)\n",
    "        elif 'bin' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            combined_dob_with_normalized_bbl_df['bin_normalized'] = combined_dob_with_normalized_bbl_df['bin'].astype(str).str.replace('.0', '', regex=False)\n",
    "    # Ensure bbl_normalized is created\n",
    "    if 'bbl_normalized' not in combined_dob_with_normalized_bbl_df.columns:\n",
    "        if 'bbl' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    print(f'Total DOB records (from Step 3A): {len(combined_dob_with_normalized_bbl_df)}')\n",
    "elif all_dob_dfs:\n",
    "    combined_dob_with_normalized_bbl_df = pd.concat(all_dob_dfs, ignore_index=True)\n",
    "    print(f'Total DOB records: {len(combined_dob_with_normalized_bbl_df)}')\n",
    "else:\n",
    "    combined_dob_with_normalized_bbl_df = pd.DataFrame()\n",
    "    print('No DOB records found')\n",
    "\n",
    "# Prepare HPD data for matching - normalize BIN and ensure BBL is string\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df = hpd_multifamily_finance_new_construction_for_matching_df.copy()\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df['bin_normalized'] = hpd_multifamily_finance_new_construction_with_normalized_ids_df['BIN'].astype(str).str.replace('.0', '')\n",
    "hpd_multifamily_finance_new_construction_with_normalized_ids_df['bbl_normalized'] = hpd_multifamily_finance_new_construction_with_normalized_ids_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "\n",
    "# Join on BIN first\n",
    "if not combined_dob_with_normalized_bbl_df.empty and 'bin_normalized' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    hpd_matched_on_bin_df = pd.merge(\n",
    "        hpd_multifamily_finance_new_construction_with_normalized_ids_df,\n",
    "        combined_dob_with_normalized_bbl_df[['bin_normalized']].drop_duplicates(),\n",
    "        on='bin_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bin = set(hpd_matched_on_bin_df['Project ID'].unique())\n",
    "    print(f'Projects matched on BIN: {len(matched_project_ids_bin)}')\n",
    "else:\n",
    "    matched_project_ids_bin = set()\n",
    "\n",
    "# Join on BBL for those that didn't match on BIN\n",
    "hpd_unmatched_on_bin_df = hpd_multifamily_finance_new_construction_with_normalized_ids_df[~hpd_multifamily_finance_new_construction_with_normalized_ids_df['Project ID'].isin(matched_project_ids_bin)]\n",
    "\n",
    "# Initialize BBL matching result\n",
    "matched_project_ids_bbl = set()\n",
    "\n",
    "# Reconstruct BBL in DOB data for sources that don't have it (like BISWEB)\n",
    "# Reconstruct BBL from borough, block, lot for records that don't have it\n",
    "def reconstruct_bbl(row):\n",
    "    if pd.isna(row.get('borough')) or pd.isna(row.get('block')) or pd.isna(row.get('lot')):\n",
    "        return None\n",
    "    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "    borough_code = borough_map.get(str(row['borough']).upper(), None)\n",
    "    if not borough_code:\n",
    "        return None\n",
    "    # Remove leading zeros from block/lot for BBL reconstruction\n",
    "    block_str = str(int(float(str(row['block']).replace('.0', ''))))\n",
    "    lot_str = str(int(float(str(row['lot']).replace('.0', ''))))\n",
    "    # Reconstruct: borough(1) + block(5) + lot(4) = 10 digits\n",
    "    bbl_str = borough_code + block_str.zfill(5) + lot_str.zfill(4)\n",
    "    return bbl_str.zfill(10)\n",
    "\n",
    "# Always reconstruct BBL for records that need it (BISWEB data doesn't have bbl column)\n",
    "combined_dob_with_normalized_bbl_df['bbl_reconstructed'] = combined_dob_with_normalized_bbl_df.apply(reconstruct_bbl, axis=1)\n",
    "# Normalize BBL in DOB data (use existing bbl or reconstructed)\n",
    "# Use bbl column if available, otherwise use reconstructed BBL\n",
    "if 'bbl' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    # Use existing bbl column, normalized to 10 digits\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    # Fill missing values with reconstructed BBL\n",
    "    if 'bbl_reconstructed' in combined_dob_with_normalized_bbl_df.columns:\n",
    "        combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl_normalized'].fillna(combined_dob_with_normalized_bbl_df['bbl_reconstructed'])\n",
    "elif 'bbl_reconstructed' in combined_dob_with_normalized_bbl_df.columns:\n",
    "    # Use reconstructed BBL if no bbl column exists\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = combined_dob_with_normalized_bbl_df['bbl_reconstructed']\n",
    "else:\n",
    "    combined_dob_with_normalized_bbl_df['bbl_normalized'] = None\n",
    "\n",
    "# Now match on BBL\n",
    "if 'bbl_normalized' in combined_dob_with_normalized_bbl_df.columns and combined_dob_with_normalized_bbl_df['bbl_normalized'].notna().any():\n",
    "    hpd_matched_on_bbl_df = pd.merge(\n",
    "        hpd_unmatched_on_bin_df,\n",
    "        combined_dob_with_normalized_bbl_df[['bbl_normalized']].drop_duplicates(),\n",
    "        on='bbl_normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    matched_project_ids_bbl = set(hpd_matched_on_bbl_df['Project ID'].unique())\n",
    "    print(f'Projects matched on BBL (fallback): {len(matched_project_ids_bbl)}')\n",
    "else:\n",
    "    matched_project_ids_bbl = set()\n",
    "    print('No BBL data available for matching')\n",
    "\n",
    "# Combine all matched project IDs\n",
    "dob_matched_project_ids = matched_project_ids_bin | matched_project_ids_bbl\n",
    "\n",
    "# Find projects without DOB matches\n",
    "mfp_projects_without_dob = mfp_project_ids - dob_matched_project_ids\n",
    "\n",
    "print(f'\\nTotal Multifamily Finance Program new construction projects: {len(mfp_project_ids)}')\n",
    "print(f'Projects with DOB matches: {len(dob_matched_project_ids)}')\n",
    "print(f'Number of these with NO DOB row in any table: {len(mfp_projects_without_dob)}')\n",
    "\n",
    "# Debug: show a sample of matched and unmatched projects\n",
    "if len(matched_project_ids_bin) > 0:\n",
    "    print(f'\\nSample matched on BIN: {list(matched_project_ids_bin)[:3]}')\n",
    "if len(matched_project_ids_bbl) > 0:\n",
    "    print(f'Sample matched on BBL: {list(matched_project_ids_bbl)[:3]}')\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(f'Sample unmatched: {list(mfp_projects_without_dob)[:3]}')\n",
    "\n",
    "# DEBUG: Analyze a sample project to understand matching\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    sample_project_id = list(mfp_projects_without_dob)[0]\n",
    "    sample_project = hpd_multifamily_finance_new_construction_for_matching_df[hpd_multifamily_finance_new_construction_for_matching_df['Project ID'] == sample_project_id]\n",
    "    print(f'\\n=== DEBUG: Sample unmatched project ===')\n",
    "    print(f'Project ID: {sample_project_id}')\n",
    "    print(f'Number of buildings in project: {len(sample_project)}')\n",
    "    sample_bins = sample_project['BIN'].dropna().astype(str).str.replace('.0', '').tolist()\n",
    "    sample_bbls = sample_project['BBL'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).tolist()\n",
    "    print(f'BINs in project: {sample_bins[:5]}')\n",
    "    print(f'BBLs in project: {sample_bbls[:5]}')\n",
    "    \n",
    "    # Check if these BINs/BBLs exist in DOB data\n",
    "    if not combined_dob_with_normalized_bbl_df.empty:\n",
    "        if 'bin_normalized' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            dob_bins = set(combined_dob_with_normalized_bbl_df['bin_normalized'].dropna().astype(str).unique())\n",
    "            matching_bins = []\n",
    "            for b in sample_bins:\n",
    "                if b in dob_bins:\n",
    "                    matching_bins.append(b)\n",
    "            print(f'BINs found in DOB data: {matching_bins[:5] if matching_bins else \"None\"}')\n",
    "        if 'bbl' in combined_dob_with_normalized_bbl_df.columns:\n",
    "            dob_bbls = set(combined_dob_with_normalized_bbl_df['bbl'].dropna().apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None).unique())\n",
    "            matching_bbls = []\n",
    "            for b in sample_bbls:\n",
    "                if b in dob_bbls:\n",
    "                    matching_bbls.append(b)\n",
    "            print(f'BBLs found in DOB data: {matching_bbls[:5] if matching_bbls else \"None\"}')\n",
    "# Show the head of the table of unmatched projects (project-level)\n",
    "if len(mfp_projects_without_dob) > 0:\n",
    "    print(\"\\nHead of unmatched Multifamily Finance Program new construction projects:\")\n",
    "    hpd_multifamily_finance_new_construction_unmatched_projects_df = hpd_multifamily_finance_new_construction_for_matching_df[hpd_multifamily_finance_new_construction_for_matching_df['Project ID'].isin(mfp_projects_without_dob)].copy()\n",
    "    # Ensure BBL is displayed as a string, not float\n",
    "    if 'BBL' in hpd_multifamily_finance_new_construction_unmatched_projects_df.columns:\n",
    "        hpd_multifamily_finance_new_construction_unmatched_projects_df['BBL'] = hpd_multifamily_finance_new_construction_unmatched_projects_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    excluded_cols = [\n",
    "        \"Lot Area\", \"Available Units\", \"Privately Financed Units\", \"Extremely Low Income Units\",\n",
    "        \"Very Low Income Units\", \"Low Income Units\", \"Moderate Income Units\", \"Middle Income Units\",\n",
    "        \"Studio Units\", \"One Bedroom Units\", \"Two Bedroom Units\", \"Three Bedroom Units\",\n",
    "        \"Four Bedroom Units\", \"Five Bedroom Units\", \"Six Bedroom Units\", \"Unknown Bedroom Units\",\n",
    "    ][:15]  # Limit extra-wide tables in notebook\n",
    "    display_cols = []\n",
    "    for c in hpd_multifamily_finance_new_construction_unmatched_projects_df.columns:\n",
    "        if c not in excluded_cols:\n",
    "            display_cols.append(c)\n",
    "    display(hpd_multifamily_finance_new_construction_unmatched_projects_df[display_cols].head(10))\n",
    "else:\n",
    "    print(\"\\nAll Multifamily Finance Program projects matched to DOB data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join HPD Multifamily Finance new construction rows to DOB data,\n",
    "# first on BIN, then using BBL as a backup for rows without a BIN match.\n",
    "# For each HPD row, add the earliest of DOB's 'paid', 'applied', or 'approved' date\n",
    "# (whichever is earliest/present among these columns in the DOB data).\n",
    "\n",
    "\n",
    "# Suppress performance warnings about DataFrame fragmentation\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Ensure all date columns in the DOB dataframe are properly converted to datetime dtype ---\n",
    "\n",
    "\n",
    "# Load BISWEB and DOB NOW data\n",
    "dob_bisweb = pd.read_csv('data/processed/multifamily_finance_dob_bisweb_bin.csv', low_memory=False)\n",
    "dob_now_path = Path('data/processed/multifamily_finance_dob_now_bin.csv')\n",
    "if dob_now_path.exists():\n",
    "    dob_now = pd.read_csv(dob_now_path, low_memory=False)\n",
    "else:\n",
    "    dob_now = pd.DataFrame()\n",
    "\n",
    "# Combine\n",
    "dob_df = pd.concat([dob_bisweb, dob_now], ignore_index=True)\n",
    "print(f'Loaded DOB data: {len(dob_bisweb)} BISWEB + {len(dob_now)} DOB NOW = {len(dob_df)} total')\n",
    "\n",
    "# Reconstruct BBL from borough/block/lot (same as cell 17)\n",
    "def reconstruct_bbl(row):\n",
    "    if pd.isna(row.get('borough')) or pd.isna(row.get('block')) or pd.isna(row.get('lot')):\n",
    "        return None\n",
    "    borough_map = {'MANHATTAN': '1', 'BRONX': '2', 'BROOKLYN': '3', 'QUEENS': '4', 'STATEN ISLAND': '5'}\n",
    "    borough_code = borough_map.get(str(row['borough']).upper(), None)\n",
    "    if not borough_code:\n",
    "        return None\n",
    "    try:\n",
    "        block_str = str(int(row['block']))\n",
    "        lot_str = str(int(row['lot']))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    # Reconstruct: borough(1) + block(5) + lot(4) = 10 digits\n",
    "    bbl_str = borough_code + block_str.zfill(5) + lot_str.zfill(4)\n",
    "    return bbl_str\n",
    "\n",
    "dob_df['bbl_reconstructed'] = dob_df.apply(reconstruct_bbl, axis=1)\n",
    "\n",
    "# Create bbl_normalized from bbl column or reconstructed BBL\n",
    "if 'bbl' in dob_df.columns:\n",
    "    dob_df['bbl_normalized'] = dob_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    # Fill missing values with reconstructed BBL\n",
    "    dob_df['bbl_normalized'] = dob_df['bbl_normalized'].fillna(dob_df['bbl_reconstructed'])\n",
    "else:\n",
    "    # Use reconstructed BBL if no bbl column exists\n",
    "    dob_df['bbl_normalized'] = dob_df['bbl_reconstructed']\n",
    "\n",
    "print(f'  - bbl_normalized non-null: {dob_df[\"bbl_normalized\"].notna().sum()}')\n",
    "\n",
    "\n",
    "# Filter to only NB applications with specific document/application numbers:\n",
    "# - BISWEB: doc__ = 1 (document number 01) - filter when doc__ column exists\n",
    "# - DOB NOW: application number ending with 'I1' (e.g., X00969702-I1)\n",
    "\n",
    "# Filter BISWEB data: only filter rows that have doc__ column (BISWEB data)\n",
    "# Keep DOB NOW rows (which have NaN in doc__) and BISWEB rows with doc__ = 1\n",
    "if 'doc__' in dob_df.columns:\n",
    "    # Keep rows where doc__ is NaN (DOB NOW) OR doc__ == 1 (BISWEB doc 01)\n",
    "    dob_df = dob_df[(dob_df['doc__'].isna()) | (dob_df['doc__'] == 1)]\n",
    "    print(f\"Filtered BISWEB to doc__ = 1 (keeping DOB NOW): {len(dob_df)} records remaining\")\n",
    "# Filter DOB NOW data: job_filing_number should end with 'I1'\n",
    "if 'job_filing_number' in dob_df.columns:\n",
    "    # Only filter rows that have job_filing_number (DOB NOW data)\n",
    "    dobnow_mask = dob_df['job_filing_number'].notna()\n",
    "    if dobnow_mask.any():\n",
    "        dob_df = dob_df[\n",
    "            (~dobnow_mask) | (dob_df['job_filing_number'].astype(str).str.endswith('I1', na=False))\n",
    "        ]\n",
    "        print(f\"Filtered DOB NOW to I1 suffix: {len(dob_df)} records remaining\")\n",
    "\n",
    "# Create bin_normalized column from bin__ or bin column\n",
    "if 'bin_normalized' not in dob_df.columns:\n",
    "    if 'bin__' in dob_df.columns:\n",
    "        dob_df['bin_normalized'] = dob_df['bin__'].astype(str).str.replace('.0', '')\n",
    "        print(f\"Created bin_normalized from bin__\")\n",
    "    elif 'bin' in dob_df.columns:\n",
    "        dob_df['bin_normalized'] = dob_df['bin'].astype(str).str.replace('.0', '')\n",
    "        print(f\"Created bin_normalized from bin\")\n",
    "\n",
    "\n",
    "if 'bbl_normalized' not in dob_df.columns:\n",
    "    # Fallback: create bbl_normalized from bbl column if it doesn't exist\n",
    "    dob_df['bbl_normalized'] = dob_df['bbl'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "else:\n",
    "    # Ensure bbl_normalized is properly formatted as string (10 digits, zero-padded)\n",
    "    def format_bbl(x):\n",
    "        if pd.isna(x) or x is None:\n",
    "            return None\n",
    "        try:\n",
    "            # Try to convert to int then string, then pad\n",
    "            return str(int(float(x))).zfill(10)\n",
    "        except (ValueError, TypeError):\n",
    "            x_str = str(x).strip()\n",
    "            if x_str in ('None', 'nan', ''):\n",
    "                return None\n",
    "            if x_str.isdigit() and len(x_str) <= 10:\n",
    "                return x_str.zfill(10)\n",
    "            return None\n",
    "    dob_df['bbl_normalized'] = dob_df['bbl_normalized'].apply(format_bbl)\n",
    "\n",
    "# Identify all possible milestone date columns across BISWEB and DOB NOW\n",
    "possible_date_cols = [\n",
    "    'pre__filing_date', 'paid', 'fully_paid', 'assigned', 'approved', 'fully_permitted',\n",
    "    'filing date', 'first approved date', 'first permit date', 'approved date',\n",
    "    'filing_date', 'first_permit_date', 'first_approved_date', 'approved_date',\n",
    "    'latest_action_date'\n",
    "]\n",
    "# Find intersection with dob_df columns (case insensitive, allow aliasing)\n",
    "normalized_col_map = {}\n",
    "dob_col_lower_map = {col.lower().replace(\" \", \"_\"): col for col in dob_df.columns}\n",
    "date_cols_final = []\n",
    "for c in possible_date_cols:\n",
    "    c_norm = c.lower().replace(\" \", \"_\")\n",
    "    # Try to match to actual dob_df columns\n",
    "    if c_norm in dob_col_lower_map:\n",
    "        real_col = dob_col_lower_map[c_norm]\n",
    "        normalized_col_map[c] = real_col\n",
    "        if real_col not in date_cols_final:\n",
    "            date_cols_final.append(real_col)\n",
    "\n",
    "# Convert these columns to datetime in dob_df\n",
    "print(f'Found {len(date_cols_final)} date columns: {date_cols_final}')\n",
    "for col in date_cols_final:\n",
    "    dob_df[col] = pd.to_datetime(dob_df[col], errors='coerce')\n",
    "    non_null_count = dob_df[col].notna().sum()\n",
    "    print(f'  - {col}: {non_null_count} non-null dates')\n",
    "\n",
    "# Earliest date selector function expects all possible date columns to be datetime now\n",
    "def _get_earliest_date(row, date_cols):\n",
    "    \"\"\"Get the earliest date and the column name that provided it.\"\"\"\n",
    "    date_values = []\n",
    "    for col in date_cols:\n",
    "        v = row.get(col, None)\n",
    "        if pd.notna(v):\n",
    "            # Ensure it's a datetime for proper comparison\n",
    "            try:\n",
    "                date_val = pd.to_datetime(v, errors='coerce')\n",
    "                if pd.notna(date_val):\n",
    "                    date_values.append((date_val, col))\n",
    "            except:\n",
    "                pass\n",
    "    if date_values:\n",
    "        earliest = min(date_values, key=lambda x: x[0])\n",
    "        return earliest[0], earliest[1]  # Return (date, column_name)\n",
    "    return pd.NaT, None\n",
    "\n",
    "# Bin/BBL handling as previously\n",
    "dob_bin_dates = dob_df.copy()\n",
    "# Apply function that returns both date and source column\n",
    "earliest_results = dob_bin_dates.apply(lambda row: _get_earliest_date(row, date_cols_final), axis=1)\n",
    "\n",
    "# Extract date and source column from tuples, then get date directly from source column\n",
    "# This ensures the date value exactly matches what's in the source column\n",
    "# Use the index from earliest_results to match with dob_bin_dates index\n",
    "earliest_dates = []\n",
    "earliest_sources = []\n",
    "for orig_idx in earliest_results.index:\n",
    "    result = earliest_results.loc[orig_idx]\n",
    "    if isinstance(result, tuple) and len(result) >= 2:\n",
    "        source_col = result[1]\n",
    "        row = dob_bin_dates.loc[orig_idx]\n",
    "        # Get the date directly from the source column to ensure accuracy\n",
    "        if source_col and source_col in row and pd.notna(row[source_col]):\n",
    "            source_date = pd.to_datetime(row[source_col], errors='coerce')\n",
    "            earliest_dates.append(source_date)\n",
    "        else:\n",
    "            # Fallback to the date from the tuple if source column not available\n",
    "            earliest_dates.append(result[0])\n",
    "        earliest_sources.append(source_col)\n",
    "    elif isinstance(result, tuple) and len(result) == 1:\n",
    "        earliest_dates.append(result[0])\n",
    "        earliest_sources.append(None)\n",
    "    else:\n",
    "        earliest_dates.append(result if not isinstance(result, tuple) else pd.NaT)\n",
    "        earliest_sources.append(None)\n",
    "\n",
    "# Create Series with matching index\n",
    "dob_bin_dates['earliest_dob_date'] = pd.Series(earliest_dates, index=earliest_results.index)\n",
    "dob_bin_dates['earliest_dob_date_source'] = pd.Series(earliest_sources, index=earliest_results.index)\n",
    "print(f'Extracted earliest dates: {dob_bin_dates[\"earliest_dob_date\"].notna().sum()} non-null out of {len(dob_bin_dates)}')\n",
    "\n",
    "# Ensure key application/filing date columns are datetime (if present)\n",
    "for col in ['pre__filing_date', 'latest_action_date', 'filing_date', 'first_permit_date', 'first_approved_date', 'approved_date']:\n",
    "    if col in dob_bin_dates.columns:\n",
    "        dob_bin_dates[col] = pd.to_datetime(dob_bin_dates[col], errors='coerce')\n",
    "\n",
    "def get_application_date(row):\n",
    "    # Check for earliest milestone in a preferred order with all values as datetimes\n",
    "    for col in ['pre__filing_date', 'latest_action_date']:\n",
    "        v = row.get(col, None)\n",
    "        if pd.notna(v):\n",
    "            return v\n",
    "    for col in ['filing_date', 'first_permit_date']:\n",
    "        v = row.get(col, None)\n",
    "        if pd.notna(v):\n",
    "            return v\n",
    "    if pd.notna(row.get('earliest_dob_date')):\n",
    "        return row['earliest_dob_date']\n",
    "    return pd.NaT\n",
    "\n",
    "dob_bin_dates['application_date'] = dob_bin_dates.apply(get_application_date, axis=1)\n",
    "\n",
    "def get_application_number(row):\n",
    "    if 'job_filing_number' in row and pd.notna(row['job_filing_number']):\n",
    "        return str(row['job_filing_number'])\n",
    "    elif 'job__' in row and pd.notna(row['job__']):\n",
    "        return str(row['job__'])\n",
    "    return None\n",
    "\n",
    "dob_bin_dates['application_number'] = dob_bin_dates.apply(get_application_number, axis=1)\n",
    "\n",
    "dob_bin_dates_sorted = dob_bin_dates.sort_values('application_date', ascending=False, na_position='last')\n",
    "\n",
    "# Group by BIN and get earliest date from most recent application (first row after sorting)\n",
    "# After grouping, re-read the date from the source column to ensure accuracy\n",
    "dob_bin_min_temp = dob_bin_dates_sorted.groupby('bin_normalized', as_index=False).first()\n",
    "dob_bin_min = dob_bin_min_temp[['bin_normalized', 'earliest_dob_date', 'earliest_dob_date_source', 'application_number']].copy()\n",
    "\n",
    "# Also get fully_permitted date if available\n",
    "if 'fully_permitted' in dob_bin_min_temp.columns:\n",
    "    dob_bin_min['fully_permitted_date'] = dob_bin_min_temp['fully_permitted']\n",
    "print(f'After BIN groupby: {len(dob_bin_min)} unique BINs, {dob_bin_min[\"earliest_dob_date\"].notna().sum()} with dates')\n",
    "\n",
    "# Re-read the date from the source column to ensure it matches\n",
    "for idx in dob_bin_min.index:\n",
    "    source_col = dob_bin_min.loc[idx, 'earliest_dob_date_source']\n",
    "    if source_col and pd.notna(source_col):\n",
    "        orig_row = dob_bin_min_temp.loc[idx]\n",
    "        if source_col in orig_row and pd.notna(orig_row[source_col]):\n",
    "            dob_bin_min.loc[idx, 'earliest_dob_date'] = pd.to_datetime(orig_row[source_col], errors='coerce')\n",
    "dob_bbl_filtered = dob_bin_dates_sorted[dob_bin_dates_sorted['bbl_normalized'].notna()].copy()\n",
    "if not dob_bbl_filtered.empty:\n",
    "    # Group by BBL and get earliest date from most recent application\n",
    "    dob_bbl_min_temp = dob_bbl_filtered.groupby('bbl_normalized', as_index=False).first()\n",
    "    dob_bbl_min = dob_bbl_min_temp[['bbl_normalized', 'earliest_dob_date', 'earliest_dob_date_source', 'application_number']].copy()\n",
    "    \n",
    "    # Also get fully_permitted date if available\n",
    "    if 'fully_permitted' in dob_bbl_min_temp.columns:\n",
    "        dob_bbl_min['fully_permitted_date'] = dob_bbl_min_temp['fully_permitted']\n",
    "    \n",
    "    # Re-read the date from the source column to ensure it matches\n",
    "    for idx in dob_bbl_min.index:\n",
    "        source_col = dob_bbl_min.loc[idx, 'earliest_dob_date_source']\n",
    "        if source_col and pd.notna(source_col):\n",
    "            orig_row = dob_bbl_min_temp.loc[idx]\n",
    "            if source_col in orig_row and pd.notna(orig_row[source_col]):\n",
    "                dob_bbl_min.loc[idx, 'earliest_dob_date'] = pd.to_datetime(orig_row[source_col], errors='coerce')\n",
    "else:\n",
    "    dob_bbl_min = pd.DataFrame(columns=['bbl_normalized', 'earliest_dob_date', 'earliest_dob_date_source', 'application_number'])\n",
    "\n",
    "hpd_df = hpd_multifamily_finance_new_construction_df.copy()\n",
    "hpd_df['BIN_str'] = hpd_df['BIN'].dropna().astype(str).str.replace('.0', '')\n",
    "hpd_df['BBL_str'] = hpd_df['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "\n",
    "# Identify placeholder BINs that should not be used for matching\n",
    "placeholder_bins = ['1000000', '2000000', '3000000', '4000000', '5000000']\n",
    "hpd_df['has_valid_bin'] = ~hpd_df['BIN_str'].isin(placeholder_bins)\n",
    "\n",
    "# Only merge on BIN for rows with valid (non-placeholder) BINs\n",
    "hpd_with_date = pd.merge(\n",
    "    hpd_df,\n",
    "    dob_bin_min.rename(columns={'bin_normalized': 'BIN_str'}),\n",
    "    how='left',\n",
    "    on='BIN_str'\n",
    ")\n",
    "\n",
    "# For placeholder BINs, clear out the incorrectly matched dates\n",
    "hpd_with_date.loc[~hpd_with_date['has_valid_bin'], 'earliest_dob_date'] = pd.NaT\n",
    "hpd_with_date.loc[~hpd_with_date['has_valid_bin'], 'earliest_dob_date_source'] = None\n",
    "hpd_with_date.loc[~hpd_with_date['has_valid_bin'], 'application_number'] = None\n",
    "if 'fully_permitted_date' in hpd_with_date.columns:\n",
    "    hpd_with_date.loc[~hpd_with_date['has_valid_bin'], 'fully_permitted_date'] = pd.NaT\n",
    "no_bin_match_mask = hpd_with_date['earliest_dob_date'].isna()\n",
    "if no_bin_match_mask.any():\n",
    "    to_fill = hpd_with_date.loc[no_bin_match_mask, ['BBL_str']]\n",
    "    filled = pd.merge(\n",
    "        to_fill,\n",
    "        dob_bbl_min.rename(columns={'bbl_normalized': 'BBL_str'}),\n",
    "        how='left',\n",
    "        on='BBL_str'\n",
    "    )\n",
    "    hpd_with_date.loc[no_bin_match_mask, 'earliest_dob_date'] = filled['earliest_dob_date'].values\n",
    "    if 'application_number' in filled.columns:\n",
    "        hpd_with_date.loc[no_bin_match_mask, 'application_number'] = filled['application_number'].values\n",
    "    if 'earliest_dob_date_source' in filled.columns:\n",
    "        hpd_with_date.loc[no_bin_match_mask, 'earliest_dob_date_source'] = filled['earliest_dob_date_source'].values\n",
    "    if 'fully_permitted_date' in filled.columns:\n",
    "        hpd_with_date.loc[no_bin_match_mask, 'fully_permitted_date'] = filled['fully_permitted_date'].values\n",
    "\n",
    "hpd_multifamily_finance_new_construction_with_dob_date_df = hpd_with_date\n",
    "\n",
    "# Use display instead of print, and always show Building ID, date source, and application number\n",
    "display_cols = ['Building ID', 'Project ID', 'BIN', 'BBL', 'earliest_dob_date']\n",
    "if 'earliest_dob_date_source' in hpd_multifamily_finance_new_construction_with_dob_date_df.columns:\n",
    "    display_cols.append('earliest_dob_date_source')\n",
    "if 'application_number' in hpd_multifamily_finance_new_construction_with_dob_date_df.columns:\n",
    "    display_cols.append('application_number')\n",
    "if 'fully_permitted_date' in hpd_multifamily_finance_new_construction_with_dob_date_df.columns:\n",
    "    display_cols.append('fully_permitted_date')\n",
    "display(hpd_multifamily_finance_new_construction_with_dob_date_df[display_cols].head(10))\n",
    "\n",
    "num_missing_earliest_dob_date = hpd_multifamily_finance_new_construction_with_dob_date_df['earliest_dob_date'].isna().sum()\n",
    "print(f\"\\nCount of HPD MFP new construction rows without an earliest DOB milestone date: {num_missing_earliest_dob_date}\")\n",
    "\n",
    "no_dob_milestone_df = hpd_multifamily_finance_new_construction_with_dob_date_df[\n",
    "    hpd_multifamily_finance_new_construction_with_dob_date_df['earliest_dob_date'].isna()\n",
    "]\n",
    "\n",
    "# Display the full table of rows without a DOB milestone date: include Building ID\n",
    "no_dob_milestone_table = no_dob_milestone_df[['Building ID', 'Project ID', 'Project Name', 'BIN', 'BBL', 'earliest_dob_date']]\n",
    "display(no_dob_milestone_table)\n",
    "print(f\"Total without DOB milestone date: {len(no_dob_milestone_table)}\")\n",
    "\n",
    "def bbl_lot_starts_with_75(bbl):\n",
    "    if pd.isna(bbl):\n",
    "        return False\n",
    "    bbl_str = str(int(float(bbl))).zfill(10)\n",
    "    lot_str = bbl_str[-4:]\n",
    "    return lot_str.startswith('75')\n",
    "\n",
    "lots_start_75_mask = no_dob_milestone_df['BBL'].apply(bbl_lot_starts_with_75)\n",
    "num_lots_start_75 = lots_start_75_mask.sum()\n",
    "print(f\"\\nHow many have BBL lot numbers starting with '75': {num_lots_start_75}\")\n",
    "\n",
    "null_bin_bbl_mask = no_dob_milestone_df['BIN'].isna() & no_dob_milestone_df['BBL'].isna()\n",
    "null_bin_bbl_df = no_dob_milestone_df[null_bin_bbl_mask]\n",
    "\n",
    "full_df = hpd_multifamily_finance_new_construction_with_dob_date_df\n",
    "duplicated_project_ids_in_full = full_df['Project ID'][full_df['Project ID'].duplicated(keep=False)].unique()\n",
    "\n",
    "null_bin_bbl_with_duplicated_projid = null_bin_bbl_df['Project ID'].isin(duplicated_project_ids_in_full)\n",
    "count_null_bin_bbl_and_duplicated = null_bin_bbl_with_duplicated_projid.sum()\n",
    "print(f\"\\nHow many have both BIN and BBL null and with a duplicated Project ID in FULL dataset: {count_null_bin_bbl_and_duplicated}\")\n",
    "\n",
    "null_bin_bbl_duplicated_mask = null_bin_bbl_df['Project ID'].duplicated(keep=False)\n",
    "count_null_bin_bbl_duplicated_in_subset = null_bin_bbl_duplicated_mask.sum()\n",
    "print(f\"How many have both BIN and BBL null and Project ID duplicated within null subset: {count_null_bin_bbl_duplicated_in_subset}\")\n",
    "\n",
    "if count_null_bin_bbl_and_duplicated > 0:\n",
    "    print(\"\\nExamples of rows with both BIN and BBL null where Project ID appears multiple times in FULL dataset:\")\n",
    "    display(null_bin_bbl_df[null_bin_bbl_with_duplicated_projid][['Building ID', 'Project ID', 'Project Name', 'BIN', 'BBL']])\n",
    "    \n",
    "    print(\"\\nAll rows for these projects in FULL dataset (for context):\")\n",
    "    projects_to_show = null_bin_bbl_df[null_bin_bbl_with_duplicated_projid]['Project ID'].unique()\n",
    "    all_rows_for_projects = full_df[full_df['Project ID'].isin(projects_to_show)]\n",
    "    display_cols = ['Building ID', 'Project ID', 'Project Name', 'BIN', 'BBL', 'earliest_dob_date']\n",
    "    display_cols = [col for col in display_cols if col in all_rows_for_projects.columns]\n",
    "    display(all_rows_for_projects[display_cols].sort_values('Project ID'))\n",
    "\n",
    "if count_null_bin_bbl_duplicated_in_subset > 0:\n",
    "    print(\"\\nExamples of rows with both BIN and BBL null and duplicated Project IDs (within null subset):\")\n",
    "    display(null_bin_bbl_df[null_bin_bbl_duplicated_mask][['Building ID', 'Project ID', 'Project Name', 'BIN', 'BBL']])\n",
    "\n",
    "\n",
    "# Export to CSV\n",
    "output_path = 'output/hpd_multifamily_finance_new_construction_with_dob_date.csv'\n",
    "os.makedirs('output', exist_ok=True)\n",
    "hpd_multifamily_finance_new_construction_with_dob_date_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nExported to {output_path}\")\n",
    "print(f\"  Total rows: {len(hpd_multifamily_finance_new_construction_with_dob_date_df)}\")\n",
    "print(f\"  Rows with DOB dates: {hpd_multifamily_finance_new_construction_with_dob_date_df['earliest_dob_date'].notna().sum()}\")\n",
    "print(f\"  Rows without DOB dates: {hpd_multifamily_finance_new_construction_with_dob_date_df['earliest_dob_date'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Step 4: Query Certificate of Occupancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B Configuration\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3B: QUERY CERTIFICATE OF OCCUPANCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract BINs from hpd_multifamily_finance_new_construction_df (in memory from Step 3A)\n",
    "bins_list = _extract_bins_from_df(hpd_multifamily_finance_new_construction_df)\n",
    "print(f\"\\nüìã Extracted {len(bins_list)} BINs from filtered dataset\")\n",
    "\n",
    "# First, query by BIN only to see which BINs are found\n",
    "print(f\"üèõÔ∏è Querying CO APIs by BIN first to identify unfound BINs...\")\n",
    "co_filings_df = _query_co_filings_from_bins_and_bbls(bins_list, bbl_list=None, output_path=None)\n",
    "\n",
    "# Identify which BINs were found in CO data\n",
    "found_bins = set()\n",
    "if co_filings_df is not None and not co_filings_df.empty:\n",
    "    if 'bin_normalized' in co_filings_df.columns:\n",
    "        found_bins = set(co_filings_df['bin_normalized'].dropna().astype(str).unique())\n",
    "    elif 'bin' in co_filings_df.columns:\n",
    "        found_bins = set(co_filings_df['bin'].dropna().astype(str).unique())\n",
    "    elif 'bin_number' in co_filings_df.columns:\n",
    "        found_bins = set(co_filings_df['bin_number'].dropna().astype(str).unique())\n",
    "\n",
    "print(f\"\\nüìä BIN query results: {len(found_bins)} unique BINs found in CO data out of {len(bins_list)} queried\")\n",
    "\n",
    "# Extract BBLs for fallback querying (placeholder BINs + BINs not found in CO data)\n",
    "bbls_to_query = []\n",
    "placeholder_bins = ['1000000', '2000000', '3000000', '4000000', '5000000']\n",
    "\n",
    "for _, row in hpd_multifamily_finance_new_construction_df.iterrows():\n",
    "    bin_val = row.get('BIN')\n",
    "    bin_str = str(bin_val).replace('.0', '') if pd.notna(bin_val) else ''\n",
    "    bbl_val = row.get('BBL')\n",
    "    \n",
    "    # Include BBL if:\n",
    "    # (1) BIN is placeholder/invalid, OR\n",
    "    # (2) BIN was not found in CO data, OR\n",
    "    # (3) No BIN but has BBL\n",
    "    should_query_bbl = (\n",
    "        (bin_str in placeholder_bins) or\n",
    "        (bin_str and bin_str not in found_bins) or\n",
    "        (pd.isna(bin_val))\n",
    "    )\n",
    "    \n",
    "    if should_query_bbl and pd.notna(bbl_val):\n",
    "        try:\n",
    "            bbl_str = str(int(float(bbl_val))).zfill(10)\n",
    "            bbls_to_query.append(bbl_str)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "bbls_to_query = sorted(list(set(bbls_to_query)))\n",
    "print(f\"üìã Extracted {len(bbls_to_query)} BBLs for fallback querying\")\n",
    "print(f\"   - Placeholder BINs: {sum(1 for _, row in hpd_multifamily_finance_new_construction_df.iterrows() if str(row.get('BIN', '')).replace('.0', '') in placeholder_bins and pd.notna(row.get('BBL')))}\")\n",
    "print(f\"   - BINs not found in CO data: {sum(1 for _, row in hpd_multifamily_finance_new_construction_df.iterrows() if str(row.get('BIN', '')).replace('.0', '') not in found_bins and str(row.get('BIN', '')).replace('.0', '') not in placeholder_bins and pd.notna(row.get('BIN')) and pd.notna(row.get('BBL')))}\")\n",
    "print(f\"   - Missing BINs: {sum(1 for _, row in hpd_multifamily_finance_new_construction_df.iterrows() if pd.isna(row.get('BIN')) and pd.notna(row.get('BBL')))}\")\n",
    "\n",
    "# Query by BBL for unfound records\n",
    "if len(bbls_to_query) > 0:\n",
    "    print(f\"\\nüèõÔ∏è Querying CO APIs by BBL for {len(bbls_to_query)} BBLs...\")\n",
    "    co_filings_bbl_df = _query_co_filings_from_bins_and_bbls(bin_list=[], bbl_list=bbls_to_query, output_path=None)\n",
    "    \n",
    "    # Combine BIN and BBL results\n",
    "    if co_filings_bbl_df is not None and not co_filings_bbl_df.empty:\n",
    "        if co_filings_df is not None and not co_filings_df.empty:\n",
    "            # Combine both dataframes\n",
    "            all_cols = list(set(co_filings_df.columns.tolist() + co_filings_bbl_df.columns.tolist()))\n",
    "            co_filings_df_aligned = co_filings_df.reindex(columns=all_cols)\n",
    "            co_filings_bbl_df_aligned = co_filings_bbl_df.reindex(columns=all_cols)\n",
    "            co_filings_df = pd.concat([co_filings_df_aligned, co_filings_bbl_df_aligned], ignore_index=True)\n",
    "        else:\n",
    "            co_filings_df = co_filings_bbl_df\n",
    "else:\n",
    "    print(f\"\\n‚úì All BINs found in CO data, no BBL fallback needed\")\n",
    "\n",
    "# Display CO data if available\n",
    "if co_filings_df is not None:\n",
    "    print(f\"üìä Certificate of Occupancy Data: {co_filings_df.shape[0]} records\")\n",
    "    print(\"Columns:\")\n",
    "    for col in co_filings_df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\nüìä Sample CO Data:\")\n",
    "    display(co_filings_df.head())\n",
    "    \n",
    "    # Show some statistics\n",
    "    if \"issue_date\" in co_filings_df.columns:\n",
    "        print(\"\\nüìà CO Issue Date Statistics:\")\n",
    "        display(co_filings_df[\"issue_date\"].describe())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CO data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join CO data with HPD data to get earliest CO date\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"JOINING CO DATA WITH HPD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if co_filings_df is not None and not co_filings_df.empty:\n",
    "    # Identify CO date columns\n",
    "    co_date_cols = []\n",
    "    for col in co_filings_df.columns:\n",
    "        if 'date' in col.lower() and 'issue' in col.lower():\n",
    "            co_date_cols.append(col)\n",
    "    \n",
    "    print(f\"\\nüìÖ CO date columns found: {co_date_cols}\")\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for col in co_date_cols:\n",
    "        co_filings_df[col] = pd.to_datetime(co_filings_df[col], errors='coerce')\n",
    "    \n",
    "    # Create normalized BIN and BBL columns for joining\n",
    "    if 'bin_normalized' not in co_filings_df.columns:\n",
    "        if 'bin' in co_filings_df.columns:\n",
    "            co_filings_df['bin_normalized'] = co_filings_df['bin'].astype(str).str.replace('.0', '', regex=False)\n",
    "        elif 'bin_number' in co_filings_df.columns:\n",
    "            co_filings_df['bin_normalized'] = co_filings_df['bin_number'].astype(str).str.replace('.0', '', regex=False)\n",
    "    \n",
    "    if 'bbl_normalized' not in co_filings_df.columns:\n",
    "        if 'bbl' in co_filings_df.columns:\n",
    "            co_filings_df['bbl_normalized'] = co_filings_df['bbl'].astype(str).str.zfill(10)\n",
    "    \n",
    "    # Get earliest CO date for each BIN\n",
    "    def get_earliest_co_date(row, date_cols):\n",
    "        dates = [row[col] for col in date_cols if col in row and pd.notna(row[col])]\n",
    "        if dates:\n",
    "            return min(dates)\n",
    "        return pd.NaT\n",
    "    \n",
    "    co_filings_df['earliest_co_date'] = co_filings_df.apply(lambda row: get_earliest_co_date(row, co_date_cols), axis=1)\n",
    "    \n",
    "    # Group by BIN to get earliest CO date per BIN\n",
    "    co_bin_min = co_filings_df[co_filings_df['bin_normalized'].notna()].groupby('bin_normalized', as_index=False)['earliest_co_date'].min()\n",
    "    print(f\"\\nüìä CO dates by BIN: {len(co_bin_min)} unique BINs with CO dates\")\n",
    "    \n",
    "    # Group by BBL to get earliest CO date per BBL (for fallback)\n",
    "    co_bbl_min = pd.DataFrame()\n",
    "    if 'bbl_normalized' in co_filings_df.columns:\n",
    "        co_bbl_min = co_filings_df[co_filings_df['bbl_normalized'].notna()].groupby('bbl_normalized', as_index=False)['earliest_co_date'].min()\n",
    "        print(f\"üìä CO dates by BBL: {len(co_bbl_min)} unique BBLs with CO dates\")\n",
    "    \n",
    "    # Join with HPD data (use the dataframe with DOB dates)\n",
    "    hpd_with_co = hpd_multifamily_finance_new_construction_with_dob_date_df.copy()\n",
    "    \n",
    "    # Drop earliest_co_date if it already exists (from previous run)\n",
    "    if 'earliest_co_date' in hpd_with_co.columns:\n",
    "        hpd_with_co = hpd_with_co.drop(columns=['earliest_co_date'])\n",
    "    \n",
    "    # Create normalized BIN and BBL in HPD data\n",
    "    hpd_with_co['BIN_str'] = hpd_with_co['BIN'].astype(str).str.replace('.0', '', regex=False)\n",
    "    hpd_with_co['BBL_str'] = hpd_with_co['BBL'].apply(lambda x: str(int(float(x))).zfill(10) if pd.notna(x) else None)\n",
    "    \n",
    "    # Merge on BIN first\n",
    "    if not co_bin_min.empty:\n",
    "        hpd_with_co = pd.merge(\n",
    "            hpd_with_co,\n",
    "            co_bin_min.rename(columns={'bin_normalized': 'BIN_str'}),\n",
    "            how='left',\n",
    "            on='BIN_str'\n",
    "        )\n",
    "    else:\n",
    "        # If no BIN matches, create empty column\n",
    "        hpd_with_co['earliest_co_date'] = pd.NaT\n",
    "    \n",
    "    # BBL fallback for rows without CO date\n",
    "    no_co_mask = hpd_with_co['earliest_co_date'].isna() if 'earliest_co_date' in hpd_with_co.columns else pd.Series([True] * len(hpd_with_co))\n",
    "    if no_co_mask.any() and not co_bbl_min.empty:\n",
    "        to_fill = hpd_with_co.loc[no_co_mask, ['BBL_str']]\n",
    "        filled = pd.merge(\n",
    "            to_fill,\n",
    "            co_bbl_min.rename(columns={'bbl_normalized': 'BBL_str'}),\n",
    "            how='left',\n",
    "            on='BBL_str'\n",
    "        )\n",
    "        if 'earliest_co_date' in filled.columns:\n",
    "            hpd_with_co.loc[no_co_mask, 'earliest_co_date'] = filled['earliest_co_date'].values\n",
    "    \n",
    "    # Update the dataframe\n",
    "    hpd_multifamily_finance_new_construction_with_dob_date_df = hpd_with_co\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Total HPD rows: {len(hpd_with_co)}\")\n",
    "    print(f\"   Rows with CO dates: {hpd_with_co['earliest_co_date'].notna().sum()}\")\n",
    "    print(f\"   Rows without CO dates: {hpd_with_co['earliest_co_date'].isna().sum()}\")\n",
    "    \n",
    "    # Display sample\n",
    "    display_cols = ['Building ID', 'Project ID', 'BIN', 'BBL', 'earliest_dob_date', 'fully_permitted_date', 'earliest_co_date']\n",
    "    display_cols = [col for col in display_cols if col in hpd_with_co.columns]\n",
    "    print(f\"\\nüìã Sample data:\")\n",
    "    display(hpd_with_co[display_cols].head(10))\n",
    "    \n",
    "    # Export the final dataframe\n",
    "    import os\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    hpd_with_co.to_csv(\"output/hpd_multifamily_finance_new_construction_with_all_dates.csv\", index=False)\n",
    "    print(\"\\n‚úÖ Exported HPD + DOB/CO matched data to output/hpd_multifamily_finance_new_construction_with_all_dates.csv\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No CO data available to join\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
